Page Number,Summary
1,"The document discusses the use of deep learning, specifically Convolutional Neural Networks (CNN), in building vision systems. It is authored by Dr. Tian Jing and is copyrighted by the National University of Singapore. The document cautions that the notations and color codes used may be inconsistent across different slides."
2,"The document discusses the use of deep learning, specifically convolutional neural networks (CNN), in building vision systems. It covers feature representation learning, classification, detection, and segmentation techniques. These methods are important for creating accurate and efficient vision systems for various applications. The document is copyrighted by the National University of Singapore and all rights are reserved."
3,"The document discusses the use of deep learning, specifically convolutional neural networks (CNN), for feature representation learning in building vision systems. This approach is compared to the traditional method of using hand-crafted features and traditional machine learning. The input image is fed into the deep learning model, which extracts features such as color, texture, and shape, and outputs a prediction of whether the image contains a Merlion or not. This method is considered more effective and efficient in learning complex features and producing accurate results."
4,"The document explains how convolution, a mathematical operation, can be used in deep learning (specifically CNNs) to extract features from images. It provides a demo website that illustrates the effects of different convolution filters, such as blur and sharpen, on images. This process is important for building effective vision systems using deep learning techniques."
5,"Convolution is a key process in extracting features from images using deep learning (CNN). It involves using stacked filters to calculate the magnitude of the image's edges, resulting in an edge map. This process is demonstrated through the example of Sobel edge detection (using non-stacked filters) and Canny edge detection (using stacked filters). The final edge map is then smoothed using a Gaussian blur."
6,"Convolution is a key process in extracting features from images using deep learning, specifically using a convolutional neural network (CNN). This process involves using multiple-resolution stacked filters, such as the Gaussian pyramid and Laplacian pyramid. The Gaussian pyramid progressively blurs and subsamples the image, while the Laplacian pyramid calculates the difference between the up-sampled and original image. This process is based on the concept of the Laplacian pyramid as a compact image code, which was first introduced in a 1983 paper by the IEEE."
7,"The GIST feature extraction method involves convolving an input image with 32 Gabor filters at 4 scales and 8 orientations, resulting in 32 feature maps. These maps are then divided into 16 cells and the average feature values within each cell are calculated. The 16 averaged values from all 32 feature maps are then concatenated to create a 512-dimensional GIST descriptor. This method is based on a 2001 study on representing the spatial envelope of a scene."
8,"The document discusses the use of convolutional neural networks (CNN) in building vision systems using deep learning. It specifically mentions the VGGNet-16 model and its key components, which include convolution, pooling, fully connected, and softmax layers. The model has 16 layers and is capable of classifying images into 1000 categories in ImageNet."
9,"The document discusses the process of calculating a single convolution filter, which involves overlaying the filter on top of an image and performing element-wise multiplication between the filter and corresponding values in the image. The sum of these products is the output value for the destination pixel in the output image. This process is repeated for all locations in the image. The result is an output feature map."
10,"The convolution layer in a CNN is used to extract features from an input image. It works by convolving a filter over all spatial locations of the image and calculating the dot product with a small patch of pixels, plus a bias term. This layer has trainable parameters based on the filter size and uses sparse connectivity, meaning each element in the feature map is only connected to a small patch of pixels. It also utilizes parameter-sharing, where the same weights are used for different patches of the input image. This is different from a multi-layer perceptron neural network, where each input is connected to the entire image."
11,"The convolution layer in deep learning systems uses a technique called stride to control how the filter moves over the input data. This affects the size of the output feature map and can help reduce the computational complexity of the network. Stride is an important parameter to consider when building vision systems using deep learning, as it can impact the accuracy and efficiency of the network."
12,"The document discusses the use of padding in convolutional layers to preserve information on the boundaries, which can be lost due to the reduction in output size. Padding involves adding extra rows and columns to the input, and there are different types of padding such as zero padding, reflect padding, replicate padding, and circular padding. This helps to improve the accuracy and performance of deep learning vision systems using CNNs."
13,"The document discusses the use of deep learning, specifically convolutional neural networks (CNN), in building vision systems. It mentions the use of multiple filters, each with a size of 5x5x3, in a convolution layer to obtain a 28x28x6 output. The number of trainable parameters is calculated to be 456, taking into account the number of filters, their size, and a bias factor."
14,"The convolution layer is a key component in building vision systems using deep learning, specifically Convolutional Neural Networks (CNN). It takes in an input of width and height, with a certain number of channels, and applies a filter of a given size and stride to produce an output of a different width and height, with a different number of channels. The output size can be calculated using a formula that takes into account the input size, filter size, padding, and stride. The number of trainable parameters in the convolution layer is determined by the filter size, input channels, and number of filters."
15,"The input for the S-VSE model is a 32x32x3 image. The model uses 10 filters with a size of 5x5, a stride of 1, and a padding of 2. This results in an output dimension of 32x32x10. The model has a total of 760 trainable parameters, with each filter having 76 parameters. The exercise demo can be accessed at https://madebyollin.github.io/convnet-calculator/."
16,"The pooling layer in a convolutional neural network helps reduce the size of the output by applying a simple operation, such as max or average, to neighboring pixels. This is based on the idea that neighboring pixels in images tend to have similar values. The pooling layer does not have any trainable parameters. An example of pooling is a 2x2 layer with a stride of 2."
17,"The pooling layer is an important component in building vision systems using deep learning (CNN). It takes in an input with a width and height of 𝑊𝑊 and 𝐶𝐶𝑖𝑖𝑖𝑖 channels. The pooling size, 𝐾𝐾, and stride, 𝑆𝑆, determine the size of the output, with the pooling function (max or average) being applied to each 𝐾𝐾×𝐾𝐾 region. The output has a width and height of 𝑊𝑊−𝐾𝐾𝑆𝑆+1 and 𝐶𝐶𝑜"
18,"The fully connected layer is an important component of a deep learning vision system using CNN. It takes the output of previous layers and flattens it into a single vector, which can then be used as input for the next stage. The number of trainable parameters in this layer is determined by the connections between layers and the biases in each layer. The formula for calculating the number of trainable parameters is (current layer's nodes * previous layer's nodes) + 1 * current layer's nodes."
19,The softmax layer is a function used in deep learning to convert a vector of real numbers into a probability distribution. It exponentiates each element and then normalizes them to create a probability distribution. It can be implemented as a built-in function in a loss function. The formula for the softmax function is also provided.
20,"The Inception block, also known as InceptionNet, is an advanced architecture used in deep learning for building vision systems. It consists of four branches, each with a different combination of convolutional layers and max pooling. The output of each branch is then concatenated to create the final output. This architecture was first introduced in 2015 and has been used in various applications for image recognition and classification."
21,"The Residual Neural Network (ResNet) is an advanced architecture for deep learning, specifically for image recognition. It uses skip connections to perform identity mappings and merge layer outputs with the original input using addition. The network is divided into stages, with each stage having multiple 3x3 convolutional layers. This architecture was introduced in 2016 and has shown significant improvements in image recognition tasks."
22,"The document discusses advanced architecture for building vision systems using deep learning, specifically the ResNet model. This model includes residual blocks, which use skip connections to improve performance in deeper networks. The structure of these blocks includes 1x1 and 3x3 convolutions to reduce and expand feature maps. This architecture is based on the paper ""Deep Residual Learning for Image Recognition"" and is used for image recognition tasks."
23,"The document discusses the advanced architecture of residual blocks (ResNet) in building vision systems using deep learning (CNN). It also compares the multiply-accumulate (MAC) operations in ResNet to those in GPT-4. ResNet is a popular deep learning architecture that uses residual blocks to improve performance and training speed. These blocks allow for easier optimization and training of deep networks. The comparison of MAC operations in ResNet and GPT-4 shows that ResNet has a higher efficiency and requires fewer operations, making it a more efficient choice for building vision systems."
24,"The document discusses the advanced architecture of ResNet, a type of convolutional neural network (CNN) used for building vision systems using deep learning. The ResNet architecture includes two types of residual blocks: the basic block and the bottleneck block. These blocks allow for the training of deeper networks and have been shown to improve performance on image recognition tasks. The source of this architecture is a research paper published in 2016."
25,"The DenseNet architecture, based on the Densely Connected Convolutional Networks paper, has a growth rate of 32 and 6 layers. Each layer adds 32 new feature maps to the output, resulting in a final output of 56 x 56 x 256. This architecture is used in advanced vision systems built using deep learning (CNN)."
26,"Page 26 of the document discusses the agenda for building vision systems using deep learning, specifically focusing on feature representation learning using CNN, as well as classification, detection, and segmentation. These are key components in creating a successful vision system, and the document provides an overview of each topic."
27,"The document discusses best practices for building vision systems using deep learning, specifically focusing on image classification tasks. It suggests using a benchmark or custom dataset and measuring performance using metrics such as accuracy, precision, and recall. The model architecture, loss, and implementation details, such as training configuration and data augmentation, are also important considerations. The use of transfer learning, finetuning, and knowledge distillation techniques are recommended for optimizing the performance of the model."
28,"Page 28 provides information on creating benchmark or custom datasets for image classification using deep learning (CNN). It highlights the importance of considering the data source, whether it is from the internet or self-collected, and the process of data cleaning or selection. It also mentions the need for proper annotation and provides questions to consider, such as how to label the data and the number of categories and instances per category. Additionally, it mentions the use of raw data or augmented data and provides an example of a dataset overview with snapshots."
29,"This section discusses the performance metrics used for evaluating the classification accuracy of a deep learning model. These include true positives, false positives, true negatives, and false negatives. The accuracy, recall, and precision metrics are also explained, with formulas provided for each. A reference is included for further reading."
30,"The document discusses the benchmark analysis of representative deep neural network architectures for building vision systems using deep learning. The benchmark analysis evaluates the performance of various models, including ResNet, DenseNet, and Inception, on different datasets. The results show that these models achieve high accuracy on tasks such as image classification and object detection. The document also provides a reference to the PyTorch library for implementing these models."
31,"The document discusses the use of deep learning, specifically convolutional neural networks (CNN), for building vision systems. It highlights the importance of loss functions, such as cross entropy loss, in measuring the difference between the model's prediction and the ground truth in image classification tasks. This helps in training the model to improve its accuracy in identifying images. The document is copyrighted by the National University of Singapore and is version 2.6, published in 2025."
32,"The document discusses various methods of image augmentation for deep learning, including flipping, rotation, scaling, translation, cropping, noise injection, contrast adjustment, and sharpening. These techniques aim to increase the diversity and size of the training dataset, which can improve the performance of CNN-based vision systems. The source for this information is a survey on image augmentation techniques published in 2023."
33,"The document discusses the importance of model weight initialization in building vision systems using deep learning, specifically convolutional neural networks (CNN). It mentions that improper initialization can lead to slow convergence or even failure of the model. The document provides an online demo and additional reading material for further understanding of weight initialization techniques."
34,"The document discusses data pre-processing for pre-trained models, using an example of a data matrix with each example in a row. It explains that pre-processing is necessary to ensure the data is in the correct format and range for the pre-trained model to work effectively. This includes resizing images, normalizing pixel values, and converting data types. The document also provides a link to a resource for pre-trained models in PyTorch."
35,"/datasets.html

The document discusses the use of deep learning and convolutional neural networks (CNN) for building vision systems. It mentions two specific networks, one for the Fashion-MNIST data set and another for the CIFAR-10 data set, which both consist of convolutional and fully-connected layers. These networks have been benchmarked and their performance can be compared using the reference links provided."
36,"Learning rate is an important factor in training deep learning models using Convolutional Neural Networks (CNN). A learning rate schedule can be used to adjust the learning rate as the training progresses, with two common methods being constant learning rate and learning rate decay. In constant learning rate, the learning rate is not changed during training, while in learning rate decay, the initial learning rate is gradually reduced according to a scheduler."
37,"This section discusses transfer learning in deep learning for building vision systems. It involves using a pre-trained model on ImageNet with 1000 categories and fine-tuning it by freezing certain layers and training additional layers to adapt it to a specific task. This approach can save time and resources compared to training a model from scratch. The number of layers to freeze and train can be adjusted, with a common choice being to freeze the first few layers and train the remaining ones."
38,"This section discusses the concept of transfer learning in building vision systems using deep learning, specifically focusing on using convolutional neural networks (CNNs). Transfer learning involves using a pre-trained model and adapting it for a new dataset. The amount of data and similarity between the new and pre-trained dataset will determine the extent of finetuning, which involves adding layers to the pre-trained model. The reference provided is a guide for determining the number of layers to finetune based on the amount of data and similarity between datasets."
39,"Transfer learning is a technique in deep learning that involves using a pre-trained model (teacher) to train a new model (student) on a different task. This is done by transferring the knowledge and parameters learned by the teacher to the student model, which can then be fine-tuned on the new task using ground truth data. This process, known as knowledge distillation, can help improve the performance of the student model and reduce the amount of training data needed."
40,"Transfer learning involves using a pre-trained model (teacher) to train a new model (student). The pre-trained model's output is used as a ""label"" or soft target for the new model, which provides more informative information than the ground truth. This allows the new model to learn from the pre-trained model's knowledge and improve its performance. The soft target provides more detailed information, such as differentiating between similar categories, which can help the new model better understand and classify data."
41,"was used to classify waste images into 6 categories for waste management.

The Waste6 workshop dataset contains 6 categories of waste images for waste management, including cardboard, glass, metal, paper, plastic, and trash. A CNN model was fine-tuned and used to accurately classify these images into their respective categories. This dataset and model can be used for waste management purposes."
42,"The document discusses the use of deep learning, specifically Convolutional Neural Networks (CNN), in building vision systems. It covers three key tasks: feature representation learning, classification, detection, and segmentation. These tasks are essential in creating accurate and efficient vision systems. The document also mentions that the copyright for this information belongs to the National University of Singapore and is reserved until 2025."
43,"The objective of object detection is to build a model that can output a box label and box coordinates, including the column and row index of the top-left corner, as well as the width and height of the box. This involves both classification and regression problems. An example photo is provided to illustrate the concept."
44,"The document discusses the milestones in object detection over the past 20 years, citing a survey on the topic. It mentions the development of various techniques such as sliding window, deformable part models, and region-based convolutional neural networks (R-CNN). It also highlights the emergence of deep learning and the significant improvements it has brought to object detection, with the introduction of popular models such as Faster R-CNN and YOLO. The document concludes by mentioning the potential for further advancements in object detection using deep learning in the future."
45,"The document discusses using deep learning (specifically, a convolutional neural network) to build vision systems, focusing on single object localization in images. The output of the model is a box label and coordinates for the object in the image. The model is trained using a combination of Euclidean distance loss for the box coordinates and cross entropy loss for the box label. The model also has separate regression and classification heads to handle the different tasks."
46,"The document discusses the challenge of training a single neural network model to detect multiple objects of varying categories. The solution proposed is to use a neural network on different crops of the image, classifying each crop as either an object or background. This allows for the detection of multiple objects in an image, such as elephants, zebras, and cars."
47,"This section discusses sliding-window based object detection, which involves applying a classifier to each sliding window in an image and using negative samples to train the classifier. The output includes a set of box labels and scores, as well as the coordinates of the boxes. Non-Maximum Suppression is then applied to select the best boxes."
48,"The object classifier in S-VSE uses image patches with the same resolution as input and outputs object/background labels with scores. Training samples for object categories are image patches with big overlapping with labelled ground-truth box, while training samples for background category are image patches with small/no overlapping with labelled ground-truth box. The Intersection of Union (IOU) is used to evaluate overlapping, with the formula 𝐼𝐼𝐼𝐼𝐼𝐼 = 𝐴𝐴𝐴𝐴𝑒𝑒𝐴𝐴𝑃𝑃𝑓𝑓 𝐼𝐼𝑂𝑂𝑒"
49,"is a technique used to select one entity, such as a bounding box, from a group of overlapping entities. The selection is based on an overlap measure, such as IOU, and is used to filter out redundant entities. The algorithm involves selecting the entity with the highest confidence score, comparing it to the remaining entities, and removing those with high overlap. This process is repeated until all entities have been evaluated. 

NMS is a method for selecting the best entity, such as a bounding box, from a group of overlapping entities. It uses an overlap measure, like IOU, to filter out redundant entities. The algorithm involves selecting the entity with the highest confidence score, comparing it to the remaining entities, and removing those with high overlap. This"
50,"The sliding window-based CNN approach involves generating multiple proposals using regular grids, selecting and resizing each proposal, and using a CNN model for object classification. Bounding-box regression is then used to refine the localization of the proposals. This method allows for per-image and per-region computation, making it suitable for building vision systems using deep learning."
51,"The objective of R-CNN is to avoid exhaustive search by using selective search to generate region proposals for object detection. This method uses OpenCV, an intensity-based pixel grouping method, to propose regions that will be used for detection. The process involves per-image computation and per-region computation, where selective search is used to generate region proposals, which are then cropped, resized, and fed into a CNN for box classification and regression. This approach is based on the paper ""Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"" and has been implemented in VSE 2 VSDL (CNN-based) v2.6."
52,"The Fast R-CNN method aims to simplify the computational complexity of using CNNs for object detection by applying the CNN on the entire image first, then cropping and resizing region features from the CNN feature map and applying a MLP. This method also addresses the challenge of varying region proposal sizes by using ROI pooling to obtain a fixed-size feature for the subsequent network input. The Fast R-CNN method is referenced in the ICCV 2015 paper and aims to improve the efficiency of CNN-based vision systems."
53,The ROI pooling layer in a CNN is used to crop and resize a region of interest (ROI) in an image to a fixed size for input into the network. This is achieved by dividing the ROI into a grid of sub-windows and performing max-pooling on each sub-window to obtain a fixed spatial extent. The ROI is defined by a four-tuple and the pooling process is illustrated with an example of a 145x200 region being scaled to a 4x6 region in the feature map. This layer is important for accurately capturing features within specific regions of an image.
54,"The Faster R-CNN algorithm uses a region proposal network (RPN) to generate region proposals, which are then cropped and resized from the CNN feature map to obtain a fixed-size network input. This input is then used to classify and refine the localization of objects. The joint loss function includes RPN classification and regression, as well as object classification and regression. This approach allows for real-time object detection with improved accuracy."
55,"and region positions are generated from a feature map using a region proposal network. The region proposals are used for binary classification to determine if an image contains an object of interest. The outputs of the network are explained as objectness score and region positions of anchors, which are placed at each position on the feature map. The classification head outputs the probability of an anchor box containing an object, while the regression head outputs values used to adjust the position of each anchor box."
56,"The Region Proposal Network (RPN) is a key component of the S-VSE vision system, which uses deep learning (specifically, Convolutional Neural Networks or CNNs) to identify objects in images. The RPN uses anchor boxes, which are pre-defined regions on the CNN feature map, to determine the location and label (object or background) of objects in the original image. These anchor boxes are adjusted through box regression, using ground-truth labelled boxes as a reference. The RPN also uses different anchor configurations, such as different sizes and aspect ratios, to improve object detection. To obtain the ground truth for the two outputs of the RPN (2𝐾𝐾 × 16 × 16 and"
57,"The region proposal network (RPN) determines where in the original image an anchor box can be seen, based on the feature maps of different layers in a CNN architecture. The setup involves using a convolutional kernel size of 3 and a stride of 1, with a square window. The same kernel size and stride are used for all layers. The feature maps have different dimensions and receptive fields, with the formula for the 𝑃𝑃-th layer being 𝑅𝑅(𝑖𝑖−1) = 𝑅𝑅(𝑖𝑖) −1 × 𝑃𝑃 + 𝑘𝑘. Layer 0 is the original image"
58,"The document discusses the differences between two-stage and one-stage object detection using Convolutional Neural Networks (CNN). Two-stage detection involves using a region proposal network to identify potential objects, followed by a box classification/regression stage. One-stage detection attempts to combine these two stages by predicting object categories and bounding box transformations directly from the image feature maps. This is achieved by placing anchors around each position in the feature map and predicting categories and transformations for each anchor."
59,"The YOLO algorithm is a one-stage object detection method that uses a grid map with a resolution of 7x7 and has 2 boxes per grid and 20 object categories. The coordinates of the box center inside each cell are represented by x and y, while the box width and height are represented by w and h. The box confidence score is represented by P(obj) and the class probability is represented by P(obj|class in box). The neural network used for training has two linear layers with 1024 and 4096 neurons respectively. YOLO was introduced in 2016 and has been trained on images with a size of 3x448x448 and produces a final output of 30x7x7"
60,"Anchor-based methods for building vision systems using deep learning (CNN) involve a backbone network trained on a large-scale image classification task, such as ImageNet, to capture hierarchical features at different scales. The neck of the network then aggregates and refines these features to enhance spatial and semantic information. The head of the network makes predictions based on the features provided by the backbone and neck for each object candidate. Non-maximum suppression (NMS) is used as a post-processing step to filter out overlapping predictions and retain only the most confident detections."
61,"The FOCS (fully convolutional one-stage object detection) method uses a CNN (convolutional neural network) to extract features from an input image and classify each point as positive or negative based on its alignment with a ground-truth box. It also regresses the distance to the edges of the box and predicts the ""centerness"" of each positive point. This method is based on the FCOS approach and is used for building vision systems through deep learning. It was developed by the National University of Singapore and is protected by copyright. The reference for this method is the FCOS paper from ICCV 2019."
62,"CenterNet is an anchor-free approach for building vision systems using deep learning, specifically convolutional neural networks (CNN). It uses a heatmap to identify potential points for the center of a bounding box, and then uses offset prediction to correct the location. Additionally, it predicts the size of the bounding box by giving the width and height of each selected point on the heatmap. This method is based on the ""Objects as Points"" paper and is developed by the National University of Singapore."
63,"/

Object detection involves identifying and localizing objects in an image. To address multiple objects, techniques such as box classification and regression, sliding window, and non-maximum suppression are used. Region proposal methods, such as selective search, are used to reduce the number of windows to examine. This can be further optimized by applying region proposal on CNN feature maps and using ROI pooling. Faster R-CNN integrates region proposal training and inference by training the region proposal network on CNN feature maps. Other techniques, such as YOLO, FOCS, and CenterNet, propose regions using anchor boxes or anchor-free representations."
64,"The challenge of imbalanced focal loss in model training configuration is addressed by using cross-entropy loss, which adjusts the contribution of each sample based on its classification error. This is achieved through binary cross entropy loss and focal loss, with a focusing factor 𝛼𝛼 that can be adjusted to prioritize problematic samples. Examples of good and bad results are shown, with focal loss significantly reducing the contribution of the loss for a good result and not much change for a bad result."
65,"The challenge in building vision systems using deep learning is handling multiple scales of features. To address this, the feature pyramid network approach was proposed, which involves attaching a detector to features at different levels and adding top-down connections to feed information from high-level features to lower-level features. This approach was introduced in the CVPR 2017 paper and has been successful in improving object detection performance."
66,"The challenge of detecting small objects in vision systems using deep learning (CNN) can be addressed by increasing image capture resolution, tiling images, generating more data through augmentation, optimizing anchors using K-means on training data, and filtering out extraneous classes. However, the fundamental challenge remains in the loss of details of small objects in deeper layers of object detection models due to the aggregation of pixels in convolutional layers."
67,"When building vision systems using deep learning, there are several key considerations to keep in mind. These include the dataset, model selection, speed, and whether to use a two-stage or one-stage approach, as well as whether to use anchor-based or anchor-free methods. Additionally, it is important to consider whether the system will be used for closed-set or open-set tasks and to carefully configure the training process, taking into account hyperparameters and anchor settings for custom datasets. Ensuring variety, consistency, and appropriate background images in the dataset is also crucial for achieving optimal performance."
68,"The document discusses the use of deep learning, specifically convolutional neural networks (CNN), for building vision systems. It covers three key areas: feature representation learning, classification, and detection. Feature representation learning refers to the process of extracting meaningful features from images using CNNs. Classification involves categorizing images into different classes, while detection focuses on identifying and localizing objects within an image. Finally, segmentation involves dividing an image into different regions based on the objects present. These techniques are important for building effective and accurate vision systems."
69,"The document discusses the use of deep learning, specifically convolutional neural networks (CNN), for segmentation tasks in building vision systems. It explains how CNNs can learn to segment images by identifying and classifying different regions. The document also mentions the use of a VSE (visual scene understanding) framework, which incorporates both visual and semantic information to improve segmentation accuracy. It also highlights the importance of data augmentation and transfer learning in improving the performance of CNN-based segmentation models."
70,"The document discusses semantic segmentation, which involves labeling each pixel in an image with a semantic label without differentiating instances. This technique has various applications, such as identifying cancerous cells in healthcare, detecting defects in manufacturing, and overlaying objects in augmented reality. The document also mentions several datasets commonly used for semantic segmentation, including COCO, Open Images, KITTI, and NYU."
71,The document discusses the challenges of modifying a classification network to achieve pixel-level prediction. The two main questions are how to represent the target label image and how to up-sample to match the input image resolution. The example of a VGG model is used to demonstrate this process. The input is a color image and the output is a label for one of 1000 image categories.
72,"The content on page 72 discusses the topic of semantic segmentation, which is a computer vision task that involves labeling each pixel in an image with a corresponding class label. The article then poses the question of how to represent the target label image, and suggests three methods: greyscale images where pixel intensity represents class id, one-hot vectors where each pixel is encoded with a value of 1 for the class it represents, and color images where different classes are assigned a specific RGB color for visualization purposes."
73,"The document discusses the performance metric for building vision systems using deep learning. It mentions that the prediction for a selected pixel is compared to the ground truth, which is represented as a one hot vector. The performance is evaluated by calculating the pixel-wise cross-entropy loss between the model prediction and ground truth for all pixels and averaging the results."
74,"loss

The performance metric for building vision systems using deep learning (CNN) is measured by the IOU (intersection over union) and Dice coefficient. IOU is calculated by dividing the overlapping region by the combined region, while Dice coefficient is calculated using the number of common elements between two regions and the total number of elements in each region. Dice loss is calculated as 1 minus the Dice coefficient, and is used to evaluate the performance of the model. This is repeated for multiple classes and averaged. The example provided shows a Dice loss score of 0.0632."
75,"The document discusses the use of deep learning, specifically Convolutional Neural Networks (CNN), for semantic segmentation, a technique that involves labeling each pixel in an image with a corresponding class label. The key challenge is to up-sample the output to match the resolution of the input image. The solution proposed is to design a network with only convolutional layers, which can make predictions for all pixels at once. However, this requires a large receptive field to accurately identify objects in the image. This can be achieved by using multiple layers of 3x3 convolution."
76,"The U-Net model is a type of convolutional neural network (CNN) used for biomedical image segmentation. It consists of an encoder, which uses convolutions and pooling, and a decoder, which uses transpose convolutions. The model also includes a connection to carry details from the encoder to the decoder. In this version, the number of feature channels is reduced by half (from 128 to 64) in the up-convolution layer, and 64 channels are copied. This model was first introduced in 2015 and has been used in various applications for biomedical image segmentation."
77,"The document discusses other pixel labeling tasks in addition to semantic segmentation, specifically instance segmentation. It introduces Mask R-CNN, which has a total loss function consisting of three components: classification loss, bounding box loss, and mask loss. The classification loss is based on the object id and classification, the bounding box loss is based on the object bounding box and regression, and the mask loss is based on the segmentation and binary cross-entropy between the predicted mask and the ground truth."
78,"The article discusses key factors to consider when building vision systems using deep learning, including the use of external data, preprocessing techniques, data augmentations, modeling methods, hardware setups, loss functions, and ensembling methods. It also references a survey on image segmentation using deep learning and a study on the topic."
79,"The appendix on YOLO series in the document 'VSE 2 VSDL (CNN-based) v2.6.pdf' provides a comprehensive review of the You Only Look Once (YOLO) series, including YOLOv1 to YOLOv8 and YOLO-NAS. It also includes a tutorial on YOLOv5 and its best training practices. These resources are available on arXiv and the Ultralytics website."
80,"The document discusses the use of deep learning, specifically convolutional neural networks (CNN), for building vision systems. It is copyrighted by the National University of Singapore and was last updated in 2025. The author, Dr. Tian Jing, can be contacted via email."
