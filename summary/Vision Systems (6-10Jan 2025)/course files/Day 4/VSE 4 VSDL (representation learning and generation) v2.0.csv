Page Number,Summary
1,"This document discusses the use of deep learning for building vision systems, specifically focusing on representation learning and generation. It is authored by Dr. Tian Jing and is copyrighted by the National University of Singapore. The document notes that there may be inconsistencies in the notations and color codes used throughout the slides."
2,"The document discusses the use of deep learning for building vision systems, specifically focusing on representation learning and generation. It covers both supervised and self-supervised methods for feature representation learning, as well as generative vision systems for distribution representation learning. The document is copyrighted by the National University of Singapore and was published in 2025."
3,"The document discusses feature representation learning, which is the process of mapping data to a representation space for analytics tasks. This can be achieved through supervised learning, where human-annotated labels are used, or self-supervised learning, where supervisory signals are generated from the data itself. The target task, or downstream task, is the actual application task for which the representations are fine-tuned. A pretext task is a pre-designed task used to help the model learn meaningful representations. The model is then fine-tuned with task-specific data."
4,"The document discusses the use of pretext tasks in computer vision, specifically the Barlow Twins method, for building vision systems using deep learning. This method involves using two views of the input batch and minimizing the correlation of the embedded representations. The process involves an encoder, embedding, and input, which are used to transform and reconstruct the input and estimate the transform. The loss function is used to compare the input and transform, and data prediction and contrastive learning are also important components."
5,"The paper discusses the use of a context encoder, which is an encoder-decoder pipeline that can predict missing regions in an image. This process involves using a deep learning approach to learn representations of images and then generating the missing content based on these learned features. The paper references a study on context encoders and highlights their potential for feature learning and inpainting."
6,"The article discusses the use of masked autoencoders (MAE) for data prediction in building vision systems using deep learning. During pre-training, a large random subset of image patches is masked out and the encoder is applied to the remaining visible patches. Mask tokens are then introduced after the encoder and the full set of encoded patches and mask tokens are processed by a small decoder to reconstruct the original image using MSE loss. After pre-training, the decoder is discarded and the encoder is used for recognition tasks on uncorrupted images. This approach has been shown to be effective in scalable vision learning and was presented at CVPR 2022."
7,"The Transform prediction technique, also known as RotNet, is a method for unsupervised representation learning using deep learning. It involves training a model on images that have been rotated by 0°, 90°, 180°, and 270°, and then predicting which rotation was applied. This is done through a 4-category classification task, with a classification loss used as the training loss. The same model is used for all rotations. This technique was introduced in 2018 and has been shown to be effective for representation learning."
8,"The document discusses the use of contrastive learning in building vision systems using deep learning. This approach involves creating a batch of images and using two different augmentations for each image. The objective is to predict which images are from the same original image and which are not. This is achieved through the use of an InfoNCE loss function, which penalizes incorrect predictions. A temperature parameter is used to control the sensitivity of the loss function. This approach has been shown to be effective in learning visual representations, as demonstrated in a referenced study from ICML 2020."
9,"The document discusses a method called contrastive learning, specifically SimCLR, for building vision systems using deep learning. This method involves using an encoder, such as ResNet, to transform and embed images into a representation space. The loss function used is InfoNCE, which calculates the similarity between a positive pair of images generated by applying data augmentation techniques. This process is iterated through multiple samples to compute the average InfoNCE loss. This method has been shown to be effective in learning visual representations and is described in a paper titled ""A Simple Framework for Contrastive Learning of Visual Representations""."
10,"The document discusses the use of contrastive learning, specifically the Momentum Contrast (MoCo) method, for unsupervised visual representation learning. This technique involves using a teacher model and a student model to create positive pairs by sampling data augmentation functions, with the teacher model updating its parameters using a momentum update. The loss function used is InfoNCE, and the approach has been shown to be effective in learning visual representations without the need for labeled data. The technique is referenced in a paper from CVPR 2020."
11,"This page introduces the concept of contrastive learning, specifically the DINO method, which uses self-distillation without labels to train vision systems using deep learning. The process involves transforming input images, encoding them, and creating embeddings, with the goal of minimizing a loss function. The DINO method also incorporates color jittering, Gaussian blur, and multi-crop techniques, and uses a ViT/ResNet encoder with an MLP head. The student model is updated using a momentum update and centering softmax, with the teacher model providing a reference for the gradient. This method has been shown to have promising results in self-supervised vision transformers."
12,"The document discusses the applications of self-supervised feature representation learning, including pre-training in vision and multimodal models, such as CLIP, Flamingo, SAM, and LLaVa. It also mentions the use of this technique in person re-identification (re-id). These applications demonstrate the effectiveness of self-supervised feature representation learning in various tasks and highlight its potential for improving vision systems."
13,", but it is not practical

The paper discusses the application of pre-training in vision models, specifically using the DINO method. DINO is a popular method for pre-training Vision Transformer (ViT) models, but it is not practical due to its high computational cost. The paper suggests using a modified version of DINO called ""Emerging Properties in Self-Supervised Vision Transformers"" (EP-ViT) which reduces the computational cost while still achieving similar performance. EP-ViT is shown to outperform DINO on multiple datasets and can be used for pre-training vision models in a more practical manner."
14,"The document discusses an application of bi-directional multimodal pre-training, specifically the CLIP (Contrastive Language-Image Pre-training) model. This model is trained to predict the correct pairings of a batch of (image, text) examples, using a dataset of 400 million pairs collected from the internet. The training process involves maximizing the cosine similarity of the image and text embeddings for the real pairs in the batch, while minimizing the similarity for incorrect pairings. This approach has shown promising results in learning transferable visual models from natural language supervision."
15,"This section discusses the application of bi-directional multimodal pre-training, specifically the CLIP model, in building vision systems using deep learning. The loss function used in this application is InfoNCE, which is a combination of two loss terms. The first term calculates the similarity between the visual and textual representations, while the second term measures the similarity between the visual representations. This approach has been shown to be effective in learning transferable visual models from natural language supervision."
16,", this application proposes a novel encoder-decoder multimodal pre-training approach called Flamingo. It aims to improve few-shot learning by learning visual and language representations simultaneously.


The document discusses a new approach called Flamingo for pre-training vision and language representations simultaneously. This is achieved by using pre-trained vision-only and language-only models. The goal of Flamingo is to improve few-shot learning, which is the ability to learn from a small number of examples. This application was presented at NeurIPS 2022 and can be found in the arXiv database."
17,"The article discusses an application of encoder-decoder multimodal pre-training called Segment Anything Model (SAM). This model uses an image encoder that is pre-trained on MAE ViT-H/16 and a text encoder from a pre-trained CLIP model. The model also includes a prompt encoder, which combines positional encoding and learned embeddings to indicate if a point is in the foreground or background, and a box encoder, which uses a positional encoding of the top-left corner and a learned embedding for both top-left and bottom-right corners. The model also includes a mask decoder, which uses convolutions and image embeddings to generate masks for objects in the image."
18,"The document discusses the application of decoder-only multimodal pre-training, specifically the Large Language and Vision Assistant (LLaVa). In Stage 1, the vision and language backbones are kept frozen while the projection layer is trained on approximately 600,000 text-image pairs. In Stage 2, the vision backbone remains frozen while the language and projection layers are fine-tuned on a high-quality visual instruction dataset. This technique, known as Visual Instruction Tuning, was presented at NeurIPS 2023 and can be found in the arXiv preprint repository."
19,"The document discusses the use of pre-trained multimodal models in building vision systems using deep learning. It specifically mentions the use of a pre-trained CLIP model and how it can be used to identify the category of an input test image by selecting the text prompt with the highest similarity. Additionally, the document suggests the use of zero-shot fine-tuning with an adapter, referencing a medical SAM adapter for medical image segmentation."
20,"The article discusses the use of deep learning for person re-identification, specifically in the context of assisting the government with contact tracing during the COVID-19 pandemic. The problem statement is to recognize a person's identity from a photo and verify if two photos belong to the same person. This technology can be applied to CCTV footage to track individuals and aid in contact tracing efforts. The source for this information is a CNA interview and a Today Online article from January 2020."
21,"The document discusses the idea of using supervised classification in building vision systems using deep learning. This involves classifying input images into specific categories, such as a person's name. However, challenges arise when there is insufficient data for a certain class or when there are a large number of classes. The classifier used can be either traditional machine learning or deep learning methods, trained from an annotated image dataset. The output of the classifier is a score."
22,"The traditional method for image matching involves extracting features from images and evaluating their similarity using techniques like color histograms, LBP, HoG, pre-trained CNN, or ViT. This is done by calculating the Euclidean distance between the features. The image with the largest similarity score is then identified and the person's name in the photo is retrieved."
23,"This section discusses feature similarity evaluation for unit vectors using pre-defined metrics, including Euclidean distance and cosine similarity distance. It explains how these metrics can be used to determine the similarity between two vectors, with a higher score indicating a greater similarity and a lower score indicating less similarity. The scenarios of same direction, nearly orthogonal, and opposite direction are also described, along with corresponding angles and similarity scores."
24,"The problem statement on page 24 of the document 'VSE 4 VSDL (representation learning and generation) v2.0.pdf' discusses the challenge in traditional image matching approaches, where the feature representation and similarity metric are not learned jointly. The input for this problem is a pair of images, and the goal is to evaluate how similar they are to each other. The output can be a binary label indicating if the images are the same or different, or a real number indicating the level of similarity between the images. The example dataset used is the Labeled Faces in the Wild dataset."
25,"A Siamese network is a type of neural network that has two identical subnetworks with shared parameters. It is used for learning representations in deep learning and is commonly used for tasks such as image recognition and similarity matching. The network is trained by feeding it pairs of inputs, with positive pairs being from the same class and negative pairs from different classes. The shared weights are updated through backpropagation, and the loss is calculated based on the distance between the features extracted from the shared backbone model. This architecture was first introduced in a paper titled ""Discriminative learning of deep convolutional feature point descriptors"" in 2015."
26,"The Siamese network uses a contrastive loss function to train models for visual similarity tasks, where 𝑦𝑦 is a binary label indicating positive or negative pairs. The positive pair loss is calculated using the Euclidian loss, while the negative pair loss uses the Hinge loss with a user-defined margin 𝑚𝑚. The backbone of the network shares weights between the two input images. The margin 𝑚𝑚 is based on the expected distance between negative samples. This approach was proposed in a 2015 paper on discriminative learning of deep convolutional feature point descriptors."
27,The Siamese network is a deep learning model used for building vision systems. It takes in a query image and extracts features to calculate similarity with images in a training dataset. Positive pairs (from the same person) and negative pairs (from different people) are generated and used to train the model. The trained model is then used to find the largest similarity score and retrieve the name of the person from a reference image database.
28,"The document discusses the task of re-identifying non-human objects, such as luggage, tigers, and cars, using deep learning representation. This task is similar to other object re-identification tasks and has been addressed by several datasets and challenges, including MVB, WACV 2020 Animal Re-ID, and CVWC 2019. These resources provide data and evaluation metrics for training and testing models for non-human object re-identification."
29,"The article discusses a computer vision-based approach for grading yoga poses using contrastive skeleton feature representations. This method utilizes deep learning techniques to accurately capture and represent the human body's skeletal structure in yoga poses. The results of this approach showed high accuracy in grading poses compared to traditional methods. The use of contrastive learning allows for a more robust and generalizable representation of human poses, making it applicable to other non-human objects as well. This approach has potential for various applications in fields such as sports, healthcare, and rehabilitation."
30,"This section discusses the use of deep learning for building vision systems, specifically in the context of automated classification of cluttered construction housekeeping images. The paper proposes a method for feature representation learning using both supervised and self-supervised techniques. The results show promising performance in accurately classifying non-human objects in cluttered images."
31,"The document discusses the use of deep learning for building vision systems, specifically focusing on feature representation learning and distribution representation learning. Feature representation learning can be done through supervised or self-supervised methods, while distribution representation learning involves creating generative vision systems. These techniques are important for creating robust and accurate vision systems that can handle complex visual data."
32,"The article discusses the difference between discriminative and generative models in deep learning. Discriminative models focus on identifying patterns in data to classify or predict, while generative models aim to understand the underlying structure of the data and generate new samples. The concept is illustrated through a humorous cartoon where a robot is unable to correctly identify an apple, highlighting the limitations of a purely discriminative approach. Generative adversarial networks (GANs) are also mentioned as a popular method for training generative models."
33,"The concept of generative vision systems is based on the allegory of the cave, where people are confined to a two-dimensional world and can only see shadows of three-dimensional objects. In this analogy, the shadows represent the output of a generative vision system, which is created by unseen three-dimensional objects passing in front of a light source. This highlights the idea that generative vision systems are able to create realistic images from abstract representations, similar to how the shadows in the cave are created by objects that cannot be seen directly. This approach can be used in deep learning to generate images and improve representation learning."
34,"The document discusses the use of autoencoders (AE) in building vision systems using deep learning. An AE consists of an encoder and decoder, with the encoder compressing the input into a latent representation and the decoder reconstructing the input from the latent representation. The latent space is visualized using the MNIST dataset, and a latent vector can be randomly defined and fed into the trained decoder to generate an image. The reconstruction loss is used to evaluate the quality of the generated image."
35,"The document discusses the use of autoencoders (AE) in deep learning for building vision systems. The latent space of an AE can be manipulated, allowing for the reconstruction of images using interpolated vectors between two latent vectors. The decoder is responsible for this reconstruction, and the interpolation is achieved through a linear equation using an interpolation parameter 𝑤𝑤. This technique allows for the manipulation and generation of images in the latent space."
36,"This page discusses the taxonomy of generative vision models, which are used in building vision systems using representation learning. There are two main types of generative models: explicit density models and implicit density models. Examples of explicit density models include generative adversarial networks, normalizing flow models, and autoregressive models. Examples of implicit density models include variational models, energy-based models, and score models. The page also references a list of papers on diffusion models and a course by Prof. Elad on Generative AI."
37,"The Generative Adversarial Network (GAN) is a deep learning model used for image generation. It consists of a generator (G) and a discriminator (D). The generator learns to create images from random noise, while the discriminator learns to distinguish between real and generated images. The latent space, which is the input for the generator, is not learned and is sampled from a predefined distribution. The generator is trained to map the latent space to the data space, creating new images. This process is repeated using random latent vectors to generate a variety of images."
38,"The page discusses the concept of Generative Adversarial Networks (GANs) in training vision systems using deep learning. GANs consist of a generator (G) and a discriminator (D), where G generates images from a random input (z) and D determines whether the image is real or generated. The goal is to train G to produce realistic images and D to accurately distinguish between real and generated images. This is achieved through a min-max game where G tries to push D to classify its generated images as real, while D tries to correctly classify all images. The process involves optimizing D while fixing G, and then optimizing G while freezing D."
39,"The Generative Adversarial Network (GAN) is a deep learning model that consists of a generator (G) and a discriminator (D). The generator takes in a random input (z) and produces a fake image, while the discriminator takes in both real and fake images and determines whether they are real or generated. The GAN is trained through a min-max game, where the generator tries to fool the discriminator, and the discriminator tries to correctly identify real images. The GAN is optimized through gradient descent, where the generator is fixed and the discriminator is optimized, and then the generator is optimized while the discriminator is frozen. This technique was introduced in 2014 and has been used in various applications."
40,The document discusses the use of Generative Adversarial Networks (GANs) in building vision systems using deep learning. GANs are a type of neural network that consists of two models - a generator and a discriminator - which work together to learn and generate new data. The document provides an example of an online demo that showcases the capabilities of GANs in generating images. This demo allows users to interactively explore the effects of different parameters on the generated images.
41,"The appendix of the document 'VSE 4 VSDL (representation learning and generation) v2.0.pdf' provides a reference for other variants of Generative Adversarial Networks (GANs) in computer vision. GANs are a type of deep learning model that can generate new data samples by learning from a training dataset. The reference includes a survey and taxonomy of GANs in computer vision, discussing their applications and variations such as conditional GANs and cycle-consistent GANs. It also mentions the challenges and future directions for GAN research in computer vision."
42,"The VAE (Variational Autoencoder) is an improved version of the AE (Autoencoder) that uses a probabilistic encoder and decoder. The encoder outputs a distribution parameterized by 𝜇𝜇𝑧𝑧|𝑥𝑥 and 𝜎𝜎𝑧𝑧|𝑥𝑥, while the decoder can handle inputs sampled from the learned latent distribution 𝑞𝑞(𝑧𝑧|𝑥𝑥). The VAE also applies a distribution loss to regularize the latent space and align it with a prior distribution 𝑝𝑝(𝑧𝑧) (e.g.,"
43,The reparameterization trick is a way to enable backpropagation through stochastic layers in a variational autoencoder (VAE). It allows for differentiable sampling from a latent distribution by expressing the random latent variable as a deterministic transformation of a noise and the distribution parameters. This enables the VAE to learn a latent representation of the input data and generate new outputs. The reparameterization trick is essential for training VAEs and involves sampling from a prior distribution and then transforming it using the mean and variance parameters learned by the encoder. This allows for efficient and accurate reconstruction of the input data.
44,"The VAE (Variational Autoencoder) loss function uses KL divergence to compare the learned distribution to a prior distribution. This is used to derive the cross-entropy loss in deep learning models. The equations (29)-(40) in the tutorial show the derivation of the VAE loss function. The prior distribution is assumed to be a normal distribution with mean 0 and standard deviation 1, and the learned distribution is represented by a one-hot encoded vector."
45,"The VAE (Variational Autoencoder) is a generative model that uses random latent vectors to generate new images. These latent vectors are passed through a decoder, which maps them to the data space. In a demo, two random numbers are input into the VAE decoder to generate a 28x28 image, which is then displayed at a specific position."
46,"The document discusses the concept of distribution representation learning, which involves using deep learning to build vision systems. It states that the dimension of a color image space is 512 × 512 × 3 = 786432, and each point in this space represents a unique combination of pixel intensities across the RGB channels for all pixels. It also mentions that an image with random uniform colors can be generated using the code ""image = torch.rand((512, 512, 3))""."
47,"The diffusion model, specifically the Denoising Diffusion Probabilistic Models (DDPM), is a technique used in deep learning for image denoising. It involves a forward diffusion process and a reverse denoising process, where a real image is compared to a random noisy image to remove noise and improve the quality of the image. This model was first introduced in 2020 and has been used in various applications."
48,"The key idea of DDPM is to use a pre-defined, non-learned forward diffusion process to transform a distribution from real image space to random image space in multiple steps. This allows for better representation learning and generation in vision systems. The process is demonstrated using a toy distribution 𝑁𝑁(5,2) in real image space and 𝑁𝑁(0,1) in random image space."
49,"The DDPM approach to generative modeling involves training a decoder neural network with shared parameters in a reverse process, turning it into a set of supervised prediction tasks. This is achieved by feeding random images from a noise space into the decoder, which then attempts to reconstruct them into real images. This process is repeated in multiple steps to improve the accuracy of the decoder."
50,"The document discusses the forward process of building vision systems using deep learning, specifically the fixed and non-learned components. It explains the forward step, which involves a reparameterization trick to generate a new state based on the previous one and a random noise. The forward jump is also discussed, which involves a combination of the initial state and a random noise. The document emphasizes the importance of preserving variance in the process, which can be achieved by ensuring that the squared coefficients of the scaled data and noise add up to 1. It also mentions the need for a controlled noise level, with a limit of less than 1, to ensure the simplicity of the process."
51,"𝑁𝑁

This section discusses the reverse process of generating images using a vision system based on deep learning. It explains how a model with parameters 𝜃𝜃 can be trained using a noise image as input to generate a real image. The unknown probability 𝑞𝑞(𝑥𝑥𝑡𝑡−1|𝑥𝑥𝑡𝑡) is learned using the model 𝜃𝜃 and the ground-truth noise image. The formula for this process is shown, along with the steps to obtain it."
52,This page discusses the concept of deriving loss in deep learning using diffusion models. It provides a link to a paper that explains the process in detail.
53,"The document discusses the process of training a model for vision systems using deep learning. The forward process involves steps starting with an initial input 𝑥𝑥0 and a random noise 𝜖𝜖, which is then passed through a decoder neural network with parameters 𝜃𝜃. The loss function used is Mean Squared Error (MSE), which is calculated by taking the difference between the output image and the original image. The use of MSE is explained by the fact that noise images have the same dimension as the original image. However, the time step 𝑡𝑡 requires further embedding."
54,"The document discusses the training process for the S-VSE model, which uses a U-Net backbone. The U-Net architecture is based on the reference from the DDPM paper and is used for denoising diffusion probabilistic models. The training process involves using a screenshot from the DDPM paper as a reference for the U-Net architecture. This allows for efficient and effective training of the S-VSE model."
55,"𝑑𝑑

The document discusses the use of deep learning for building vision systems and introduces the concept of generation or sampling in this context. This involves generating images in the real image space by sampling from a random image space using a function that incorporates a noise parameter. The function is defined by a Gaussian distribution with a mean of the current image and a standard deviation parameter. This process allows for the generation of new images based on the learned representations."
56,"The DDPM (Diffusion-based Deep Generative Model) is a neural network model used for image generation. It consists of a model backbone, noise scheduler, and generation standard deviation. During training, a sample image is taken from the dataset and a random time step and noise vector are generated. The image is then updated using a gradient step on the loss function. During generation, a random noise vector is used to generate an image, and then the process is repeated in reverse order to generate the final image."
57,"The Fréchet Inception Distance (FID) is a method for evaluating the quality of images generated by generative models. It measures the distance between the feature distributions of real and generated images, using a pre-trained Inception network to extract features. Lower FID scores indicate that the generated images are closer in quality and diversity to the real images."
58,".html

The document discusses the potential future developments in diffusion models, which use deep learning to build vision systems. These developments include optimizing the models for faster processing, using denoising techniques, scaling up for high-resolution image synthesis, and incorporating conditional generation and personalized generation. Other potential applications include interactive editing, inpainting, and downstream tasks such as classification. The document also mentions additional resources for learning about diffusion models and a toolkit for implementing them in practice."
59,"The ""Generative Learning Trilemma"" refers to the challenge of balancing three key factors in generative learning: sample quality, diversity, and training stability. To address this, a new approach called Denoising Diffusion GANs (DDGANs) has been proposed, which uses a diffusion process to gradually generate high-quality and diverse samples while maintaining training stability. This technique has shown promising results in generating realistic images and has potential applications in various fields such as computer vision and natural language processing."
60,"Page 60 of the document discusses the use of deep learning for building vision systems, specifically the S-VSE model. This model utilizes representation learning, which involves learning features from data rather than relying on handcrafted features. The S-VSE model has been updated to version 2.0 and is being developed at the National University of Singapore. The author, Dr. Tian Jing, can be contacted via email for more information."
