Page,Summary
Page 1, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 2, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 3, Feature representation learning: Learn to mapfrom data space to representation space (foranalytics tasks) Supervised learning: Use data with (manually) human-annotated labels . Self-supervised learning : Use supervisory signals that
Page 4, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 5, The encoder takes an input image with missing missing regions and produces a latent feature representation of that image . The decoder takes this feature representation and produces the missing image content .
Page 6," A large random subset of image patches (e.g., 75%) is masked out . The encoder is applied to the small subset of visible patches . Masked autoencoder (MAE) is processed by a small dec"
Page 7," Images rotated by: 0Â°, 90Â°, 180Â°, and 270Â° . Task: Predict which rotation is applied; 4-category classification task. Assign a â€œlabelâ€ to each rotation . Assign â€˜"
Page 8," A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020, is a simple framework for contrastive learning of visual representations ."
Page 9, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 10, National University of Singapore/Building vision systems using deep learning (representation)/V2.0 Â© 2025 . S-VSE: Building vision systems with deep learning using deep-learning .
Page 11, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 12, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 13, DINO is popular for pre-training ViT models . S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 14," CLIP trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) examples . CLIP is trained to predict which of the  ğ‘ï¿½ï¿½"
Page 15, Application 2: Bi-directional multimodal multodal pre-training (CLIP) The CLIP is based on deep learning (representation/V2.0)
Page 16," Application 2: Encoder-decoder multimodal multi-modal . Flamingo: a Visual Language Model for Few-Shot Learning, NeurIPS 2022 ."
Page 17," Application 2: Encoder-decoder multimodal  Â  Â  Â  Â  Â  Â  Â  Â pre-training (SAM)Reference: Segment Anything, ICCV 2023, https://arxiv.org/abs/2304.02643 ."
Page 18," Application 2: Decoder-only multimodal pre-trainers (LLaVa) Large Language and Vision Assistant (LaVA) The LVA Assistant will train vision + language backbones frozen, train projection layer on Â "
Page 19," Using pre-trained multimodal models? Pre-trained CLIP model . Text prompt (designed by users, possible image recognition result)Identify the category of the input test image by  selecting the text prompt with the largest similarity"
Page 20, Application 3: Person re-identification (Re-ID) Re-ID: verify that two photos belong to the same person . S-VSE/Building vision systems using deep learning (representation)/V2.0 Â©
Page 21, Multiple classes (# classes = # persons) Classifier (either traditional machine  learning or deep learning methods) trained  from image dataset with annotation .
Page 22," The traditional approach for matching images, relies on the following pipeline: Feature extraction, e.g., color histogram, LBP, HoG, pre-trained CNN or ViT ."
Page 23," We have various pre-defined metrics, which are fully specified  without the knowledge of data . The Euclidian distance:   Â ï¿½ğ‘“  ğ² Â , Â Â - Â "
Page 24, Problem statement: Challenge in traditional image matching approach: The feature representation and the similarity metric are not learned jointly . Problem formulation: Evaluate how â€˜similarâ€™ a pair of images are to each other .
Page 25, Siamese network: Standard architecture . Forward pass using both inputs through the two backbone networks (sharableweights) Back propagate through two networks (the weights are updated with the sum of the two gradients)
Page 26, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 27, Siamese network: Summary: Builds vision systems using deep learning (representation)/V2.0 . Generate positive pair of photos (from  the same person) and negative pair of images (from different person)
Page 28," The task is similar to other object re-identification tasks . Luggage, Tiger and car are examples of non-human objects ."
Page 29, A Computer Vision-Based Yoga Pose has been developed by the National University of Singapore . S-VSE/Building vision systems using deep learning (representation/V2.0)
Page 30, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 31, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 32, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore. All Rights Reserved .
Page 33, A group of people are chained inside a cave and can only see two-dimensional shadows . The shadows are generated by three-dimensional objects . S-VSE/Building vision systems using deep learning (representation)/V2.0
Page 34, A visualization of latent Â space from MNIST can be visualized with a latent vector . Feed the latent vector to the trained decoder to generate an image . The latent vector is then fed to a trained image .
Page 35, The Autoencoder (AE) can be manipulated using an interpolated vector   ğ‘§ï¿½llğ‘ğ‘ (between two latent vectors  Â ï¿½ollï¿½ullï¿½
Page 36, Taxonomy of generative vision models . Quoted from Prof. Elad's course on Generative AI . References: https://vsehwag.io/blog/2023/2/all_papers_on_diff
Page 37, Generative Adversarial Network (GAN) learns to create images from random noise . Generator learns to mimic the distribution of real images . Discriminator learns between real and generated images .
Page 38," Generative Adversarial Networks, NeurIPS 2014, published by National University of Singapore . S-VSE/Building vision systems using deep learning (representation/V2.0)"
Page 39, Generative Adversarial Networks (G) uses deep learning (representation/V2.0) S-VSE/Building vision systems using deep-learning (representative) V2 .0 Â© 2025 National University of Singapore
Page 40, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore. All Rights Reserved .
Page 41, Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy . S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 42, VAE decoder can handle inputs sampled from the learned latent distribution Â rather than deterministic latent values as in an AE . VAE applies a distribution loss (KL divergence) to evaluate whether encoder's output
Page 43," Reparameterization trick: It allows differentiable sampling from a latent distribution by expressing the random latent variable as a deterministic.transformation of a noise (i.e., a random number) and the distribution parameters (e."
Page 44, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 45, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 46, Each point in this space represents a unique combination of pixel intensities across the uablyRGB channels for all pixels . The dimension of the color.inousimage space is 512 Ã— 512 .
Page 47, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 48," A pre-defined (fixed, non-learned) forward diffusion process is transforming a toy distribution in multiple steps from real image space to random image (noise) space . S-VSE/Building vision systems using deep learning"
Page 49," A decoder neural network, trained with shared parameters in a multi-step reverse process, re-formulating generative modeling into a set of supervised prediction tasks ."
Page 50, Each step performs a simple mixture mixture of previous state and noise . The noise level is 1-1 . The result is the result of the resulting noise level .
Page 51, Using deep learning (representation)/V2.0 Â© 2025 National University of Singapore. S-VSE/Building vision systems using deep learning .
Page 52," If youâ€™re curious about how to  retrieve the loss: Understanding Diffusion Models . A Unified Perspective, Â - a Unified Perspective ."
Page 53, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 54, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 55, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 56," A summary of the neural network perspective . A model backbone . Model backbone . The noise scheduler . Noise scheduler   Â setsuler Â nearlyÂ scheduledÂ (0,1), e.g"
Page 57," FID scores indicate that the images are closer in quality and diversity to the real ones . It calculates the distance between the feature distributions of the real and generated images, using a pre-trained Inception network to extract features ."
Page 58, National University of Singapore: Building vision systems using deep learning (representation/V2.0) S-VSE/Building vision systems . Using deep-learning (representative) and deep-learned (vision/vision)
Page 59, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore .
Page 60, S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore. All Rights Reserved .
Overall Summary, The notations and color codes used across multiple slides might be inconsistent . S-VSE/Building vision systems using deep learning (representation)/V2.0 Â© 2025 National University of Singapore.
