Page Number,Summary
1,The document discusses the use of deep learning and transformer-based techniques in building vision systems. It is a version 2.0 of the S-VSE (Scene-Vision-System-Engine) project developed by Dr. Tian Jing at the National University of Singapore. The document cautions that notations and color codes across multiple slides may vary.
2,The document discusses the use of deep learning with transformer-based architecture for building vision systems. It explains how attention and tokenization are used to process input images and make classification decisions. The reference cited is a paper on using transformers for image recognition at scale. This approach is expected to be utilized in 2025 by the National University of Singapore.
3,"The document discusses the concept of tokenization for building vision systems using deep learning with transformers. Tokenization involves breaking down input into smaller chunks and converting them into feature embeddings through a learnable transform. This can be applied to various types of data, including images, text, and audio."
4,"The document discusses the tokenization process in the S-VSE (Building Vision Systems using Deep Learning) project, which uses a transformer-based approach. This involves reshaping and transposing a matrix of tokens, with each token having a certain number of dimensions. The two main operations are a linear combination of tokens and a token-wise nonlinearity via a function. This process allows for efficient processing and analysis of visual data."
5,"The document discusses the use of deep learning with transformers in building vision systems. It mentions that convolution can be understood as attention, where the output is the inner product between a local patch and weights. There are two types of convolution: distance-based and intensity-based. Distance-based convolution uses a domain filter or Gaussian function of spatial distance, while intensity-based convolution uses a range filter or Gaussian function of intensity difference."
6,"The Squeeze and Excitation (SE) block is a component of the S-VSE vision system that helps to improve feature representation and increase model performance. It consists of two operations: squeeze, which uses global average pooling to condense feature maps, and excitation, which uses a fully connected network to learn channel dependencies and output attention weights. This allows the system to emphasize important channels and suppress less relevant ones. The SE block is based on a paper published in CVPR 2018 and is a key observation in the development of the S-VSE system."
7,The spatial attention module in the S-VSE system uses two pooling operations to aggregate channel information and generate two 2D spatial maps. These maps are combined and passed through a convolutional layer with a sigmoid activation to create a spatial attention map. This map highlights important spatial locations and is applied to the input feature maps through element-wise multiplication. This approach is based on the Convolutional Block Attention Module and was presented in the ECCV 2018 conference.
8,"The general attention mechanism, known as QKV, is a function that maps a query and a set of key-value pairs to an output. This function uses a compatibility function to assign weights to each value based on its corresponding key. The output is a weighted sum of the values. The query, keys, and values are all vectors, and the output is also a vector. The query vector represents a specific position in a sequence, while the key and value vectors represent positions in an arbitrary sequence. The output vector corresponds to the query position."
9,"The document discusses the general attention mechanism (QKV) used in S-VSE, a deep learning transformer-based vision system. This mechanism involves calculating attention weights, alignment scores, and output vectors based on a query and key-value pairs. The attention weights are calculated using the softmax function, while the alignment scores are determined by a scaled dot-product operation. This mechanism allows the system to focus on relevant information and produce accurate output vectors."
10,"The general attention mechanism, known as QKV, calculates attention weights and alignment scores based on the query, key, and value vectors. These weights are then used to calculate output vectors, which are a weighted sum of the value vectors. The alignment scores are determined by a function that takes in the query and key vectors. The output vectors are calculated by multiplying the attention weights with the value vectors. The query, key, and value vectors are all in a specific dimension, and the alignment scores are determined by a scaled dot-product function."
11,"The attention mechanism in deep learning is represented by the QKV (query, keys, values) matrix. The output matrix is represented by AV, and the alignment scores are calculated using the scaled dot-product. The attention matrix is then calculated using the softmax function and the QK transpose matrix. This matrix is then multiplied by the values matrix to obtain the final output. The notation for this process is represented by A = softmax(QK^T/K)V."
12,"The attention mechanism, specifically self-attention, is an important component of building vision systems using deep learning with transformers. It involves three trainable matrices, ğ‘¾ğ‘¾ğ‘ğ‘, ğ‘¾ğ‘¾ğ‘˜ğ‘˜, ğ‘¾ğ‘¾ğ‘£ğ‘£, and the embedding size of tokens ğ‘¿ğ‘¿ğ‘›ğ‘›. The formula for self-attention involves ğ‘¸ğ‘¸ğ‘²ğ‘²ğ‘‡ğ‘‡ and ğ‘¾ğ‘¾ğ‘ğ‘, ğ‘¾ğ‘¾ğ‘˜ğ‘˜, ğ‘¾ğ‘¾ğ‘£"
13,"âˆ’âˆ 0 âˆ’âˆ âˆ’âˆ âˆ’âˆ âˆ’âˆ âˆ’âˆ 0 âˆ’âˆ âˆ’âˆ âˆ’âˆ âˆ’âˆ 0.

The attention mechanism in deep learning systems uses masked self-attention, where certain tokens are purposely omitted from contributing to attention weights. This is represented by a mask matrix ğ‘´ğ‘´, where omitted entries are given a value of âˆ’âˆ. For example, if ğ‘˜ğ‘˜3 is omitted, the mask matrix would be [REDACTED_PHONE] âˆ’âˆ âˆ’âˆ âˆ’âˆ âˆ’âˆ 0 âˆ’âˆ âˆ’âˆ âˆ’âˆ âˆ’âˆ âˆ’âˆ 0 âˆ’âˆ âˆ’"
14,"The attention mechanism in deep learning can be categorized into self-attention and cross-attention. In self-attention, the keys, values, and queries are all derived from the same source. In cross-attention, the keys, values, and queries are derived from separate sources, such as channels, branches, or frames. The calculation of cross-attention involves the use of keys, values, and queries from both sources to produce a new output."
15,"ğ‘¾ğ‘¾ğ‘œğ‘œ ğ‘‘ğ‘‘â„

Multi-head self-attention is a mechanism used in deep learning for building vision systems using transformers. It involves projecting inputs and outputs with different learned linear projections and performing parallel attentions in multiple heads. To maintain the input and output dimensions, the embedding size is divided by the number of heads. Trainable matrices are used for the projections and the softmax function is applied to the resulting outputs."
16,"This section discusses the use of attention calculations in building vision systems using deep learning with transformers. It explains the three types of attention: channel attention, spatial attention, and self-attention. Each type has a different input and output calculation, with examples provided. The concept of cross-attention is also introduced, which involves using a different query input (Y) to calculate the attention output (Z) based on the key and value inputs from the original image (X). A summary table from GPT-4 is included for reference."
17,"The document discusses various attention mechanisms in computer vision, which are used to dynamically select and weight features in a vision system. These mechanisms include channel attention, spatial attention, temporal attention, branch attention, and combinations of channel and spatial attention, as well as spatial and temporal attention. These mechanisms are used to identify and focus on important features in a given input feature map."
18,"The toy experiment aims to train a sequence-to-sequence mapping model using deep learning techniques. The input consists of four types of waves with fixed width, random height, and center, with no overlapping. The output is the average height of the same type of two waves. The dataset includes 1000 sequences, with an 80/20 train-test split. The model is evaluated using the mean squared error loss function."
19,"This section discusses a toy experiment using a 1D CNN model with self-attention for building vision systems using deep learning. The experiment involves training the model on a dataset and evaluating its performance on a test set. The results show that the model with self-attention outperforms the regular 1D CNN model, indicating the effectiveness of incorporating self-attention in deep learning for vision tasks."
20,The document describes a toy experiment using 1D CNN models with and without self-attention. The MSE (mean squared error) for the model with self-attention was significantly lower (0.5537) compared to the model without self-attention (1.9508). This suggests that incorporating self-attention can improve the performance of deep learning models in building vision systems.
21,"The vision transformer (ViT) is a deep learning model used for image recognition. It involves splitting an image into patches and embedding each patch. A [class] token is prepended and positional information is added to the embedded patches. The resulting sequence of vectors is then fed into a stack of standard Transformer Encoders. The output from the [class] token section of the Transformer Encoder is extracted and fed into a classification head to obtain the final output. This method was described in a paper titled ""An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"" published in 2021."
22,"ViT (Vision Transformer) is a deep learning algorithm used for building vision systems. It works by transforming each patch of an input image into a vector representation, with a patch size of 16x16 and a total of 196 patches for a 224x224x3 image. This transformation is achieved through a convolutional layer, followed by a linear projection using a multi-layer perceptron (MLP). There are two options for implementing this process: using a single convolutional layer or using a combination of a convolutional layer and an MLP."
23,"The ViT (Vision Transformer) model for building vision systems using deep learning includes a class token, which is a learnable embedding added to the sequence of embedded patches. Position embeddings are also added to the patch embeddings to preserve positional information. The input to the transformer block is a combination of the patch embedding, position embedding, and class token. This helps the model to better understand the spatial relationships between different parts of an image."
24,"The ViT model uses a transformer-based approach to build vision systems using deep learning. This involves repeating the transformer encoding process L times, with each repetition including a residual connection, a multi-layer perceptron (MLP) with 2 layers, layer normalization, and a multi-head self-attention with h heads. The layer normalization operates within each token, normalizing the feature values of a single token using the mean and variance of its features. The final output is a matrix with dimensions of (n+1) x d, where n is the number of tokens and d is the dimensionality of the features."
25,"The ViT (Vision Transformer) model uses a [class] token from the Transformer Encoder output as a learned summary for the entire sequence of tokens. This [class] token is then fed into a classification head, consisting of a layer normalization and a MLP, to produce the final output. This approach allows for fine-tuning for downstream tasks, such as 1000 category classification in ImageNet."
26,"The ViT (Vision Transformer) model has a total of 86 million parameters, with the majority of them coming from the patch embedding layer (590,592 parameters) and the multi-head attention (MHA) layer (1,771,776 parameters). The model also includes layer normalization, a class token, and an MLP classification head. The discrepancies between this information and the online reference are the number of parameters in the MHA layer (1,771,776 vs 1,772,160) and the total number of parameters (86,567,656 vs 86,601,768). These differences may be due to rounding or slight variations in the model architecture. Model B, with a patch size of 16 and"
27,"The Swin Transformer is a deep learning model for building vision systems using transformer-based techniques. It splits an RGB image into patches and treats each patch as a ""token"" with a feature dimension of 48. A linear embedding layer is then used to project the features to an arbitrary dimension. This method was introduced in a research paper titled ""Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"" published in ICCV 2021."
28,The Swin Transformer is a deep learning model used for building vision systems. It involves dividing the input image into smaller patches and processing them in parallel. The patches are then merged using linear projection and concatenation to provide a multi-scale representation. This approach allows for efficient processing of large images and improved performance in tasks such as image classification.
29,"FLOPs.

The Swin Transformer uses a Window MSA method to compute attention within non-overlapping windows, reducing computational cost. This method involves computing attention for each window separately, resulting in a cost of 4â„ğ‘¤ğ‘¤ğ¶ğ¶2 + 2(â„ğ‘¤ğ‘¤)2ğ¶ğ¶. This is a significant cost saving compared to applying MSA on the entire feature map, which would result in a cost of 4â„ğ‘¤ğ‘¤ğ¶ğ¶2 + 2(â„ğ‘¤ğ‘¤)2ğ¶ğ¶. For example, if â„ ="
30,"The Swin Transformer is a deep learning model used for building vision systems. It uses a window-based self-attention module, but this has limitations due to lack of connections across windows. To address this, a shifted window partitioning approach is proposed, where the window is shifted by half its size and masked multi-head self-attention is performed within each window. This allows for cross-window connections while maintaining efficient computation."
31,"The document compares the use of Convolutional Neural Networks (CNNs) and Transformers in building vision systems using deep learning. While CNNs can only use information from neighboring pixels, Transformers allow for the combination of information from distant patches through the use of positional embedding. However, for pure vision tasks, both CNNs and Transformers perform similarly. A reference is provided for further information on the topic."
32,The hybrid model of ViT and CNN has been found to improve the performance of transformer-based vision systems. This approach was proposed in a 2021 study and has been shown to enhance the ability of transformers to process visual information. This hybrid model has the potential to improve the overall performance of deep learning vision systems.
33,"The ViT (Vision Transformer) model relies on non-local relationships between image patches, which allows it to perform well even when the image context is altered. This is in contrast to CNNs like ResNet, which use local receptive fields and spatial hierarchies that make them less compatible with techniques that disrupt spatial structure, such as masking or patch-specific transformations. To train a ViT model effectively, data, augmentation, and regularization techniques should be considered, such as RandAugment."
34,"The page discusses the scaling law for building vision systems using deep learning with transformer-based models. It mentions three different models (Tiny ViT-Ti/16, Base ViT-B/32, and Large ViT-L/16) and their corresponding model sizes and number of images seen during training. The detailed configurations can be found on GitHub and the reference is a paper on scaling vision transformers."
35,"The document discusses ViT finetuning, which is a strategy for building vision systems using deep learning and transformers. This involves using a patch embedding layer, layer normalization, multi-head attention, and a multi-layer perceptron with a linear classifier. The strategy involves selecting a subset of parameters to fine-tune, such as only the top MLP head for classification, while keeping the rest frozen. A reference for this strategy can be found on GitHub."
36,"The ViT finetuning method involves using a patch embedding layer, layer normalization, multi-head attention, and a linear classifier to fine-tune a transformer encoder. This is done by adding adapter layers with few parameters and only tuning the adapter layer, while keeping other parameters frozen. This strategy is based on a parameter-efficient transfer learning method for natural language processing."
37,"The document discusses the ViT finetuning process, which involves adding weights in parallel to the frozen pre-trained weights in the attention module. This strategy, called LoRA, is only experimentally applied to the Query and Value components. The Multi-head attention layer is also modified with LoRA, resulting in a combination of frozen and trainable parameters. This approach is based on the concept of Low-Rank Adaptation of Large Language Models and has been presented in a research paper. The original attention block is compared to the modified attention block with LoRA."
38,"The ViT finetuning process involves using LowRank Adaptation (LoRA), where the original pretrained parameters are kept fixed while only learning an additive modification to these parameters with fewer parameters. The trainable parameters are initialized to a normal distribution with mean 0 and variance 2. This is represented by a LoRA linear layer, where the original linear layer is modified by adding the trainable parameters. The reference for this method is a paper titled ""LoRA: Low-Rank Adaptation of Large Language Models""."
39,"The appendix provides a reference to a survey on visual transformers, which are a type of deep learning algorithm used for building vision systems. The survey discusses the current state of visual transformers and their potential applications. It also highlights the challenges and limitations of using this technology. The appendix is copyrighted by the National University of Singapore and includes a link to the survey for further reading."
40,"The appendix provides a reference for a survey on efficient vision transformers, a type of deep learning algorithm used in building vision systems. This survey covers various techniques and benchmarks for measuring performance, and is published by the National University of Singapore in 2025. The link to the survey is provided, but the phone number is redacted."
41,"The article discusses the use of Convolutional Neural Networks (CNN) and Vision Transformer (ViT) for building deep learning vision systems. It presents a guide for practitioners to choose between the two based on certain factors such as dataset size, model efficiency, availability of pretraining data, and likelihood of data drift at deployment. It suggests starting with ViT for large datasets and time-sensitive projects, and trying CNN if there is time."
42,"The idea of using a transformer-based approach for object detection involves using a transformer backbone, such as Swin Transformer and Mask R-CNN, to extract features. The transformer neck-based detector then uses a set of learnable object queries to determine class labels and bounding box coordinates. This approach does not change the backbone used for feature extraction. A reference for this approach can be found in the paper ""Toward transformer-based object detection."""
43,"The document discusses a transformer-based approach to object detection, specifically the idea of using a transformer neck, such as DETR. This approach uses a CNN backbone to extract features from an image and then applies an encoder-decoder transformer and a feed forward network to make the final detection prediction. This method has been shown to be effective in end-to-end object detection, as demonstrated in a 2020 paper."
44,") The DETR model uses a CNN backbone, specifically the ResNet-50 model, to generate a feature map from the input image. 2) The feature map has a channel dimension of 2048 and a size of ğ»ğ»0 32 x ğ‘Šğ‘Š0 32. 3) A 1x1 convolution is then used to reduce the channel dimension to a smaller size, resulting in a new feature map with dimensions of ğ»ğ»Ã—ğ‘Šğ‘ŠÃ—ğ‘‘ğ‘‘. This backbone is used in the DETR model for object detection and recognition tasks."
45,"DETR (2) introduces the concept of positional encoding in building vision systems using deep learning. This involves generating a 2D positional encoding matrix of size ğ»ğ» Ã— ğ‘Šğ‘Š Ã— ğ‘‘ğ‘‘, where each position in the grid has a unique encoding of size ğ‘‘ğ‘‘. This is achieved by encoding the row and column positions separately and then concatenating them to get a ğ‘‘ğ‘‘-dimensional encoding for each position. The formula used for encoding is sin or cos of ğ‘€ğ‘€ 100002ğ‘¦ğ‘¦ğ‘‘ğ‘‘ğ‘šğ‘šğ‘šğ‘šğ‘š"
46,"The DETR (3) model uses a 3-layer perceptron with ReLU activation function and a linear projection layer to make predictions for normalized center coordinates, height, width, and class label of objects in an input image. It also has 3 object queries, each with learnable parameters for the number of queries and object classes. These parameters are represented as â„ğ‘ğ‘ğ‘ğ‘Ã—ğ‘‘ğ‘‘, â„ğ‘ğ‘ğ‘ğ‘Ã—ğ‘ğ‘ğ‘ğ‘, and â„ğ‘ğ‘ğ‘ğ‘Ã—4, respectively."
47,"The DETR (4) loss function for building vision systems using deep learning (transformer) is composed of two components: the classification cost, which measures the difference between predicted class probabilities and ground truth labels, and the bounding box cost, which combines the L1 distance and generalized intersection over union (IoU) between predicted and ground truth bounding boxes. The generalized IoU is calculated as the intersection over union between two boxes, and the loss is then calculated as 1 minus the generalized IoU. The generalized IoU loss also takes into account the area outside the union of the two boxes but inside the smallest enclosing box. This loss function is based on the paper ""Generalized Intersection over Union: A Metric and A Loss for Bounding"
48,"DETR (Detection Transformer) is a deep learning model for object detection that uses a transformer-based architecture. Unlike traditional methods, DETR does not rely on box regression, box labeling, or non-maximum suppression (NMS) for object detection. Instead, it uses a fixed set of non-geometric ""object queries"" for classification and box prediction. This approach was discussed by Ross Girshick, creator of the R-CNN series, in a CVPR 2020 tutorial."
49,"The appendix discusses the DETR model and its variants, which are used for object detection using transformers in deep learning. DETR stands for ""Detection Transformer"" and is a popular model for object detection tasks. It works by predicting a set of bounding boxes and their corresponding class labels in a single pass, making it more efficient compared to traditional methods. The appendix also provides a reference to a review paper on object detection with transformers."
50,"This appendix discusses the use of transformers in small object detection, providing a benchmark and survey of current state-of-the-art methods. It highlights the challenges and limitations of using transformers in this task, such as the need for large datasets and computational resources. The appendix also provides recommendations for future research and development in this area."
51,"The transformer-based semantic segmentation approach, SETR, rethinks the traditional method of semantic segmentation by using a sequence-to-sequence perspective. This involves a decoder that generates segmentation results in the original 2D image space, based on the features extracted by the encoder. This approach was presented in CVPR 2021 and can be referenced in the provided link."
52,"The decoder design for SETR-Naive model includes a 1x1 convolution layer with BatchNorm and ReLU, followed by another 1x1 convolution layer and bilinear upsampling. This is then followed by a Transformer block. The input image is resized to a dimension of 16x16x1024 for SETRmlr model and 19 channels for the Cityscapes dataset. A naive upsampling method is used for reshaping the output. This design is based on the SETR.py code from the GitHub repository."
53,"The decoder design for the SETR-PUP model involves a series of convolution and transformer blocks, with the input image being converted to a smaller size and then gradually upscaled using progressive upsampling (PUP). The SETR model has a channel size of 1024 and is trained on the Cityscapes dataset with a class number of 19. The progressive upsampling is achieved through 4 convolution operations. The code for this design can be found on GitHub."
54,"4

The SETR-MLA decoder design involves a transformer block with input image dimensions of H x W x 3. The 24-th layer output is reshaped and combined with the 18-th, 12-th, and 6-th layer outputs through a series of 1x1 and 3x3 convolutions. This is followed by four up-sampling layers and a final 1x1 convolution to produce a feature map with dimensions H/4 x W/4 x C, where C=1024 for the SETR-MLA model. The feature map is then concatenated with a multi-level feature aggregation (MLA) and used as input for the final 1x1 convolutional layer to produce"
55,"The decoder design for the SegFormer vision system consists of four transformer blocks, with each block having a different size and number of channels. The input image is passed through four 1x1 convolution layers before being concatenated with the output of the transformer blocks. The concatenated output is then upsampled and passed through another 1x1 convolution layer. This design is based on the SegFormer model, which uses transformers for efficient semantic segmentation. The number of object classes is determined by the last layer's number of channels."
56,"The meta architecture for detection and segmentation using deep learning with transformers consists of several key components: a backbone network, which extracts features from the input image; a neck network, which further refines the features; an object query mechanism, which assigns a query vector to each potential object in the image; a decoder network, which uses the query vectors to generate object-specific features; and a mask output network, which produces the final segmentation mask for each object. This architecture is described in more detail in the referenced paper."
57,"The Safe Distance @ Parks program implemented by National Parks in Singapore has eliminated the need for staff to manually count crowds, allowing them to focus on other tasks and engage with the public. A web portal has been created to provide the public with visibility of park crowd conditions for self-management. This program has been successful in improving efficiency and promoting self-regulation in park visitors."
58,"The objective of this challenge is to use deep learning techniques, specifically transformers, to track the amount of food and beverage wastage on flights. This will help collect data on passengers' consumption patterns more efficiently. The challenge is hosted by Singapore Airlines and the National University of Singapore."
59,"This page discusses future developments in the use of deep learning and transformer-based technology for building vision systems. Specifically, it mentions the potential for using computer vision to detect missing barricades and the possibility of automation in construction by November 2021. These advancements are being developed by the National University of Singapore and are expected to improve the accuracy and efficiency of building vision systems."
60,"The document discusses the future direction of S-VSE, a building vision system that uses deep learning with transformers. It mentions the development of a block progress monitoring feature, which will use image segmentation to track the progress of construction. This feature will be useful for construction companies and project managers to monitor and manage their projects more efficiently."
61,"The next step in building vision systems using deep learning is crowd counting, which can be done through detection-based methods, regression-based methods, or density estimation-based methods. Detection-based methods involve identifying people in an image and counting them, while regression-based methods involve extracting low level features from cropped patches of the image. Density estimation-based methods create a density map for objects and then use an algorithm to learn a mapping between the extracted features and their object density maps. A useful reference for this topic is ""CNN-based Density Estimation and Crowd Counting: A Survey."""
62,"The document discusses the use of deep learning (specifically transformer-based models) for building vision systems. It mentions the next steps for this technology, specifically in the area of crowd counting. This includes evaluating the performance of the counting by looking at the image-level, pixel-level, and point-level. The document also mentions the use of manual annotation for human heads in order to calculate density maps."
63,"The page discusses the future direction of open-world detection in vision systems using deep learning with transformers. It references a paper on simple open-vocabulary object detection with vision transformers, which will be presented at ECCV 2022. This approach aims to improve the ability of vision systems to detect and classify objects in open-world scenarios where new objects may appear that were not seen during training."
64,"The document discusses the use of deep learning with transformer-based models in building vision systems. It is the second version of the S-VSE (Simplified Vision System Engineering) and is copyrighted by the National University of Singapore. The author, Dr. Tian Jing, can be contacted via email for further information."
