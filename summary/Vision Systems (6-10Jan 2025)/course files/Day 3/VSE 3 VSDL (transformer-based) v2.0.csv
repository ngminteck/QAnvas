Page,Summary
Page 1,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 2,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 2 Overview of Vision Transformer Attention + Tokenization Input image Classification
Page 3,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 3 Tokenization .
Page 4,"Tokenization Project Reshape Transpose Rnndd: nn tokens, each has dd dimensions Operation 1: Linear combination of tokens TToooooo[ii,] ="
Page 5,"convolution can be explained as attention Convolution: the output is the inner product between the local patch and the weights . distance-based (e.g., domain filter, Gaussian function of spatial distance) (rr"
Page 6,Squeeze: A global average pooling operation condenses each feature map into a single value . a two-layer fully connected network learns channel dependencies and outputs attention weights for each channel .
Page 7,spatial attention module Spatial attention map . maps are concatenated and passed through a convolutional layer with a sigmoid activation to generate the spatial attention map.
Page 8,attention function maps a query and a set of key-value pairs to an output . the output is computed as a weighted sum of the values . a compatibility function of the query with the corresponding
Page 9,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 10,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 10 General attention mechanism (QKV) zz1 zz2
Page 11,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 11 .
Page 12,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 12 Attention mechanism: Self-attention Three trainable matrices WW
Page 13,"mask matrix MM contains elements with a value of 0 for included entries . if kk3 is omitted, then MM = [REDACTED_PHONE] ."
Page 14,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore . all rights reserved Page 14 Attention mechanism: cross-attention . keys-values and
Page 15,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 15 Attention mechanism: Multi-head self-attention Embedding size
Page 16,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore . Attention type Query (QQ) Key (KK) Value (VV) Attention calculation
Page 17,attention mechanism in a vision system can be treated as a dynamic selection process that is realized by adaptively weighting features according to the importance of the input feature map . channel attention Generate attention mask across the channel domain and use it
Page 18,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 18 A toy experiment .
Page 19,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 20,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 20 A toy experiment A 1D CNN model MSE: 1.95
Page 21,vision transformer (ViT) 2025 National University of Singapore . all rights reserved. page 21 Overview of vision transformer .
Page 22,image (HH WW 3): 224 3 • Patch size (MM): 16 16 • # patches (nn) • Sequence of embedded patches: nn dd
Page 23,Class token: Prepend a learnable embedding to the sequence of embedded patches . dd (nn+ 1) dn . Class token + = Input to transformer block .
Page 24,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 25,a layer normalization followed by a MLP to perform classification for nnn_ccMMMMssseess . it can be fine tuned for the downstream task . 1000 category classification in ImageNet R
Page 26,"there are a few discrepancies between this slide and the online reference, even though they have the same total amount . ddh h = 1,771,776 (dd h + 1)"
Page 27,"Swin Transformer (1) Patch partition and linear embedding: it first splits an input RGB image into non-overlapping patches by a patch splitting module, like ViT . each patch is treated as a “token” and"
Page 28,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 29,"it computes attention only within each non-overlapping window (MM MM each), h MM ww MM windows in total for the whole feature map . it saves 15,091,034,112"
Page 30,"the window-based self-attention module lacks connections across windows, which limits its modeling power . shifted window partitioning approach alternates between two partitioning configurations in consecutive Swin Transformer blocks ."
Page 31,CNNs can only use information in neighboring pixels/cells . the receptive field (pixel area considered) grows larger as network gets deeper .
Page 32,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 33,"ViTs rely on non-local relationships between patches, which allows them to perform well even when the spatial continuity or the full image context is altered . CNNs like ResNet are designed with local receptive fields and spatial"
Page 34,"Models # Model size # images seen Tiny ViT-Ti/16 37 MB # images during the training, not the dataset size ."
Page 35,ViT finetuning (1) Patch embedding layer Norm Multi-head attention + Layer Norm MLP + Linear classifier LL Transformer encoder Strategy (subset) Frozen params Trainable par
Page 36,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 36 ViT finetuning (2) Patch embedding Layer Norm
Page 37,layer Norm Multi-head attention + Layer Norm MLP + Linear classifier LL Transformer encoder Strategy (LoRA): Add weights in parallel to the frozen pre-trained weights . original paper
Page 38,"(4) LowRank Adaptation (LoRA): Keep the original pretrained parameters WW0 fixed during the fine-tuning . only learn an additive modification to these parameters AA and BB, which have fewer"
Page 39,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 40,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 40 Appendix: Other vision transformer Reference: a survey on efficient
Page 41,"cnn-vs-vit.htmlAre you training on a very large dataset (10M+ images)? ViT CNN Is model efficiency important to you (e.g., real-time of embedded application"
Page 42,the Transformer neck-based detector infers the class labels and bounding box coordinates with a set of learnable object queries . it does not change the backbone used for feature extraction .
Page 43,Transformer backbone-based detectors propose a generic visual backbone that flattens the image into a sequence instead of convolution for feature extraction . detection TRansformer uses a CNN backbone to extract
Page 44,"a backbone generates a feature map RHHWWCC, such as CC = 2048, HH = HH0 32, WW = WW0 32 . a 1 1 convolution"
Page 45,"each position in the HH WW grid needs a unique encoding of size dd . for a position (rr, cc) and a dimension index yy [0,"
Page 46,the final prediction is computed by a 3-layer perceptron with ReLU activation function and a linear projection layer . the linear layer predicts the class label using a softmax function .
Page 47,"LL1 loss measures the absolute differences between the coordinates and dimensions of the predicted and ground truth bounding boxes . IIMMII AA, BB = |AABB| |AA'BB| GGIIMMI"
Page 48,DETR (5): A debate Detection = the classification and box prediction of a fixed set of non-geometric “object queries”.
Page 49,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 50,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 50 Appendix: Small objects Reference: A Benchmark and Survey of
Page 51,the goal of the decoder is to generate segmentation results in the original 2D image space from the encoder's features .
Page 52,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore . all rights reserved Page 52 Decoder design (1) .
Page 53,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 54,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 54 Decoder design (3).
Page 55,"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers, NeurIPS 2021, https://arxiv.org/abs/[REDACTED_PHONE] NNc"
Page 56,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 57,Counting Source: https://garuda.io/safe-distancing-nparks/ Safe Distance @ Parks . online portal provides visibility of park crowd conditions .
Page 58,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore . Counting Source: https://singaporeair.agorize.com/
Page 59,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 60,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 60 What is next: Block progress Monitor the construction progress via image segmentation
Page 61,Crowd counting and Detection-based methods . we first create a density map for the objects . algorithm learns a mapping between the extracted features and their object density maps .
Page 62,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore. All Rights Reserved Page 62 What is next: Crowd counting • Image-level for
Page 63,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Page 64,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore .
Overall Summary,S-VSE/Building vision systems using deep learning (transformer)/V[REDACTED_PHONE] National University of Singapore . All Rights Reserved Page 1 BUILDING VISION SYSTEMS USING DEEP LEARNING (2) TRANSFORMER-BASED Dr TIAN Jing [REDACTED_EMAIL] Note: notations and color codes used across multiple slides might be inconsistent .
