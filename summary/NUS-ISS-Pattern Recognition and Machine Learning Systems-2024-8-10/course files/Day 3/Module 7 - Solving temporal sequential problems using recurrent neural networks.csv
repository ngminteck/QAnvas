Page,Summary
Page 1,NUS-ISSPattern Recognition using Machine Learning System 2024 National University of Singapore. All Rights Reserved.1of 29 .
Page 2,When time is a factor
Page 3,2024 National University of Singapore . all rights reserved.prumls/y2024/v1.13of 29 applicationswith time Source: http://www.cvisiontech.com .
Page 4,"the nets introduced assume inputs are independent from each other . but for some tasks/problems, this is not true . you better know what have been typed/ said before ."
Page 5,the output of a current element/ segment depends on the outputs from the previous elements/segments‘This’‘‘is’Source: https://adventuresinmachinelearning.com/recurrent-n
Page 6,CNN vs RNNComparison 2024 National University of Singapore. All rights reserved.prumls/y2024/v1.1 RNN / LSTMCNNSuitable for temporal data (s
Page 7,"we would like to predict the next value0, 1, 1, 3, 2, 2, 2, 4, 5, 4, 7, 8, 8, 9,? 2024 National University of Singapore."
Page 8,"we chop the series into segments, each with a fixed amount of time steps . length 2024 National University of Singapore ."
Page 9,"makeStepsThe procedure: 2024 National University of Singapore . preprocessed input: 931,1,0,42,2,3,74,5,4,98,8,7."
Page 10,preprocessed input:segment 3not sufficient length to form segmentmakeStepsThe procedure 2024 National University of Singapore .
Page 11,"signature0, 1, 1, 1, 3, 2, 2, 4, 5, 4, 7, 8, 8, 923,1,1,0,31,1,0 . the procedure>defmakeSteps(dat, length, dist):•length"
Page 12,the output feature size Dot productThe workingW x=w11w12w1nx1 w21w22w2nx2 . wm1wm2
Page 13,2024 National University of Singapore . all rights reserved.prumls/y2024/v1.1 Wx1•Assume .
Page 14,"the workingW=[REDACTED_PHONE][2 0,0 1,1]Wx1=[21 0 1,0 1]0113210 + 01 + 1"
Page 15,the length ofvandxmust be equalMultiplicationElement-wisevx=v1 v2vn .
Page 16,"we feed the input segment by segment or row by row0, 1, 1, 3, 23, 2, 2, 4, 54, 5, 4, 7, 8RNNThe workingx1x2x3Ws1=Wx1activationx1"
Page 17,"the same U and W forx1,x2, x3•U and W are changedonlyduring updating phase of training•U is a square matrixx3RNNThe workingWs1=Wx1activation"
Page 18,"vanishing gradientcomes in rnn 2024 National University of Singapore . the more we perform recurrent operation,mathematically, that is equivalent to adding more layers to the net ."
Page 19,RNNHow to build rnn in Keras>fromtensorflow.keras.layersimportSimpleRNN . what is the length of each segment?
Page 20,rnn builds in KerasInputimporttensorflow.keras.layers>fromDenseimportedRNNimport TensorFlow . kremls: r
Page 21,"LSTM 2024 National University of Singapore. All rights Reserved.prumls/y2024/v1.1Source: ""Deep Learning with Python"" by Francois Chollet ."
Page 22,"the internal working of 'carry' carryci i=(Uwhi1+Wwxi+bw) ci=ii1,ci2i"
Page 23,LSTM cell 2024 National University of Singapore . all rights reserved.prumls/y2024/v1.1 Source: https://medium.com/analytics-vidhya/l
Page 24,LSTMHow to build lstm in KerasInputimporttensorflow.keras.layers>fromDenseimport TensorFlow .kerneas-layers
Page 25,it is possible to stack recurrent layers and make the net deeper•Superscript denotes LSTM layer . Subscript time stepStackingTime step 1LSTM 1 h11LSTM 2 x1 c
Page 26,Subscript denotes LSTM layer; Time stepStackingTime step 2 . 1h12LSTM 2x2c12h22c22 . Dense y 2024 .
Page 27,Subscript denotes time stepStackingTime step 3 LSTM 1 h11 c11 h21 c21LSTM1h13LSTM 2x3h23Dense y 2024 National University
Page 28,"for LSTM 2 to dense, only the final output sequence fed into Dense layerStackingOn the output to Densese LSt 1h11 LSMC 2x1 c11 h21"
Page 29,"return_sequences must be set to True05)3,(none,input_3 (InputLayer)3647)3, (None,lstm_1 (LSTM)6129"
Overall Summary,2024 National University of Singapore . All Rights Reserved.prumls/y2024/v1.14of 29 Recurrent neural network . a net that perform the same calculation on elements/segments from a sequence .
