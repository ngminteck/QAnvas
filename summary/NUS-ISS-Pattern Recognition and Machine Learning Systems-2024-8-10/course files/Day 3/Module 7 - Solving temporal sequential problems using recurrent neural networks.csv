Page,Summary
Page 1,All Rights Reserved.Module 7 - Solving temporal sequential problems using recurrent neural networks by Dr. Gary Leung .
Page 2,When time is a factor
Page 3,ocr-neural-networks-and-other-machine-learning-techniques.html 2024 National University of Singapore.
Page 4,http://www.androidpolice.com/2015/11/06/swiftkey-update-to-v6-0-with-redesigned-settings-menu-double-word-prediction-and
Page 5,Recurrent neural network: a net that perform the same calculation on elements/segments from a sequence .
Page 6,CNN vs RNNComparison 2024 National University of Singapore. All Rights Reserved.
Page 7,"time stepPrediction•Suppose we have a series of 13 values, and we would like to predict the next value0, 1, 1, 3, 2, 2, 4, 5, 4, 7, 8, 8, 9."
Page 8,"we chop the series into segments, each segment with a fixed amount of time steps, called length 2024 National University of Singapore ."
Page 9,"makeStepsAssume we want to have a lengthof 4, and a distance of 30, 1, 1, 2, 2,4, 5, 4,segment 3segement 47, 8, 8, 931,1,0,"
Page 10,"if we want to have a length of 5, and a distance of 34, 5, 4, 7, 8, 8, 9segment . 20, 1, 1, 3, 2, 2,23,1,0,54,2,2,3,87,4,5"
Page 11,"Write a function that can generate the desired preprocessed input from a 1D series/signals with the below signature . the procedure>defmakeSteps(dat, length, dist):•"
Page 12,the workingW x=w11w12w1nx1 w21w22w2nx2 wm1wm2wmnxn =w11x1
Page 13,2024 National University of Singapore. All Rights Reserved.prumls/y2024/v1.1 Wx1•Assume•and we knowDot productThe workingW=1 0
Page 14,all rights reserved.prumls/y2024/v1.11=[20 + 01 + (1)3 + 12] 14of 29 students at the national university of Singapore have
Page 15,the output of element-wise multiplication is: v1 v2vn x1 x2xn =v1x1v2x2 2024 National University of Singapore.
Page 16,"•For RNN, the we feed the input segment by segment or row by row0, 1, 1, 3, 23, 2, 2, 4, 54, 5, 4, 7, 8RNNThe workingx1x2x3Ws1=Wx"
Page 17,"the same U and W forx1,x2, x3•U and W are changedonly during updating phase of training . rnn 2024 National University of Singapore."
Page 18,"rnn's vanishing gradient is equivalent to adding more layers to the net . the more we perform recurrent operation, the more layers we add . all rights reserved ."
Page 19,rnn is a web-based programming language developed by the national university of singapore . it can be used to build a single layer of a multi-layer RNN framework .
Page 20,"rnn can be built in kerasInputimporttensorflow .layers from SimpleRNN . simpleRNN (32)(inputs)= Dense(1, activation"
Page 21,'Deep Learning with Python' by Francois Chollet . source: https://y2024/v1.1Source: LSTM 2024 National University of Singapore .
Page 22,"the internal working of 'carry' carryci is the sigmoid function . let's denoteas element-wise multiplication, and we have 2024 National University of Singapore ."
Page 23,https://medium.com/analytics-vidhya/lstms-explained-a-complete-technically-accurate-conceptual-guide-with-ker
Page 24,LSTMHow to build lstm in KerasInputimporttensorflow.keras.layers>fromLstmpimporttense.ker.models> fromModelim
Page 25,it is possible to stack recurrent layers and make the net deeper . Dense y 2024 National University of Singapore.
Page 26,Dense y 2024 National University of Singapore. All Rights Reserved. LSTM 1 26of 29 .
Page 27,•Superscript denotes LSTM layer; Subscript denots time stepStackingTime step 3 . •LSTM 1 27of 29 .
Page 28,LSTM layer 1h11 c11 h21 c21 lstm 2x2 c12 h22 c22 LStm 1h13 ssm2x3h23
Page 29,"lstm output shape is(none, 3, 7) because of returned sequences, otherwise it will simply be(None, 7)"
Overall Summary,all Rights Reserved.prumls/y2024/v1.14of 29 LSTM vs recurrent neural network . a lstm that returns sequences is always equal to the number of rows of the input to the unit .
