Page,Summary
Page 1, Module 6 - Case studies on using convolutional neural networks for machine learning systems . NUS-ISSPattern Recognition using Machine Learning System .
Page 2,The long march to the deepest network
Page 3," The deeper the net, the harder the harder totrain; sometimes not possible totrain . Why deep learning came so late?"
Page 4, A simple example: What is being back-propagated in backpropagation? P1p2p3p4p5q1q2q3q4q5q6q7q8q9q
Page 5," Backpropagation from the last layer . Assume errors at last layer is δt,Θt,  ΘT, ΘD,   ; Assume the errors at the"
Page 6, 1 1w: Weight connect neuron sand ts t1 1Backpropagation Weight connection at the last layer .
Page 7," The error ats11(1) is the activation function of choice, f’is the derivative of the activated function,zis the output of a neuron before activation function ."
Page 8," The error ats11(1) [1 1 1 1 2 1 2 2 4 1 4]δs=f′zsδtws t,zis the output of a neuron before activation function,z"
Page 9," The error atr1 is the expression of the error ats1,s2, …s10, if we put them into the above expression, we getδr1=f′(zr1)f′"
Page 10," The error atq1,r2, …r10, however if we put them into the above expression, the equation will be too long to be displayed at here . And thus let’s look at the first termr"
Page 11," A lot of multiplications is involved in the calculation for each item . At hidden layer 1, we have 3 multiplications among derivative of activation function . If a net is deeper than the example, the earliear layers will have even"
Page 12," In backpropagation, the amount of update applied on each weight is proportional to the value of error . If the chosen activation function is not well behaved, i.e. tend to produce small values, then the errors at the"
Page 13," Earlier layers are key to good performance . Earlier layers can detect simple patterns underlying input . If feature extractors fail, garbage in garbage out for the classifier ."
Page 14, How to get the earlier layers trained? 3 main strategies to start . Pick the right activation function and give the weights proper values to start. Restrict the weights of 96 of 96 layers .
Page 15,"First, pick the right activation function"
Page 16," The problem with SigmoidSmall gradient is either too high or two low, its derivative very small . Poor learning as a consequence . Occur when weights are pooly initialized (with large negative or positive values)"
Page 17," The problem with SigmoidSmall gradient is hard to update earlier layers, many static layers . In the path of backpropagation, the multiplication of this small number leads to vanishing gradient ."
Page 18," Rectified linear unit is usedf(x) = max(0,x) When x > 0, the derivative value is 1 ."
Page 19, The 2024 National University of Singapore is the largest university in Singapore . It is the first time the university has held a university degree degree degree in Singapore's history . The university has a history of coming late and why .
Page 20,"Second, give the weights proper values"
Page 21," If weights are too small, signal shrinks as it passes through each layer; at later stage, neurons are 'dead', almost no activation coming out from neurons . At later stages, network becomes unstable and becomes unstable ."
Page 22," Xavier initializationproposed in 2010 . At that time designed for Sigmoid and tanh function . Initialize biases to be 0, the weight of each layer is . Weight is drawn randomly from the distributionnw∼U−,"
Page 23," Xavier initialization: In the case of Xavier initialization, the distribution has a constant probability . A=−b=1 1n n. The probability of a random distribution has been defined as a continuous probability ."
Page 24," The number of inputs to a neuron in a layer is called fan-in . In literature, the number of . inputs to the . neuron in . a layer in the layers is called 'fan-in' and 'Fan-in"
Page 25, The number of inputs to a neuron in the layer that has 18 channels is 96 . A neuron has 96 inputs in a layer of 18 channels . An average of 96 inputs is given to each neuron .
Page 26," N is the number of inputs to a neuron in the layer . N is a normal distribution with a zero mean and a variance of 2/n.0 . Initialize biases to be 0, the weight of each layer is 0,"
Page 27," In actual application, the normaldistribution is truncated to avoid unnecessary large values–2n . The National University of Singapore has used the truncated distribution to avoid large values ."
Page 28, He initializationproposed in 2015. He init.ensibly30-layer small model. Heinitated in 2015 . He proposed in 2015 for a new model .
Page 29," Keras by default uses Xavier initialization for weights, and zero initialization for bias . See the code to make the change to KerasInitialization ."
Page 30,"Third, restrict the weights"
Page 31, Large weights make net unstable . Minor variation in values or addition of noise will result in big differences in output . Prefer simpler model with smaller weights .
Page 32, The problem with large values Regularization . Add a penalty to loss function; the penalty should be proportional to the magnitude of the weight . L2 regularization is more often used; it calculates the sum of squared values .
Page 33," L2 regularization is an algebraic expression to update a weight . It is a loss function with regularization, the weight, the learning rate, and a derivative . The weight is the weight and learning rate ."
Page 34," The penalty is applied on per-layer basis . Not all type of layer support regularization, and not all support in the same way . Dense, Conv1D, Conv2D and Conv3D have a unified API ."
Page 35," Fromtensorflow.kerasimport regularizers>model = models.Sequential()>model.add(Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu"
Page 36," But, even with these, we still have problems .... ... But even with this, there are still some problems .... We still have a lot of problems to overcome, he says ."
Page 37," Deep, large neural nets on small dataset learn more on statistical noise in the data rather than key features . When new data comes in, error increases; poorgeneralization increases ."
Page 38, Dropoutproposed in 2014 . The 'dropping out' can be performed simply by setting the activation output from a neuron to zero .
Page 39," The idea came out by Geoffrey Hinton, inspired by a fraud-prevention mechanism used by banks . Dropoutproposed in 2014 . Hinton: 'I figured it must be because it would require cooperation between employees to successfully def"
Page 40," The zeros will be applied to different neurons in different training epochs . But the zeros are applied only during training phase, not in testing ."
Page 41, Dropout in Kerascomparison>fromtensorflow.keras.layersimportDropout>model = models.Sequential()
Page 42, The training of a deep neural net is a long march to fight overfitting . So far we have these techniques to use: Choose the right activation function . Use the suitable initialization and regularize the weights . Add dropout between layers .
Page 43," Batch normalizationNormalization...?...... ... ... ............and we have this five numbersx=[1.6, −0.5, 0.3,  0.4,  0.9]"
Page 44, Batch normalization Normalization or standardization…? How do we do that? What is the mean and standard deviation of the new vector?
Page 45, Train and validate on this data setcat non-cat data set . Batch normalization normalization . Covariate shift: The distribution of the data changes .
Page 46," In training deep learning model, gradients are used to update parameters under the assumption the other layers do not change . In practice, all the layers are updated simultaneously . The layers’ output distribution can change as a consequence ."
Page 47, The batch mean for v is the batch variance for v . The batch size ism47 of 96 of 96 . Batch normalization is the result of normalization on every item .
Page 48, Batch normalization is performed channel by channel . It starts with channel 1 . All the elements in channel 1 (of 4 samples) form theviand normalized accordingly .
Page 49," By default normalization is performed on last axis(axis = -1) Theµand ωare counted as non-trainable parameters,γandβ are trainable parameters . Used in Keraskeras.layers"
Page 50, Keraskeras.layerstensorflow.keras.layers tensorflow.tensorflow .BatchNormalization()()()(x)=>xBatch normalization()('relu')(x)
Page 51,Time for exercise
Page 52, Build the wBNRg modelfor cifar 10.52 of 96 . Build the model based on the model plot in the '4_1 wBNrg.pdf’
Page 53," Given the available techniques, do you think we can train a 1000-layer network? Is vanishing gradient still an issue?"
Page 54," Normalization and proper initialization have somewhat solved the problem ofvanishing gradient . But in very deep network,degradation happens: accuracy gets saturated and then degrades . Degradation is not caused by overfitting; the deep models"
Page 55, The equal between shallow and deep net should not perform worse than shallow net . The added layers should perform the belowy=H(H(x):xshallow deepadded layers .
Page 56," The training error from deeper net is higher, it is safe to say that, the added layers do not perform identity mapping . But to get the net goes deeper, we need layers that can be trained to at least . We need layers"
Page 57, He et al. proposed that the bottom structure shoud work . It is easier to get non-linear layers pushed to values of 0 rather than values of 1 .
Page 58," F(x)is doing a residual mapping of the residual layer . F(X) is doing a . residual mapping . Residual mapping F(y=H(x), F(Y) +x ."
Page 59, Let's build the parts and parcel required to form a very deep neural net! We're building the parts of the parts required to build a very neural net . Let's take a look at how to make a deep neural network .
Page 60," Conv2D solely performs convolution with padding, no activation function involvesConv2DBatchNormReLU ."
Page 61, Define resLyr . Create a function that can encompass the several layer arrangements . Define the function . Use the function to create the layer arrangement .
Page 62," Conv2D convLyrx=conv2D(numFilters=16, kernelSz=3, strides=1) convFirst=True, lyrName=None:x = ConvLyr(x) conv"
Page 63," Conv2D convLyrx=conv2D(numFilters=16, kernelSz=3, kernel_initializer=he_normal', kernel_regularizer=l2(1e-4), name"
Page 64," Conv2D convLyrx=conv2D(numFilters=16, kernelSz=3, strides=1) convFirst=True, lyrName=None:x = ConvLyr(x) conv"
Page 65," ConvLyrx= Conv2D(numFilters=16, kernelSz=3, stride=1, strides=1) If activation is not None:x = Activation(activation,name=lyrName"
Page 66, Residual blocks mix and match . Two types of residual blocks in resnet v1.0: 1BatchNormReLUConv2Dstride 1 and 2 .
Page 67, Resnet v1The full structure is based on the structure of the National University of Singapore's Resnet . The full structure of Resnet is now available for download .
Page 68, Define resBlkV1 is the function to produce these two sets of blocks . Define the function that produces these two blocks .
Page 69, ResBlkV1 configuration:WhendownsampleOnFirstisTrue numBlocksis true numBlocks is 3 . The function creates the below 3 blocks .
Page 70, ResBlkV1 configuration:WhendownsampleOnFirstisFalse numBlocksis3 . The function creates the below 3 blocks .
Page 71, ResBlkV1 configuration is based on the size of blocks and number of blocks . We can have as many blocks as we like; this is what we get whendownsampleOnFirstisTrue numBlocksis5Down Res
Page 72," The variable runcontrols the creation of blocks down . ResBlkV1 configuration>def resBlkv1(inputs,numFilters=16, numBlocks=3, downsampleOnFirst=True, names"
Page 73," ResBlkV1 configuration>def resBlkv1(inputs,numFilters=16, numBlocks=3, downsampleOnFirst=True, names=None):x = inputsfor run in range("
Page 74, 'Down ResBlk' is created by the National University of Singapore . Let's take a look at how it was created .
Page 75," Down ResBlkV1:x = inputsfor run in range(0,numBlocks): strides = 1blkStr = str(run+1) Strides is 2 for the first part of the block: 2 for"
Page 76," Down ResBlk>def resBlkV1(inputs,numFilters=16, numBlocks=3, downsampleOnFirst=True, names=None):x = inputsfor run in range(0,"
Page 77," The third part of the block: The last block: 'Revealrererev2Dstride 2, kernel 1ReLU.0'"
Page 78, The last of the block: The last block: Reveal7878of 96 . Down ResBlkV1 is the last block of 96 .
Page 79," How about 'Simp ResBlk'? How about a new acronym for 'ResResBlk'? How about an acronym for ""Resparation Resblk""?"
Page 80," Simp ResBlk>def resBlkV1(inputs,numFilters=16, numBlocks=3, downsampleOnFirst=True, names=None):x = inputsfor run in range(0"
Page 81," Simp ResBlk>def resBlkV1(inputs,numFilters=16, numBlocks=3, downsampleOnFirst=True, names=None):x = inputsfor run in range(0"
Page 82," Simp ResBlk>def resBlkV1(inputs,numFilters=16, numBlocks=3, downsampleOnFirst=True, names=None):x = inputsfor run in range(0"
Page 83," Resnet v1>defcreateResNetV1(inputShape=32,32,3),numClasses=10) ResBlkV1= AveragePooling2D(pool_size=8,name='"
Page 84, Adam>seed =29>np.random.seed(seed)optimizers.Adam(lr=0.001)=>optmz'cifar10ResV1Cfg1'=>modelname .
Page 85, TrainingThe problem? Training is training testing testing . Testing accuracy is 78.32% . Training is the key to success in Singapore .
Page 86, How to unleash the power of a deep net? How do you do it? How to do it with your own net?
Page 87, Large learning rate causes model to converge too fast to a suboptimal solution . Small learning rate gets training stuck .
Page 88, Training solution to improve? Try learning scheduler>from tensorflow.keras.prumls/y2024/v1.0: lr = 1e-3; lr *= 0.5e
Page 89, TrainingBetter? Training better? Training testing . Accuracy: 81.02% accuracy . Training better: Testing better? Testing better than training .
Page 90," Under current setup, the net sees the same set of images every epoch . Need to create variety to force the net to learn the features that really matter for classfication .Generates randomly varied images in the beginning of every epoch,"
Page 91, TrainingImage augmentation•Types of augmentation. Types of augmentation. Training. Training . Training . Types of augmentedmentation.
Page 92, Trainingwith image generator. Build image generator and use fit_generator to train . Train image generator to train with model .
Page 93, TrainingThis looks good. Training this looks good training testing. accuracy: 90.91% Accuracy:90.91%. Accuracy: 90% accuracy .
Page 94," Large batch size is faster training, but accuracy may suffer . Small batch size: training gets noisy, offering regularizing effect and lower generalization .GPU may not have sufficient memory to hold large batch of large size ."
Page 95," Trainingsmaller batch size is 32 . Make changes on the fit_generator>model.flow(trDat, trLbl, batch_size=32)adjust accordingly ."
Page 96, TrainingThe finale of the training program was completed at the 2024 National University of Singapore . Training was completed with accuracy of 91.67% and accuracy of 95% .
Overall Summary," Module 6 - Case studies on using convolutional neural networks for machine learning systems . The deeper the net, the harder totrain; sometimes not possible totrain . NUS-ISSPattern Recognition using Machine Learning System."
