Page Number,Summary
1,The document is about the second day of a course on pattern recognition and machine learning systems. It is presented by Dr. Zhu Fangming from the National University of Singapore's Institute of Systems Science. The content cannot be reproduced without written permission from the institute.
2,"The second day of the S-PRMLS workshop focused on neural network models and designs. The key points discussed were the basics of neural networks, including their structure and function, as well as different types of neural network architectures such as feedforward, recurrent, and convolutional networks. The concept of backpropagation, which is the main learning algorithm for neural networks, was also introduced. The session also covered the importance of data preprocessing and the use of regularization techniques to prevent overfitting. The different layers and activation functions commonly used in neural networks were also discussed."
3,"This section of the document discusses three types of neural networks: Radial Basis Function Networks, General Regression Neural Networks, and Self-Organizing Map Networks. Radial Basis Function Networks use a radial basis function as the activation function and are commonly used for classification tasks. General Regression Neural Networks are useful for both classification and regression tasks and use a linear output layer. Self-Organizing Map Networks, also known as Kohonen networks, are used for unsupervised learning and can be used for clustering and dimensionality reduction."
4,"The Radial Basis Function Network is a type of neural network with three layers: input, hidden, and output. The input layer is connected to the hidden layer, and the hidden layer is connected to the output layer. This architecture allows for the network to process and learn from data in a more efficient and effective manner."
5,"The architecture of RBF networks involves nodes in the input layer receiving input values, hidden nodes providing radial basis functions, and output nodes corresponding to non-linear functions. The weights in the network are adjustable, with a weight vector indicating the center of the radial basis function and weights connecting internal nodes to output nodes being adjusted through unsupervised and supervised learning, respectively."
6,"The process of activation in RBF networks involves the use of hidden layer nodes, also known as kernel nodes, which have Gaussian activation basis functions. These functions are dependent on the distance between the input vector and the node's centre vector. The activation value of a node is calculated by using a smoothing parameter, and the output unit activation is a linear combination of the kernel outputs. This is represented by a weight matrix, where the weight from a hidden node to an output node is denoted as Wji."
7,"The RBF network learning process involves finding two parameters (ci and σi) for each kernel node and a full weight set for output nodes. This is done in two stages, with the first stage being unsupervised learning where the centre and smoothing parameter values for the hidden nodes are found using Hard C-Means clustering. The second stage involves supervised learning to find weights for the output nodes."
8,"The document discusses learning methods for RBF networks, specifically for finding the kernel centre and smoothing parameters. If there are a small number of input samples, the kernel centre can be set as the input samples themselves. If there are a large number of samples, clustering techniques like k-means can be used to find the cluster prototypes as kernel centres. The smoothing parameters are typically found by calculating the average distance between the cluster centres and the training patterns."
9,The document discusses learning in the output layer of a radial basis function (RBF) network. It explains that the weights between the hidden and output nodes are adjusted in each iteration using a learning rate and error calculation. The error is computed by comparing the target output and the actual output. This process is repeated until the network converges.
10,"The document discusses a simple example of using a radial-basis function (RBF) network to solve the XOR problem. The RBF network has four hidden units, with radial-basis function centers determined by four input patterns. These input patterns and their corresponding outputs are used to determine the weights and centers of the RBF network. The document provides a visual representation of the network and its connections, as well as the weights and centers for each unit. This example helps to illustrate the concept of RBF learning and its application in solving problems."
11,"This section introduces a simple example of radial basis function (RBF) learning. The RBF used in this example has a weight vector of hidden unit Ci and an input vector x. The activation of the output node is a linear combination of the kernel outputs, and the initial weights of the output layer are 0.5 for each output node. The weight adjustment is based on a learning rate of 0.5 and is calculated using the target output, output activation, and output of the kernel node."
12,"This section discusses a simple example of radial basis function (RBF) learning. The weights for the kernel nodes are listed, and the weights for the output nodes are shown for two different inputs. For input P1, only C1 is activated, resulting in an output of 0.5. The weight for output node 1 is then updated using the formula Wo1(t+1) = Wo1(t) + ∆Wo1 = 0.5 + 0.5 * (0 – 0.5) * 1 = 0.25. For input P2, only C2 is activated, resulting in an output of 0.5. The weight for output node 2 is then updated using"
13,"RBF networks have a variety of learning strategies, including different types of basis functions and methods for selecting centers. These can include random selection or self-organized learning through clustering. The two-stage training process of RBF networks allows for the use of unlabeled data during the creation of kernel nodes, reducing the need for a large amount of labeled data for training."
14,"o o o o o o o o

The General Regression Neural Network (GRNN) is a type of neural network that consists of an input layer and three computational layers: pattern, summation, and output. The pattern units receive input data and pass it on to the summation units, which perform a weighted sum of the input data. The output units then use this weighted sum to produce the final output. This architecture allows the GRNN to learn and make predictions based on patterns in the data."
15,"The architecture of a General Regression Neural Network (GRNN) consists of input, pattern, summation, and output layers. The input layer distributes input patterns to the pattern layer through fully connected links with adjustable weights. The pattern layer computes the distance between the input vector and their connection weight vector values. The summation layer has two types of neurons, type A and type B, that perform a linear summation of weighted input from the pattern layer. The output layer uses the output of the two summation layer units to estimate the regression of z on x through a division operation."
16,"This section discusses the activation process of a general regression neural network (NN). It explains that the pattern-layer unit computes the distance between its weight vector and the input vector, and transforms it into a scalar activation function value using exponential functions. The summation units in the output layer perform a division operation on the outputs of two summation-layer units (type A and B) to produce the regression of z on x. The formula for this calculation is also provided, which involves a linear sum of weighted inputs from the pattern layer."
17,"The learning process for General Regression Neural Networks involves creating a new neuron for each exemplar pattern in the pattern layer, with weight values equal to the exemplars. In the summation layer, the weights are updated each time a training observation is seen, with the values of Ai and Bi increasing. The value of σ, which is determined experimentally, plays a role in defining the decision surface boundary."
18,"The GRNN mapping surface is affected by the smoothing constant, σ. Smaller values of σ create narrow and precise surfaces that fit well near sample points. On the other hand, larger values of σ produce smoother and flatter surfaces. Finding a balance between these two extremes is crucial for good generalization."
19,"The GRNN network is a type of self-growing neural network that uses a one-pass learning algorithm. It is well-suited for applications such as forecasting and control, where the training data is generated by an unknown probability distribution. The network is memory-based and creates one pattern unit for each new training pattern or cluster center. This allows it to adapt and learn from new data without needing to retrain the entire network."
20,"GRNN networks have several advantages, including the ability to converge to the true regression surface with increasing samples, work with sparse data and in real-time environments, and have fast and straightforward training with only one pass through the training set. However, they may become very large and have a heavy computation load."
21,"The document discusses an example of using a Generalized Regression Neural Network (GRNN) for adaptive control in the aerodynamics of a fighter aircraft. The aerodynamics of a fighter aircraft are highly nonlinear due to various factors such as kinematic and inertial couplings, aerodynamic nonlinearities, and control deflection rate limitations. The study simulated two nonlinear maneuvers, low angles of attack dynamics and deep-stalls, to compare the performance of different ANN architectures in predicting the dynamics of the aircraft. The network had five input variables and predicted two response variables at discrete time points."
22,"This section discusses an example of an application of a Generalized Regression Neural Network (GRNN) for adaptive control. The input variables for this application include a control deflection command, angles of attack, and pitch rates at two different time points. The output is a prediction of the angle of attack and pitch rate at the next time point. This process involves using an aircraft system, a neural network identifier, and an adaptive signal."
23,"The document discusses an example of using GRNN for adaptive control. A performance comparison was conducted between GRNN, MLP (two hidden-layer), and a radial basis function network. The results showed that GRNN had the fastest learning speed, highest modelling precision, and was the most flexible and simple compared to the other two networks. The MLP required 50 nodes in each hidden-layer and took more than 50 epochs to converge, while the RBF network used 18 basis nodes and converged after a few epochs. The GRNN only required a single pass over the training patterns and generated 1600 or 200 nodes, with a range of σ values (0.1 to 5.0) producing satisfactory results for generalization."
24,"( )

The document discusses an example of the application of GRNN (General Regression Neural Network) in adaptive control. The network was used to approximate the angle of attack and pitch rate, and the performance was compared to other networks. The results showed that GRNN had the lowest approximation errors for both variables. Only three networks are presented, but six were tested in total. The equations for calculating the errors are also provided."
25,"The Self-Organizing Map (SOM) network, also known as the Kohonen network, is a type of neural network used for clustering. Its common strategy is ""winner-takes-all,"" which involves competitive learning. The network has two layers: an input layer and an output layer, making it a basic architecture for clustering."
26,"The competitive network operation involves signals feeding forward from input nodes and feeding laterally among output neurons. It is a single-layer network with n inputs and m output units, where each output represents a different class or category. The network uses prototype weight vectors to classify input patterns, with the closest weight vector to the input pattern determining the class. This is known as a ""winner-takes-all"" network and is fully connected with all inputs connected to all outputs."
27,"The Self-Organizing Map (SOM) Network, also known as the Kohonen network, is a type of feature mapping that converts patterns from a higher-dimensional space into a one- or two-dimensional array of neurons. The Kohonen learning rule is used to design this network, which consists of a group of neurons organized in one or two dimensions. In a one-dimensional network, the neurons are arranged in a row, while in a two-dimensional network, they are arranged in a lattice array. This type of network uses unsupervised competitive learning, where the winning neuron shares its gains with its neighboring neurons to create an activation zone."
28,"The Simple Self-Organizing Map (SOM) network is a one-dimensional network where the input pattern vector is connected to all neurons. The neurons compete with each other and only one neuron wins, creating a ""winner-takes-all"" scenario. Lateral interactions between neurons limit the activation to a specific area, known as the ""excitation zone."" This network is used for competitive learning and can be trained to cluster similar input patterns."
29,"The SOM (Kohonen) Network is a 2-dimensional discrete lattice network that uses adjustable weight vectors to fully distribute input vector patterns to each node. The node with the weight vector closest to the input vector becomes the ""excitation centre"" winner for the lattice. This allows for the network to organize and classify input data."
30,"The Kohonen Learning Algorithm involves initializing all weights to random numbers, applying an input signal vector, selecting the winning output unit based on the smallest dissimilarity measure, and updating the weights of the winner unit using a neighborhood function and learning rate. This process is repeated until the network weights stabilize, and the neighborhood and learning rate parameters can be adjusted if necessary."
31,"The Kohonen Learning Algorithm is a type of neural network that uses a learning process called self-organization to create a map of input data. It involves adjusting the weights of the neurons in the network to match the input data and create a topological map. The activation neighbourhood can be defined as a square, hexagon, or smooth function such as Gaussian. This algorithm is commonly used for data clustering and pattern recognition tasks."
32,".

The SOM mapping operation involves mapping the input signal space onto the neural network (NN) lattice. The weight vectors of neurons in the neighborhood of the winning neuron are adjusted towards the input vector value, with closer neurons having greater weight adjustments. This allows the SOM to effectively cluster similar input signals in the same region of the lattice."
33,"The presentation discusses simulations of a self-organizing map (SOM) using two inputs, x1 and x2, mapped onto an array. The input patterns are chosen randomly from a unit square. The weight vector values for a single neuron are specified by line intersections on the map, while lines between nodes on the graph connect weight points for neurons that are topologically nearest neighbors. The presentation shows the evolution of weight values after 30 and 1000 iterations, starting from initial weights."
34,"The document discusses simulations of self-organizing maps (SOM) using input data uniformly distributed within a specific range. It shows the initial weights, as well as the weights after 30 and 1000 iterations. The results demonstrate how the SOM algorithm adjusts the weights to better represent the input data and form clusters. This process is useful for visualizing and understanding complex data sets."
35,"The Self-Organizing Map (SOM) algorithm is a viable alternative to C-Means clustering and offers several advantages. It eliminates the need to specify the number of clusters upfront and also provides a method of data compression. Additionally, when used with labeled data, it can act as a classifier. The most significant advantage of SOM is its ability to provide data visualization, which is lacking in many other algorithms."
36,"controller

The one-dimensional SOM network has been applied to solve optimization problems, including the traveling salesman problem. It has also been used in robot control systems to perform data fusion and provide input data to the robot controller."
37,"The document discusses the use of self-organizing maps (SOMs) in various applications, including handwritten signature authentication. This application uses a hybrid neural network system with SOM networks and multilayer perceptrons (MLPs). Two SOM networks are used to cluster signatures into similar sets, while two MLPs are used for authentication. A supervised learning neural network is also utilized for the final decision of accepting or rejecting a signature based on geometrical features and signature images."
38,"The workshop focused on three different neural network models: Radial Basis Function (RBF), Generalized Regression Neural Network (GRNN), and Self-Organizing Maps (SOM). RBF is a type of feedforward neural network that uses radial basis functions as activation functions. GRNN is a type of regression neural network that uses a Gaussian function to map inputs to outputs. SOM is a type of unsupervised learning neural network that is used for data clustering and visualization. The workshop provided hands-on exercises for participants to implement and understand these models."
39,"This section of the document discusses the use of Weka for implementing a radial basis function (RBF) network. Participants are instructed to open the Weka Explorer and load the iris.csv file. The classifier is then changed to weka.classifiers.functions.RBFNetwork, which may need to be installed through the Weka GUIChooser first. The properties window for the classifier is then opened, and participants can experiment with different options and test the model performance. The model can be saved for future use."
40,"The workshop on GRNN and SOM in Python/Neupy provides jupyter notebooks for participants to follow along. It is important for participants to understand how the neural network models are built, and they can save notes in the notebook as markdown."
