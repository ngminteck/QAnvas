Page,Summary
Page 1, AtA\S-PRMLS\Day2a.ppt\V3.1 © 2024 National University of Singapore. All Rights Reserved .
Page 2, ATA\S-PRMLS\Day2a.ppt\V3.1 © 2024 National University of Singapore. All Rights Reserved .
Page 3, AtA\S-PRMLS\Day2a.ppt\V3.1 © 2024 National University of Singapore. All Rights Reserved .
Page 4, The hidden layer is fully connected to the output layer . ATA\S-PRMLS\Day2a.ppt\V3.1 © 2024 National University of Singapore .
Page 5, Each node in the input-layer receives the value of an input variable . Each hidden node provides a radial basis function of input variables . Weights: An adjustable weight vector is on the connections between the input nodes and the i-th
Page 6, Hidden layer (kernel nodes) gives the same activation value for all inputs that lie within the same radial  distance of its kernel centre cj . Kernel nodes often use Gaussian activation basis functions which depend on the distance between the ipient
Page 7, Training requires two parameters (ci and σi ) to be found for each kernel node i and a full weight set for output nodes . Training is performed in two stages: Unsupervised learning and Supervised learning .
Page 8," The number of input samples xi is small, set kernel centre ci = xi for each i (a form of self growing network) If the number of samples is large, do clustering (e.g. k-"
Page 9," The weight change is computed by the weight from hidden node i to output node j at time t (or the t-th iteration) At a given point, Wji(t+1) = Wji (t) + W"
Page 10," A Simple Example of RBF Learning: Build a RBF network with four hidden units to solve the XOR problem . The radial-basis function centers C1, C2, C3, and C4 are determined by the"
Page 11," The activation of output node is a linear combination of the kernel outputs: Out = ΣiWoiOi. The initial weights of the output layer are:..5, Wo1 = 0.5 . The weight adjustment"
Page 12," A Simple Example of RBF Learning . Weights for kernel nodes: W11, W12, W21, W22, W31, W32, W41, W42, W2, W3, W4,"
Page 13, The two-stage training process of RBF permits the use of unlabeled training data while creating kernel nodes . Only a relatively small number of labelled data will be needed to find the output layer .
Page 14, AtA\S-PRMLS\Day2a.ppt\V3.1 © 2024 National University of Singapore. All Rights Reserved .
Page 15, The input-layer distributes input patterns to the pattern-layer through fully connected links with adjustable weights . The output layer performs a division operation on the output of the two summation-layer units to produce the estimate  of
Page 16, NN is the number of pattern nodes and number of patterns nodes . The activation of each summation unit is a linear sum of weighted inputs from the pattern layer . The output layer performs a division operation on the output of the two summ
Page 17," In the pattern layer, a new neuron is created for each exemplar pattern (or cluster centre) The weight values are set equal to the exemplars or cluster centres (centroid values) For the weights in summation-layer,"
Page 18," Small values of σ give narrow peaked surfaces that fit well near samples points . Larger values result in flatter, smoother surfaces . Good generalization requires a trade-off between the two extremes ."
Page 19," A form of self-growing net with one pattern unit created for each new training pattern or cluster centre . Good for forecasting, control and similar applications where the training  data set is generated by an unknown probability distribution ."
Page 20, The estimate converges to the true regression surface with increasing samples . The network can begin to perform the regression after a single training sample has been presented . The computation load for the network is relatively heavy .
Page 21," The aerodynamics of a fighter aircraft are typically very nonlinear in nature . Significant nonlinearities occur as a result of kinematic and inertial couplings, aerodynamic nonlinearity and control deflection rate . Sim"
Page 22," An Example of GRNN Application (cont.)— Adaptive Control — — . Input variables:. control deflection command: δc(k), k(k) and k-pitch rates at time k-1 ."
Page 23," Performance comparison was based on learning speed, modelling precision, network flexibility, and network complexity . The GRNN was trained in a single pass over the training patterns . σ values were tested for best generalization ."
Page 24," The performance of three networks (six networks have been done) was  satisfactory, but the GRNN was best (compare the errors for angle of attack and pitch rate, E a and Eq below)"
Page 25," Self-Organizing Map (SOM, Kohonen) Network for clustering network . Common strategy: winner-takes-all (a competitive learning)"
Page 26," A single-layer network architecture with n inputs and m output units, one output for each class or category . Signals feed forward from input nodes and feed lateral among output neurons, a form of recurrent feedback ."
Page 27," Self-Organizing Map (SOM, Kohonen) Network is a form of unsupervised competitive learning where winner shares the gains with neighbours to produce activation zone ."
Page 28, Simple SOM (Kohonen) Network is a simple 1-dimensional Kohonen (SOM) network . Lateral interactions between neurons constrain activations to spatially bounded ‘excitation zone’
Page 29," 29SOM (Kohonen) Network (cont.)— Two-dimension — A 2-dimensional Kohonen discrete lattice network . The node with weight vector closest to the input vector becomes the ""excitation centre"" for the"
Page 30, The winning output unit is the one with the smallest dissimilarity measure (or largest similarity measure) among all units . The neighbourhood function is a neighborhood function with maximum value centered at the winning neuron .
Page 31, AtA\S-PRMLS\Day2a.ppt\V3.1 © 2024 National University of Singapore .
Page 32, 32SOM Mapping the input signal space onto the NN lattice . Weight vectors of neurons in the neighborhood of the winning neuron s are shifted towards the input vector value .
Page 33, Simulations of SOM simulations . Mapping two inputs x1 and x2 onto a SOM array . Lines between nodes merely connect  weight points for neurons that are topologically nearest .
Page 34, AtA\S-PRMLS\Day2a.ppt\V3.1 © 2024 National University of Singapore . 34Simulations of SOM (cont.)
Page 35," SOM is a viable alternative to C-Means clustering . It provides a method of data compression . When used for labeled data, this acts as a classifier too ."
Page 36, One-dimensional SOM networks have been used to successfully solve optimization problems such as the traveling salesman problem (TSP) The network performs “data fusion” to provide the input data to the robot .
Page 37, Handwritten signature authentication uses a hybrid neural network system with SOM networks and MLPs . 2 SOM networks were used for initial signature clustering into similar sets . 1 supervised learning NN was used for final decision .
Page 38, AtA\S-PRMLS\Day2a.ppt\V3.1 © 2024 National University of Singapore. All Rights Reserved 382.2 .
Page 39, Weka Explorer continues from the Day 1 Workshop on using Weka for MLP . The RBF network is NOT available under the classifier options in Weka .
Page 40," Open the jupyter notebooks provided for this workshop . As you go through the notebooks, make sure you understand how the NN models are built ."
Overall Summary," Not be reproduced in any form or by any means, without the written permission of ISS, NUS, other than for the purpose for which it has been supplied ."
