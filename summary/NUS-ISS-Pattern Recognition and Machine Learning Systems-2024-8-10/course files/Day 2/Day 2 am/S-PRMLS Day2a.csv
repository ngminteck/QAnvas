Page,Summary
Page 1,"Dr Zhu Fangming NUS-ISS fangming@nus.edu.sg Not be reproduced in any form or by any means, without the written permission of ISS, NUS ."
Page 2,Neural Network Models and Designs . ATAS-PRMLSDay2a.pptV3.1 2024 National University of Singapore.
Page 3,ATAS-PRMLSDay2a.pptV3.1 2024 National University of Singapore. All Rights Reserved.
Page 4,ATAS-PRMLSDay2a.pptV3.1 2024 National University of Singapore.
Page 5,ATAS-PRMLSDay2a.pptV3.1 2024 National University of Singapore. All Rights Reserved 5 Architecture of RBF Networks.
Page 6,j-th basis function gives same activation value for all inputs that lie within the same radial distance of its kernel centre cj . i-th node in hidden layer is usually linear combinations of the kernel
Page 7,training is performed in two stages: finding centre and smoothing parameter values for the hidden (kernel) nodes . Usually Hard C-Means clustering is employed .
Page 8,ATAS-PRMLSDay2a.pptV3.1 . All Rights Reserved 8 RBF Network Learning (cont.) Learning for Kernel Centre .
Page 9,all rights reserved 9 RBF Network Learning (cont.) Learning in output layer . Adjust weights by Wji(t+1) = wji (t) + Wji .
Page 10,Build a RBF network with four hidden units to solve the XOR problem . a simple example to help understanding the xor problem is shown in this video .
Page 11,"ci is the weight vector of hidden unit Ci (i = 1, 2, 3, 4), x is an input vector (the simple function requires NO smoothing parameter) the activation of output node is a linear combination of the"
Page 12,ATAS-PRMLSDay2a.pptV3.1 2024 National University of Singapore. All Rights Reserved 12 ...
Page 13,the two-stage training process of RBF permits the use of unlabeled training data (unsupervised training methods) while creating kernel nodes .
Page 14,"GRNN architecture consists of an input-layer and three computational layers: pattern, summation, output . all rights reserved 14 general regression neural networks (GRNN)"
Page 15,the input-layer distributes input patterns to the pattern-layer through fully connected links with adjustable weights . the output layer performs a division operation on the output of the two summation-layer units .
Page 16,pattern-layer unit i computes the distance between its weight vector and the input vector (x wi) and transforms this to a scalar activation function value (typically exponential functions - Gaussian
Page 17,"in the pattern layer, a new neuron is created for each exemplar pattern (or cluster centre) and the weight values are set equal to the exemplars or cluster centres (centroid values)"
Page 18,"small values of give narrow peaked surfaces that fit well near sample points . larger values result in flatter, smoother surfaces . good generalization requires a trade-off between extremes ."
Page 19,GRNN Networks is a memory-based one-pass learning algorithm . one pattern unit is created for each new training pattern or cluster centre .
Page 20,training a GRNN is fast and straightforward with only a single pass through the training set needed . the net may grow to be very large since one pattern unit is added for each pattern .
Page 21,aerodynamics of a fighter aircraft are typically very nonlinear in nature . simulations were carried out to see how well different ANN architectures could model the dynamics for low angles of attack dynamics and deep-stalls.
Page 22,"aircraft systems nnn identifier predicts angle of attack at time k+1 . predicted pitch rate at time 1+1: q(k), k(k-1)"
Page 23,"GRNN was trained in a single pass over the training patterns . values were tested for best generalization, any value in the range of 0.1 to 5.0 was satisfactory ."
Page 24,"the GRNN was the best network for Adaptive Control . a total of six networks have been done, but only three are given here . approximation errors were found for angle of attack and pitch rate ."
Page 25,"all rights reserved 25 Self-Organizing Map (SOM, Kohonen) Network • Neural Network for clustering • common strategy: winner-takes-all (a competitive learning)"
Page 26,"a single-layer network architecture with n inputs and m output units, one output for each class or category . the network classifies each input pattern x as belonging to class i iff ||wi"
Page 27,Feature mapping converts patterns of arbitrary dimension (the pattern space) into the response of one- or two-dimensional arrays of neurons (the feature space) a one-dimensional network is a single layer of units arranged
Page 28,ATAS-PRMLSDay2a.pptV3.1 . All Rights Reserved 28 Simple SOM (Kohonen) Network — One-dimensional .
Page 29,"the node with the weight vector closest to the input vector becomes the ""excitation centre"" winner for the lattice Outputs Input Vector Weight vectors wr ."
Page 30,"all rights reserved 30 Kohonen Learning Algorithm . initialize all weights wr to random numbers following uniform distribution (0,1) 2) Apply an input signal vector x to the network . 3) Select the"
Page 31,Kohonen Learning Algorithm (cont.) can be used to 'square' or 'hexagon' Smooth functions such as Gaussian or other types may also be used.
Page 32,"weights of neurons in the neighborhood of the winning neuron s, are shifted more and those farther away shifted less . Those nearer to s have weights shifted . more ."
Page 33,mapping two inputs x1 and x2 onto a SOM array . Line intersections on the maps specify weight vector values for a single neuron .
Page 34,ATAS-PRMLSDay2a.pptV3.1 . all rights reserved 34 Simulations of SOM (cont.)
Page 35,"SOM provides the user with much wanted ‘data visualization’ feature, which is absent in many of the competing algorithms . it does so more elegantly, by removing the main drawback of C-Means - specifying"
Page 36,"one-dimensional SOM networks have been used to successfully solve optimization problems . as a part of a robot control system, SOM network provides the input data to the robot ."
Page 37,a hybrid neural network system with SOM networks and MLPs was used for initial signature clustering into similar sets . 1 supervised learning NN for final decision accepted .
Page 38,ATAS-PRMLSDay2a.pptV3.1 2024 National University of Singapore. All Rights Reserved.
Page 39,RBFNetwork is NOT available under the classifier options . please go to Weka GUIChooser TOOLS package manager to install the package first .
Page 40,GRNN & SOM – Python / Neupy Workshop . jupyter notebooks are provided for this workshop .
Overall Summary,ATAS-PRMLSDay2a.pptV3.1 2024 National University of Singapore . all rights reserved . a GRNN is a one-dimensional network with a single layer of neurons arranged as a lattice . the network is based on a memory-based learning algorithm .
