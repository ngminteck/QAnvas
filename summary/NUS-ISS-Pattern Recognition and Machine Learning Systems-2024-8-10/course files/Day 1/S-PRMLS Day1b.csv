Page,Summary
Page 1,"Dr Zhu Fangming NUS-ISS fangming@nus.edu.sg Not be reproduced in any form or by any means without the written permission of ISS, NUS ."
Page 2,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. All Rights Reserved 2 1.2 Neural Network Basics:
Page 3,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. All Rights Reserved.
Page 4,artificial neural networks (ANN) or Neural Networks (NN) are based on physiology of brain . conventional computing paradigms are not suitable to solve this type of problems .
Page 5,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. All Rights Reserved 5 Example: Character Recognition by Human Brain and NN
Page 6,a neural network is a massively parallel distributed processor . knowledge is acquired by the network through a learning process . inter neuron connection strengths known as synaptic weights are used to store knowledge .
Page 7,axon cell body nucleus Synapse Dendrites is a multiple signal processor based on electrochemical processing principles . it is the brain's basic biological computing element — the neuron .
Page 8,"a neuron is a small cell that receives electrochemical stimuli from multiple sources and responds by generating electrical impulses . about 10% of the neurons are input and output, the remaining 90% are interconnected with other neurons"
Page 9,N input signals each weighted for its importance are added to produce a cumulative (net) input . if net threshold f(net) = 1 otherwise .
Page 10,"the bias input is a fixed positive signal, and the weight w0 on the bias output is also adjustable . if x 0 f(x) = 1 otherwise, the activation function is"
Page 11,"a simple NN example shows that given the same inputs, the output will be different when different weights are used . changing weights purposely to reduce the difference between the target output and the NN output can change the behaviour"
Page 12,"NNs can learn to model complex systems from examples . they can mimic many intelligent traits found in humans: learning, generalisation, tolerance to missing or noisy data, associative recall ."
Page 13,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore . all rights reserved 13 applications of neural networks .
Page 14,W. McCulloch & W. Pitts (1943) described a logical calculus of neural networks . von Neumann used idealized switch-delay elements derived from the .
Page 15,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. all rights reserved 15 Some of Pioneering Research Work of Neural Networks.
Page 16,ATAS-PRMLSDay1b.pptV5.0. All Rights Reserved 16 Some of Pioneering Research Work of Neural Networks .
Page 17,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. all rights reserved.
Page 18,a neural network has a parallel-distributed architecture . each connection points from one node to another and is associated with a weight .
Page 19,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. all rights reserved.
Page 20,there are feedback connections or loops j i Wji Output Layer Hidden Layer Input Layer Feedforward connections Recurrent connections . all connections point in one direction (input output)
Page 21,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. All Rights Reserved 21 General Architecture of Neural Networks (cont.)
Page 22,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore.
Page 23,All Rights Reserved 23 Category of Neural Network Learning — by Learning Methods . MLP — Multi-layer perceptron RCE — Reduced Coulomb Energy GRNN .
Page 24,ATAS-PRMLSDay1b.pptV5.0 . All Rights Reserved 24 Single-layer Perceptron .
Page 25,"NN Learning: Single-layer perceptron . if the output is ONE and should be ONE (no error), do nothing . increase the weight values on all active input links ."
Page 26,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. All Rights Reserved 26
Page 27,"using the 10 given patterns for training (assuming the sequence as appearance) let initial weights be w0 = -0.5, w1 = -1 and w2 = 1.5 learning step = 0.5."
Page 28,"with the new set of weights, all ten patterns are classed correctly (verify by yourself) the new weights mean that all patterns are classified correctly ."
Page 29,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. All Rights Reserved 29 Single Perceptron and Linear Separability .
Page 30,All Rights Reserved 30 Single Perceptron and Linear Separability . a neuron can also solve the same problem (for your exercise)
Page 31,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. All Rights Reserved.
Page 32,"in the activation function n WiXi = i=1, forms a hyperplane in the n-dimensional space, dividing the space into two halves . when n = 2, the"
Page 33,"a single perceptron will converge only when the problem is linearly separable . if a training case is presented to the network with a desired output, there is a procedure guaranteed to find a"
Page 34,ADALINE (ADAptive LINear Element) has a single output which receives multiple inputs . it takes the weighted linear sum of the inputs and passes it to a bipolar function .
Page 35,All Rights Reserved 35 Widrow-Hoff Delta Rule • Linear Outputs: ith output for pattern p .
Page 36,multilayer perceptron is a feedforward neural network with at least one hidden layer . it can form more complex decision regions to solve nonlinear classification problem . the delta rule does not apply to training a multilayer
Page 37,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. All Rights Reserved 37 Multilayer Perceptron and Backpropagation Learning.
Page 38,backpropagation algorithm is based on a weighted random number generator . it can be used to generate a network output pattern zp . backpropagate the errors according to the BP weight adjustment formula
Page 39,total error over all training patterns p and m output units: zk+1 Tk Tk+1 Ek Ek+1 Hj Ik f f Zk+1 ptot EE 2
Page 40,"for output units k = 1, 2, ..., m, adjust the weights to reduce the error for each pattern p (Gradient descent)"
Page 41,ATAS-PRMLSDay1b.pptV5.0 All Rights Reserved 41 Summary of Backpropagation Algorithm .
Page 42,the weight change is computed by Wji = jOi where is a trial-independent learning rate . k is the error term at unit k to which a connection points
Page 43,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. All Rights Reserved 43 Typical Activation Functions.
Page 44,All Rights Reserved 44 Typical Activation Function Source: Caffe Tutorial . ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore.
Page 45,the softmax function is often used in the final layer of a neural network-based classifier . the sofmax function serves as a multinomial probability distribution .
Page 46,"after many iterations, a set of final weights give the mean squared error of less than 0.01 . a weight of 0.505(1 – 0.508) = 0.0006 W23"
Page 47,"Error surface: one global minimum, but many local minima . BP is never assured of finding a global minimum ."
Page 48,"generalization is the ability of a network to correctly classify a pattern it has not seen (not been trained on) NNs generalize when they recall full patterns from partial or noisy input patterns, when they recognize patterns not previously"
Page 49,overtraining can be avoided by: choosing a large training set when possible; selecting the patterns randomly during training; stopping the training process before excessive training occurs . eliminate unnecessary hidden-layer nodes & weights; adding regularization terms
Page 50,BP training is based on the gradient descent computations . network variables that minimize the error function E over the training set are the weight values . all rights reserved .
Page 51,the chosen error measure (or LOSS function) should be differentiable . the search process depends on the shape of the error surface as well as learning algorithm and training set .
Page 52,setting the initial weights to small (but not too small) random values is an effective way to avoid shallow troughs and possible entrapment .
Page 53,All Rights Reserved 53 Improving the Rate of Convergence Source: Stanford Courses . ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore.
Page 54,"Separate momentum terms can be added to each set of the layer weights using different values of for each layer . the most important parameter, the number of hidden nodes, is empirically chosen as a number around or close"
Page 55,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore. All Rights Reserved 55 Control Overtraining/ Overfitting .
Page 56,https://hackernoon.com/gradient-descent-aynk-7cbe95a778da . all rights reserved .
Page 57,ATAS-PRMLSDay1b.pptV5.0 . All Rights Reserved 57 General Design Guidelines .
Page 58,Workshop: Building Multi-Layer Perceptron Neural Networks using Weka and Python . ATAS-PRMLSDay1b.ppt2024 National University of Singapore.
Page 59,"we are given a data set of 150 samples (patterns) of Iris flowers each with 4 different feature variables representing petal length, petal width, sepal length and sepal width . each pattern falls into one"
Page 60,Download and install weka from https://ml.cms.waikato.ac.nz/weka/ .
Page 61,All Rights Reserved 61 Workshop – Weka - MLP . A BP network is auto-built according to the architecture setting .
Page 62,"you may install and create your own Anaconda environment and run Jupyter Notebooks . you can also use Google colab to run your own Python program . to start working with colab, you first need to log"
Page 63,download and install latest Anaconda for Python 3.7 . .ipynb files can be opened within your browser .
Page 64,this dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases . the objective of the dataset is to diagnostically predict whether or not a patient has diabetes .
Page 65,open the jupyter notebook provided for this workshop . check and compare the model performance . save your notebook with the cell output and upload it to Canvas .
Overall Summary,ATAS-PRMLSDay1b.pptV5.0 2024 National University of Singapore . all rights reserved . a single perceptron can only solve a classification problem when it is linearly separable .
