Page Number,Summary
1,"The document discusses spatial sensing and 3D scene modelling, specifically focusing on the S-SRSD (Spatial-Sensing Remote Sensing Device) system. This system uses a combination of sensors, including cameras and LiDAR, to create accurate 3D models of environments. The document is authored by Dr. Tian Jing, and is copyrighted by the National University of Singapore."
2,"The document discusses spatial sensing and reasoning from sensor data, specifically focusing on camera modeling for 3D vision and keypoint-based feature extraction from multiple images. It explains the importance of understanding spatial relationships and how sensor data can be used to infer these relationships. The document also provides an overview of camera modeling techniques and the process of extracting keypoint-based features from multiple images."
3,"The document discusses different ways of defining position, including global (absolute) position using systems like GPS, relative position based on selected coordinate references, and symbolic position information such as ""interaction classroom"" or ""PGP canteen."" These different methods can be used for spatial sensing, which involves using sensors to gather information about the physical environment."
4,"This page discusses various types of sensors used in spatial sensing, including the Kinect 3D sensor, passive infrared sensor, ultrasonic time of flight sensor, WiFi sensor, laser range-finding sensor, array microphone, and floor pressure sensor. These sensors are used to detect and measure different aspects of the environment, such as movement, temperature, distance, and sound. The advancement of smartphone technology has led to the rapid development and integration of these sensors, making spatial sensing more accessible and efficient."
5,"The introduction on page 5 of the document 'SRSD 1 Spatial sensing v3.4.pdf' discusses the two types of sensors used in spatial sensing: proprioceptive and exteroceptive. Proprioceptive sensors measure internal values of the system, such as motor speed and battery status, while exteroceptive sensors gather information from the external environment, such as distances to objects and ambient light intensity. Additionally, sensors can be categorized as passive or active, depending on whether they measure energy from the environment or emit their own energy and measure the reaction."
6,"The document discusses positioning systems, which involve navigation sources with known locations and users whose locations need to be determined. Location sensors provide information for determining proximity and quality of communication link, including received signal strength, bit error rate, and RFID read success rate. Different positioning principles, such as fingerprinting, time of arrival (TOA), trilateration, time difference of arrival (TDOA), multilateration, and angle of arrival (AOA), can be used to determine the location of users."
7,"The concept of proximity in spatial sensing refers to the user's position being determined by the closest navigation source. This can be achieved through the use of beacons and other location-based technologies, which are revolutionizing the way retailers and businesses operate. Professional location awareness involves not only presence and proximity, but also tracking of a user's movements. This technology is constantly evolving and has the potential to greatly improve the efficiency and accuracy of positioning."
8,"The document discusses the use of fingerprinting in positioning, where an n-dimensional space is used to contain received signal strength (RSS) vectors of reference points. The nearest neighbour method is used to find the reference point with the largest RSS, and the decision is made based on this point. Multiple nearest neighbours can also be used, where the k (e.g. three) closest reference points are considered. Interpolation can also be used, where the three closest reference points are used to obtain a more accurate position through an interpolation algorithm."
9,"The document discusses the concept of trilateration for positioning, which involves using three measurements to determine an object's location. It explains the mathematical formula for trilateration and provides an example using points A, B, and C. The document also mentions the use of time of arrival (TOA) for trilateration, where the propagation time of a sound wave is used to calculate distance. An example of this is shown in a YouTube video titled ""Locating Dad."""
10,"The document discusses positioning using trilateration, which involves using the time difference of arrival (TDOA) between reference stations and the current position. The TDOA is obtained by dividing the distance by the signal propagation speed and calculating the differences between the source and reference station coordinates. This method does not require a global time, only time differences between stations."
11,"This section discusses positioning in S-SRSD, specifically through the use of angulation angle of arrival (AOA). A base station can measure the angle to a mobile terminal by rotating its antenna to the highest received signal strength (RSS) value. This can be further refined by using an antenna array to derive the angle from individual antenna RSS values. A reference link is provided for more information on this technique."
12,"The document discusses the use of deep learning for acoustic localization, which involves determining the position of a source based on audio signals. The authors propose an end-to-end approach that uses a deep learning model to directly predict the source position coordinates. This approach has been shown to outperform traditional methods that involve multiple stages of processing. The document also provides a detailed overview of the deep learning model and its training process, as well as potential applications of this technology."
13,"The document discusses the concept of positioning in spatial sensing, specifically using a visual map as a reference. This visual map includes information such as appearance, geometry, and scene description, as well as a set of mapping images with camera poses and calibration. The document references two projects, Multi-View Stereo for Community Photo Collections and Exploring Photo Collections in 3D, which utilize this visual map for positioning purposes."
14,"This section discusses the positioning of the SRSD device, specifically its height, breadth, and arch type. The device measures 8.08 inches in height and 3.41 inches in breadth, and has a high arch type. A4 paper can be used as a reference for these measurements. The source also provides a link to a blog post that explains how deep learning can be used to measure feet."
15,"The agenda for this section includes an introduction to spatial sensing and reasoning from sensor data, camera modeling for 3D vision, and keypoint-based feature extraction from multiple images. These topics are important for understanding how spatial sensing works and how to extract useful information from sensor data."
16,"The document discusses the issue of ambiguity in distance estimation between a physical point and a camera. It explains that any point on the ray from the camera center to the point can be projected onto the same image point, making it difficult to accurately measure depth. The solution to this problem is to use a second camera, which allows for triangulation and more precise depth measurement. A reference is provided for further information."
17,"The document discusses the concept of stereo vision, which involves using two cameras to capture images and create a 3D representation of the environment. There are two types of stereo camera systems: parallel and general. Parallel systems have cameras placed side by side, while general systems have cameras at different angles. Both systems have their own advantages and limitations, and the choice depends on the specific application. The document also provides a reference for further reading on the topic."
18,"The document discusses camera modelling and the different coordinate systems used in spatial sensing. The camera reference system is based on the physical coordinates in meters, while the image reference system is based on the center point of the image plane. The pixel reference system is based on the top left point of the image in digital pixels. The document also mentions the extrinsic matrix, which represents the relationship between two coordinates of the same point due to camera movement, the pinhole camera model, which represents the relationship between the camera reference system and the image reference system, and the intrinsic matrix, which represents the relationship between the image reference system and the pixel reference system. These concepts will be further studied in the next class."
19,"The pinhole camera model is a simplified representation of how light travels through a camera to create an image. It assumes that light rays travel in straight lines and that a small aperture, or pinhole, is used to let light into the camera. This model can be used to predict the location and size of objects in an image, and is commonly used in computer vision and graphics applications. However, it does not take into account factors such as lens distortion and light diffraction, which can affect the accuracy of the model in real-world scenarios."
20,The document discusses the pinhole camera model and its use in converting coordinates from the camera reference system to the image reference system. This conversion is dependent on the camera's focal length and can be represented in a matrix format using homogeneous coordinates. The triangle similarity theorem is also mentioned as a reference for this process.
21,"The intrinsic matrix is a key component in camera calibration, which includes values for focal length, skew, and principal point. An example of an intrinsic matrix for a 640x480 pixel image with a focal length of 210 pixels is provided. The physical size of each pixel and the position of the image plane origin are also important factors in determining the intrinsic matrix. Homogeneous coordinates are used to represent the coordinates of a point in an image."
22,"The appendix discusses the use of homogeneous coordinates in spatial sensing. These coordinates involve adding a ""1"" at the end of the original coordinate vector, allowing for the expression of both translation and rotation using matrix multiplication. The appendix also shows how to use homogeneous coordinates for translation and rotation around the z-axis."
23,"Stereo vision involves using two calibrated cameras with a known distance between them to create a 3D representation of the environment. The distance between a point and the camera can be calculated using the triangle similarity theorem, and the physical distance between the two cameras is known as the baseline. The focal length of the camera and the physical size of a pixel on the camera sensor are also important factors. Virtual image planes and target points in the physical world can be determined using these calculations. An illustration is provided to show how stereo vision works from a bird's eye view."
24,"The document discusses disparity estimation, which is the process of finding the corresponding point in one image to a point in another image. The idea is to find an image patch in the right image that is similar to the patch in the left image. This is achieved by scanning along the horizontal line and comparing patches to find the most similar one. The matching cost is typically calculated using SSD (sum of squared differences). A reference link is provided for further information."
25,"This page discusses disparity estimation from stereo images, which involves calculating the difference in pixel location between corresponding points in left and right images. The disparity map shows the disparity value at each pixel location, which can be used for tasks such as depth perception and 3D reconstruction. The reference for this topic is the imaging geometry lecture from the Advances in Computer Vision course at MIT."
26,This page discusses distance estimation in the context of spatial sensing. It provides a reference to a lecture on the topic and poses two questions: the social distance between two detected persons and the distance a football player has run.
27,"The concept of intuition in spatial sensing involves transforming the view from a CCTV camera to a bird's-eye view. This can be achieved through techniques such as deep learning and social distancing demos, as demonstrated by the links provided. This approach allows for a better understanding and analysis of spatial data, particularly in crowded areas."
28,"The document discusses various transformations, such as Euclidean, affine, and homography, and how they preserve different properties such as translation, orientation, and parallelism. The transformations also differ in their ability to preserve straight lines and lengths. These concepts are important in spatial sensing and can be further explored in the reference provided."
29,A Homography matrix is a 3x3 transformation matrix that maps points from one image to corresponding points in another image. It is applicable to all sets of corresponding points on the same plane in the real world. The matrix is represented by nine elements and can be calculated using the cv2.findHomography() function in OpenCV.
30,"The process of homography, also known as perspective correction, involves collecting four corners of an object plane and setting a target plane with desired resolution and aspect ratio. This is done using the OpenCV function cv2.findHomography(). The homography matrix obtained is used to map the position of a point in the source image to its location on the target plane. This technique is useful for correcting perspective distortions in images."
31,"The fundamental task in spatial sensing is keypoint matching in multiple-view images. By matching pairs of keypoints in these images, we can estimate disparity and distance between physical points and the camera. We can also estimate transformations between images, such as a Homography matrix, which can help to estimate distance between two pixel positions in the image."
32,"This section introduces spatial sensing and reasoning from sensor data, specifically focusing on camera modeling for 3D vision and keypoint-based feature extraction from multiple images. The goal is to understand how to use sensor data to create a 3D model of the environment and extract key features for further analysis."
33,"The document discusses the process of identifying matched keypoints through feature matching. This involves detecting distinctive keypoints in two images and extracting feature descriptors around them, such as ùêóùêó1 and ùêóùêó2. The distance between these feature vectors is then computed to find correspondence, using a user-defined threshold ùëáùëá. The formula for this is ùëëùëë ùêóùêó1, ùêóùêó2 < ùëáùëá."
34,"The Harris corner detector is a method for detecting corners in an image by looking for significant changes in all directions. It involves finding patches of the image that generate a large variation when moved around with a shift value. This is done by calculating the difference between the original patch and a shifted window, using a mask function to determine the patch's size and shape. The goal is to find a patch with a large variation among different shifted versions."
35,"SIFT (scale-invariant feature transform) is a method for detecting keypoints in an image, which are points that are robust to changes in scale and rotation. It involves creating an image pyramid with a Gaussian filter at different scales, and finding the maximum value in a 9x9 neighbourhood around each pixel. These maxima are then compared across scales to identify keypoints. A step-by-step tutorial for SIFT is available online."
36,"SIFT (scale-invariant feature transform) is a feature extraction technique that involves detecting keypoints, warping the region around each keypoint to a canonical orientation, and creating a histogram of local gradient directions. The patches are then rotated to make them rotation invariant. The region is divided into 16 squares and the gradient direction histogram is computed for each square, resulting in a 128-dimensional feature. This technique was suggested in the original paper and is used to extract distinctive features from images."
37,"The document discusses feature matching using two methods: Euclidean distance and cosine similarity. Euclidean distance calculates the distance between two points in a multi-dimensional space, while cosine similarity measures the similarity between two vectors by taking into account the angle between them. These methods are used to match features between two images, with the features represented as squares in the left and right images."
38,"The document discusses feature matching using similarity in spatial sensing. It suggests comparing the distance of the closest and second-closest feature vector neighbors, and if they are similar, it indicates an ambiguity match. On the other hand, if the distance of the closest neighbor is significantly smaller than the second closest, it is considered a good match. The document also mentions the use of a database of 40,000 keypoints to determine the probability of a match being correct by comparing the distance ratio of the closest and second closest neighbors. A reference to a study on distinctive image features is provided."
39,"This section of the document discusses a workshop focused on 3D sensor data representation and modelling. The main task of the workshop is to extract and match features from multiple view images, using the SfM Camera trajectory quality evaluation dataset from GitHub. The workshop is designed to improve participants' skills in working with 3D sensor data and evaluating the quality of camera trajectories."
40,"Page 40 of the document discusses the S-SRSD (Spatial Self-Regulatory Strategy Development) framework and its application in spatial sensing. The framework, developed by the National University of Singapore, focuses on developing self-regulatory strategies to improve spatial sensing skills. These strategies include planning, monitoring, and evaluating one's own performance. The document also provides contact information for Dr. Tian Jing, the developer of the S-SRSD framework."
