Page Number,Summary
1,"This document is about the use of deep learning in problem solving and its evolution over time. It is written by Nicholas Ho, a lecturer and consultant at the Institute of System Science at the National University of Singapore. The document is copyrighted by the university and discusses the changes in deep learning before and after 2024."
2,"Nicholas Ho is an Artificial Intelligence System (AIS) lecturer at NUS ISS and has expertise in Robotics Systems, Autonomous Robots and Vehicles, and Human Robot Systems Engineering. He also works as a consultant for a SME manufacturing company, providing services such as designing an Intelligent Voice Prosthesis Manufacturing Workcell system and optimizing its performance using AI and IoT technologies. He holds a BEng and PhD from NUS's School of Mechanical Engineering."
3,"This page discusses the potential future of classrooms in 2024, as envisioned by the National University of Singapore. It highlights the use of technology and digital tools in the classroom, such as virtual reality and artificial intelligence, to enhance learning and engagement. It also mentions the importance of personalized learning and collaboration among students. The document emphasizes the need for teachers to adapt and embrace these changes in order to effectively prepare students for the future."
4,"The article discusses China's efforts to implement artificial intelligence (AI) in classrooms. This includes using deep learning technology to translate brain signals into an attention score, recognize faces for attendance, identify gestures such as raising hands or using mobile phones, and even quiz students through robots. The use of sensory data can also generate and inform students' health status. These advancements are expected to be implemented by 2024, with the National University of Singapore holding the rights to the technology."
5,"The article discusses how increased attention in the classroom can lead to improved grades. This is achieved through the use of technology, such as automated attendance taking, which reduces the workload for teachers and allows them to focus more on teaching. Additionally, the article mentions the role of parents in monitoring their children's education, as some responsibility is shifted to them instead of solely relying on teachers. This approach has shown satisfactory results in China's efforts to lead in AI technology."
6,"The use of technology in classrooms, particularly for surveillance and data collection, raises concerns about privacy and obtaining parental consent. There is also the risk of negative societal reactions to these technologies, such as undesirable social media feeds. Additionally, there are concerns that students may feel pressured under constant surveillance. These issues highlight the potential challenges and ethical considerations surrounding the use of technology in classrooms."
7,"Deep learning is a type of artificial intelligence that uses algorithms to analyze large amounts of data and learn from it in order to make predictions or decisions. It is a rapidly growing field and has shown promising results in various applications such as image and speech recognition. However, there are still challenges in terms of data availability and interpretability of the models. The National University of Singapore is actively involved in research and development of deep learning techniques."
8,"The timeline of deep learning shows the development of this field from its early beginnings in the 1950s to its current state in 2024. Key milestones include the creation of the first artificial neural network, the development of backpropagation algorithm, and the introduction of convolutional neural networks. The timeline also highlights major breakthroughs and advancements in deep learning, such as the use of GPUs for faster processing and the emergence of deep learning applications in various industries."
9,"The image on page 9 shows a simplified illustration of a biological neuron with a synapse. A neuron is a specialized cell in the nervous system that transmits electrical and chemical signals. The cell body contains the nucleus and other organelles, while the dendrites receive signals from other neurons. The axon carries signals away from the cell body and ends in a synapse, which is a small gap between neurons. The synapse allows for communication between neurons through the release of neurotransmitters. This illustration was created by the National University of Singapore and is used with all rights reserved."
10,"The article discusses the rapid development of artificial intelligence (AI) in China and its potential impact on society. It highlights the country's goal of becoming a global leader in AI by 2025 and the government's efforts to support and promote its growth. The article also explores the evolution of AI from its origins in neuroscience to the development of neural networks, which are now used in various industries. It mentions the concerns surrounding AI, such as job displacement, and the need for ethical guidelines to be established. Overall, the article emphasizes the significant role that AI will play in shaping the future of China and the world."
11,"The McCulloch-Pitts Neuron, created by neuroscientist Warren McCulloch and logician Walter Pitts in 1943, was the first artificial neuron. It was designed to mimic the basic functions of a biological neuron, including receiving and integrating inputs, and producing an output based on a threshold. The neuron has a simplified structure compared to a biological neuron, with inputs represented as binary values and a threshold function used to determine the output. This concept laid the foundation for future developments in artificial neural networks."
12,The McCulloch-Pitts neuron is the first artificial neuron and can be used to represent Boolean functions such as AND and OR. It is represented by a mathematical equation and has a threshold value that determines its output. This neuron was developed in 1943 and is still used in modern artificial neural networks. It was created by Warren McCulloch and Walter Pitts and is a simplified model of the biological neuron. The McCulloch-Pitts neuron has one input and one output and can only produce binary outputs of 0 or 1.
13,The McCulloch-Pitts Neuron was the first artificial neuron created by Warren S. McCulloch and Walter H. Pitts Jr. It only accepts boolean values as inputs and does not have a learning algorithm to update the neurons. This was developed in 1943 and is still used as a foundation for modern neural network models.
14,"The Perceptron, an artificial neuron, was first introduced by Frank Rosenblatt and later improved by Minsky and Papert. Unlike previous models, the Perceptron can handle real inputs rather than just boolean values. This makes it a more versatile and useful tool for data analysis and machine learning."
15,"The Mark I Perceptron, developed at the Cornell Aeronautical Laboratory, was an improved version of the artificial neuron Perceptron. Its creator, Frank Rosenblatt, demonstrated that artificial neurons could learn from data. He also created a supervised learning algorithm and implemented the Perceptron in custom hardware. This hardware was able to learn and correctly classify simple shapes with 20x20 pixel-like inputs."
16,"The Mark I Perceptron was the first implementation of the artificial neuron Perceptron, developed at the Cornell Aeronautical Laboratory. It consisted of a single layer perceptron, with neurons arranged in a single line for each layer, represented by a 1D array. This design was improved upon in later versions, such as the multi-layer perceptron."
17,"In 1969, Marvin Minsky and Seymour Papert published a book called 'Perceptrons' which showed that a single perceptron is not capable of performing the XOR function. While multiple layers of perceptrons, also known as MLP, can perform XOR, the proposed learning algorithm did not work for it. This led to the onset of the first AI winter."
18,"The article discusses the history of the development of learning algorithms for artificial neural networks. It mentions that in the 1960s and 1980s, researchers were able to derive a solution for the multilayer perceptron, but this was not widely known. In 1986, Rumelhart, Hinton, and Williams published their method for training neural networks, which they called ""backpropagation."" This marked the beginning of the thaw of the AI winter, as neural networks became a popular and successful approach for solving problems."
19,"The learning algorithm for multilayer perceptrons was discovered in the 1980s, after a period of stagnation in the field of AI known as the ""AI winter."" Prior to this, several researchers had independently derived the solution, but it was not widely known. In 1986, Rumelhart, Hinton, and Williams published the method in Nature, calling it ""backpropagation."" This algorithm is crucial for the success of multilayer perceptrons in neural network training."
20,"The learning algorithm for artificial intelligence has been revived after a period of stagnation known as the ""AI winter."" At the National University of Singapore, a visual illustration of the backpropagation process is used to explain how the learning algorithm works. This process involves a forward pass, computing error, a backward pass, and updating weights."
21,"In 1989, there were two significant developments in the field of neural networks and deep learning. First, it was proven that multilayer feedforward networks could approximate any function, making them universal approximators. This was a major breakthrough in the field. Second, LeNet was proposed and put into use for recognizing numbers. LeNet combined a neural network with convolutional layers, which allowed for weight sharing and improved performance. These advancements set the stage for further developments in the field of deep learning."
22,"In the late 1980s, it was discovered that deep neural networks trained with backpropagation were difficult to train and did not perform as well as networks with fewer layers. This led to the rise in popularity of support vector machines (SVM) and random forests, which were found to be more effective."
23,"The timeline of deep learning can be divided into three main periods: the first winter in the late 1960s, the second winter in the late 1980s, and the current period. During the first winter, there were significant developments in neural networks, but they were limited by computational power and data availability. The second winter saw a decline in interest and research in deep learning due to the inability to effectively train deep neural networks using backpropagation. However, in 2012, a breakthrough occurred with the use of deep convolutional neural networks for image recognition, leading to the current period of rapid growth and advancement in deep learning."
24,"The document discusses ImageNet, a large-scale dataset created by the Stanford Vision Lab. It contains 1.2 million images and 1000 object categories. The dataset is commonly used for image classification tasks and has been a key factor in the development of deep learning models. It is constantly updated and has been used in various computer vision competitions. The document also mentions that the dataset is copyrighted by the National University of Singapore."
25,The ILSVRC (ImageNet Large Scale Visual Recognition Challenge) was established in 2010 as a competition for teams to test their algorithms on a given dataset and achieve the highest accuracy in visual recognition tasks. The training dataset for ILSVRC contains 1000 object categories and 1.2 million images. This competition is organized by the National University of Singapore and is still ongoing.
26,".2%

In 2024, a team led by Geoffrey Hinton from the National University of Singapore entered the ILSVRC2012 competition and achieved an error rate of 15.3% with their AlexNet model, which was significantly better than the next closest error rate of 26.2%. This marked a significant breakthrough in image classification using deep convolutional neural networks."
27,"The success of AlexNet, a deep convolutional neural network in computer vision, can be attributed to several factors. While the net structure is not the most important point, the use of Rectified Linear Unit (ReLU) activation function and dropout technique played a significant role. Additionally, the implementation on GPU through CUDA resulted in a 20x time difference, making it more efficient. The availability of large amounts of data also contributed to the success of deep learning in computer vision."
28,"ReLU is a popular activation function in deep learning due to its simplicity and ability to introduce sparsity representation in each layer. This means that only important neurons contribute, making it easier to calculate compared to other activation functions. This has led to its widespread use in various neural network architectures."
29,"ReLU is a popular activation function in neural networks due to its simplicity and efficiency. It has a range of (0, ∞) and a low computational cost compared to other activation functions like sigmoid and tanh. ReLU also does not suffer from the vanishing gradient problem, making it suitable for fast convergence. However, it can lead to saturation in some cases. ReLU is commonly used in hidden layers of convolutional and feedforward neural networks, while sigmoid and tanh are often used in output layers of binary classification and recurrent neural networks."
30,"The article discusses the reasons why backpropagation, a key technique in deep learning, failed in the past. One major reason was the lack of large labelled datasets, which are necessary for deep learning applications. Additionally, computing power was much slower in the past, making it difficult to process large amounts of data. Weight initialization was also a problem, with weights being set to 0 or 1 instead of being randomly assigned. Finally, the use of incorrect activation functions also hindered the success of backpropagation. However, with advancements in computing and access to larger datasets, backpropagation has become a powerful tool in deep learning."
31,"The National University of Singapore (NUS) has been committed to providing quality education and fostering a vibrant and inclusive learning environment since its establishment in 1905. NUS has consistently ranked among the top universities in the world and has a strong reputation for research excellence. The university offers a diverse range of academic programs and opportunities for students to engage in experiential learning and global experiences. NUS also values community engagement and encourages students to contribute to society through various community service initiatives. As a leading institution of higher education, NUS continues to innovate and adapt to the changing needs of society while upholding its core values of excellence, integrity, and service. 

NUS, established in 1905, is dedicated to providing quality education and"
32,"Deep learning is a type of artificial intelligence that involves training a neural network to learn and make predictions based on large amounts of data. It has been increasingly used in various industries and has shown great potential in solving complex problems. However, there are still challenges and limitations in deep learning, such as the need for large amounts of data and computational resources, as well as the lack of interpretability in the decisions made by the neural network. Ongoing research and advancements in deep learning are expected to address these issues and further improve its capabilities."
33,"The article discusses the differences between artificial intelligence, machine learning, and deep learning. It mentions that Arthur Samuel coined the term 'machine learning' in 1959 while Rina Dechter introduced the term 'deep learning' in 1986. It also notes that machine learning and statistics are closely related, leading to the suggestion of the term 'data science' by Michael I. Jordan to refer to the overall field."
34,"The article discusses the differences between machine learning and deep learning. It explains that a feature is a numerical or vector representation of an input, and it is used to describe something about the input. Machine learning involves using algorithms to learn patterns and make predictions based on features, while deep learning uses artificial neural networks to process large amounts of data and identify complex patterns. Deep learning requires more data and computing power, but can often achieve better results than traditional machine learning methods."
35,"The article discusses the creation of a HTML5 canvas drawing app using JavaScript. It outlines three main categories for the app: create, identify, and act. The source, National University of Singapore, is credited and the use of reinforcement learning is mentioned. Another source, blog.ss8.com, is also referenced."
