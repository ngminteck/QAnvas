Page,Summary
Page 1, Problem Solving Using Pattern Recognition is a problem solvable using pattern recognition software . The solution was developed at the National University of Singapore .
Page 2, National University of Singapore 2: Introduction to Principal Component Analysis method . Introduction to Linear Discriminant Analysis method. Hands on workshop: Hands-on workshop .
Page 3, The dimension of a dataset is given as n-rows by p-columns . Netflix rating: 18kx480k; Films x Customer ratings; Image data: 800x800x4 image = 800x3200 .
Page 4," Problems with High Dimensionality: Takes up storage space and hard to visualise, takes up space and takes up training time . Computationally complex models result in complex models, harder to interpret and run into the Curse of Dimensionality ."
Page 5," Many types of data analysis become significantly harder as the dimensionality of the data increases . For classification, this can mean that there are not enough data objects to allow the creation of a model that reliably assigns a class to all possible data objects"
Page 6, Many data mining algorithms work better if the dimensionality - number of attributes in the data- is lower . This is partly because dimensionality reduction can eliminate irrelevant features and ishlyreduce noise and partly because of the curse of
Page 7, There are two different approaches to reducing data dimensionality . One is to remove features that do not  bring much information or add no new information . Another is to extract features to form new independent variables .
Page 8, The goal of feature selection in machine learning is to find the best set of features that allows one to build useful analytical models . The best feature selection is one that does not contain redundant and irrelevant features (i.e. less
Page 9," Tan, Steinbach & Kumar, Introduction to Data Mining (2nd Ed), Addison-Wesley . Tan and Kumar used data mining techniques to select feature data mining methods ."
Page 10, Filter methods select features from a dataset based on the intrinsic characteristics of the features . These methods are simple yet powerful to quickly remove features and are less computationally expensive than wrapper methods .
Page 11, The Chi-Square test of independence is a statistical method used to determine if  there’s a significant association between two categorical variables . Remove numerical features that are highly corelated (r > 0.9)
Page 12, Wrapper method employs a greedy search strategy aimed to find the best possible combination of features that results in the best machine learning  performance . It is computationally expensive and can be impractical for very large datasets .
Page 13," The procedure stops when the desired number of selected features is reached . In general, forward and backward selection do not yield equivalent results . One may be much faster than the other depending on the requested number of features ."
Page 14," Machine Learning to Identify Flexibility of Class A GPCR Inhibition Biomolecules 2020, 10,  - 10,454.454."
Page 15, Recursive feature elimination (RFE) is a popular technique that iteratively selects the best number of features . The goal of RFE is to select features by considering smaller and smaller sets of features to select .
Page 16, RFE vs BFS: BFS differs from RFE and SelectFromModel in that it does not require the underlying model to expose a coef_ or feature_importance_ attribute .
Page 17, Embedded methods performs the feature selection within the machine learning algorithm itself during the model training . The method solves issues with the filter and wrapper methods: 'Take into consideration the interaction of features like wrapper methods do'
Page 18, Embedded techniques are implemented using algorithms that have their built-in feature selection methods . LASSO and Ridge regression have inbuilt penalization functions to reduce overfitting .
Page 19, Random Forests are useful for feature selection in addition to being effective classifiers . Some features contain the most informative subset used to predict a target class . Importance score is calculated as the number of times that the feature was selected for
Page 20, Features are selected before the application of machine learning . Feature selection occurs intrinsically as part of model building (ML) Algorithm itself decides which attributes to use and which to ignore .
Page 21, Feature selection is embedded  during the model building  process . Selection is done by observing each iteration of model . Selection does not always always always the best .
Page 22, Feature Extraction Method was developed at the National University of Singapore . It is the result of a study by the university of Singapore at the centre of its research at the university .
Page 23," Principal Component Analysis was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics . PCA concepts can be challenging even for the mathematical oriented ."
Page 24, PCA Intuition PC1 = 0.314*Income +                 0.237*Education +                                   - 0.484*Age + Age + age + age            0.
Page 25, Principal Component Analysis uses a vector space transform to project data from a high-dimensional space into a lower-dimensional . PCA uses a linear combination of components to reduce a correlated multidimensional data set (xi) to an uncor
Page 26," Principal Components capture the maximum amount of variance in the data . 1st component captures the most, followed by 2nd, then 3rd, etc. Principal Components are orthogonal (uncorrelated) to each other . Principal Components"
Page 27," National University of Singapore 27-year-old's Principal Component Analysis Method . Feature 1, 2, 3, 4, 5, and 6-8 were created by the National Institute of Singapore . Feature 2, 5 and 7 were created"
Page 28, Principal Component Analysis Method tries to find a new axis that captures the maximum variance within the data once it is projected onto the new axis: The new axis is found to capture the maximum variance within the data .
Page 29, Concept of eigenvectors and eigenvalues was developed at the National University of Singapore 29 years ago . The concept was created by the university of Singapore's National Institute of Singapore .
Page 30, The new axes are called principal ipientcomponents (PCs) The 1st PC accounts for the maximum ipientvariance in the data that has not been accounted by the 1st variable . The 2nd PC account for the
Page 31," The Principal Components are considered ipientas new variables, and they are all  uncorrelated . If a substantial amount of the total                 variance in the data is accounted for by a                 few principal components then one can"
Page 32, National University of Singapore 32PCA EXAMPLE EXAMPL . 32 PCA EXample was created by the university of Singapore . 32PCa EXAMple is a 32PCPA example of a 32-PCA
Page 33, National University of Singapore's PCA example: Loan Application . PCA Example: Income: Yearly Income.                PCA Example of Loan Application: #credit cards owned.                                                                                                             - 
Page 34, The National University of Singapore 34-year-old example is based on data from PCA . Run PCA and examine the outputs of the PCA function . Check for appropriateness of PCA Method .
Page 35, National University of Singapore 35-1.1.2.3.4.5.6.7.8.5 .
Page 36, National University of Singapore 36-year-old record-breaking statistics . Statistics show that Singapore's economy has been hit hard by a recession in recent years .
Page 37, National University of Singapore's 37-correlation Matrix shows correlation between correlation and correlation . The correlation matrix was created by the National Institute of Singapore . The Matrix Matrix is based on a correlation of correlation between a person's activity and their activity
Page 38," National University of Singapore 38-Scatter Plots . 38-scatter plots were created by the university's 38,000-strong team ."
Page 39, National University of Singapore 39C Correlation Heatmap . The heatmap was created by the university of Singapore's 39C correlation heatmap . It was created in response to a study by the National Institute of Singapore .
Page 40, Check for appropriateness for PCA with the help of the National University of Singapore 40.2. Check for appropriate use of PCA before using PCA or PCA .
Page 41, Kaiser-Meyer-Olkin (KMO) test is a statistical measure that helps to  determine how suitable your data is for PCA . KMO returns values between 0 and 1.
Page 42," Bartlett’s Test of Sphericity tests the hypothesis that your correlation matrix is an identity matrix, which would indicate that your variables are unrelated and therefore unsuitable for structure detection . If true, it implies"
Page 43, The KMO and Bartlett’s Test provides a minimum standard which should be passed before Principal Component analysis can be conducted successfully .
Page 44, Run the Principal Component Analysis with the Principal Components of the PC . The Principal Components were analyzed by the National University of Singapore . The analysis was based on the data from the PC's Principal Components .
Page 45, National University of Singapore 45PPCA Transformation Transformation . Principal   Principal  Dr. Michael Tan Tan is Singapore's first female university president .
Page 46, National University of Singapore 46Principal Components extracted . Principal Components extracted from Singapore University . Singapore University Singapore is Singapore's largest university university .
Page 47, Principal Components can be interpreted to see how much each original ogenous variable contributes (i.e. correlates) to each PC . We can compute this correlation by taking the square root of the                 eigenvalues and multiply them by
Page 48,"Component loadings are interpreted as correlation coefficients between the  original variables and components . It is analogous to Pearson's r, and the squared loading is the percent of variance in that variable explained by the component ."
Page 49, Loading Plots allow you to identify which variables have the largest effect  on each component . Loadings close to -1 or 1 indicate that  the variable strongly influences the ipient component. The Loading Plot graphs the loadings of 
Page 50, How Many Components (Kaiser Rule) how many Components to retain? How many Components are to retain and how many components are required to retain . How many components to retain is how much variance is to be explained?
Page 51, Eigenvalue represent the total amount of variance that is explained by a given principal component . Eigenvalues are standardized and hence it  carries one unit of information .
Page 52, The Kaiser Rule is perhaps the most widely used criterion for selecting components to keep . It is based on the idea that a component should be considered insignificant if it does worse than a single field . Each single field contains one unit
Page 53, The first 4 components explains 90% of the variability with 10% loss of information . Eigenvalues can also be expressed in terms of a percentage of the total variance of the original fields .
Page 54," The Scree Plot shows the eigenvalues vs the components in decreasing order . Look for a large drop, followed by a  “plateau” in eigen values – i.e. a                 bend or"
Page 55, Communality represents the total amount of variance of a specific field that is jointly  jointly  accounted for by all the components . High communality values indicate that the  original field is sufficiently explained by the reduced PCA solution .
Page 56," National University of Singapore 56.5.5 . Proceeding to the next step, the university will take a further step ."
Page 57, The eigenvectors are comprised of coefficients corresponding to each variable . The coefficients indicate the relative weight of each variable in the component . Principal components are the linear combinations of the original variables that account for the variance in the data .
Page 58, The scores represent the number  of standard deviations above or below  the overall mean where each customer lies . The scores are standardized with mean=0 and SD=1 .
Page 59," Use the score plot to assess the data structure and detect clusters, outliers, and trends . Clusters: Observations that are close to each other on the plot have similar profiles . Outliers that are far from the  main"
Page 60, There should be 150+ cases and there should be  at least five cases for each variable . PCA assumes a linear relationship between features .
Page 61, The goal of PCA is to extract the smallest number of components which account for as much as possible of the information of the original .
Page 62," 62.7% of PC images are compressed compressed, resulting in loss of data loss . 62% of images were compressed, meaning loss was lost in PC images . 62-7% loss was loss in PC Images Compression . 62"
Page 63, The process of performing PCA from scratch is as follows: read in the dataset and prepare for PCA . Plot the results and then decide on the number of components to retain .
Page 64, National University of Singapore 64.LINEAR DISCRIMINANT ANALYSIS (LDA) is published by the National Institute of Singapore .
Page 65, What if you had access to class labels? We not only reduce the number of features BUT at the same time create class separability . This is most useful for multi-variate classification-type problems .
Page 66, Discriminant Analysis is a classic method of classification . There are lots of similarities between LDA and PCA . LDA takes the class label into consideration whereas PCA ignores them .
Page 67, The LDA Method finds a new dimension that yields the maximum separation between the class means and minimum variance within class .
Page 68," The goal of LDA is to find a projection where the projected data from two classes do not overlap as far as possible . Similar to PCA, the data is being projected onto a new dimensional line ."
Page 69, 69LDA method for more than 2 variables . 69D1 d2.2.3. = Mean value of the projected data. 69D2.4: D2: d2: 1-2: 2-2
Page 70, This data set has 178 examples from three classes . Each example consists of 13 real-valued features . The data set is based on the National University of Singapore 70 years old .
Page 71, Results of LDA vs PCA were announced at the National University of Singapore 71.7 per cent . Results of PCA vs LDA were also announced in Singapore . LDA and PCA have been compared to each other for more
Page 72, PCA maximizes the variation explained in the data with LDA . PCA ranks the components according to their importance . LDA ranks them according to the largest amount of variance explained between the classes .
Page 73," LDA produces c-1 discriminant vectors, where c = number of classes . Assumes that the data is uni-modal gaussian (normally distributed) LDA will fail if the discriminatory information is not in the"
Page 74," National University of Singapore 74Demo, Exercises and Workshop is a workshop at the university's National Institute of Singapore ."
Page 75, National University of Singapore 75 per cent of Singapore students are Singaporeans . Singaporeans are proud of their achievements at the university .
Overall Summary, The curse of dimensionality refers to the phenomenon that many types of data analysis  become significantly harder as the ipientdimensionality of the data increases . The dimension of a dataset is given as n-rows by p-
