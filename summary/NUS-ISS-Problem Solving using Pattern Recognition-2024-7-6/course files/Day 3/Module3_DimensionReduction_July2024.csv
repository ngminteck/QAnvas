Page,Summary
Page 1,Copyright National University of Singapore 1 Problem Solving Using Pattern Recognition 3 . Dimensionality Reduction : a method for reducing dimensionality .
Page 2,Hands on workshop aimed at tackling high dimension problems . workshop will include: Feature Selection Methods; Principal Component Analysis method; Linear Discriminant Analysis method . workshops will be held at the National University of Singapore
Page 3,"the dimension of a dataset is given as n-rows by p-columns . we are usually more concerned about p, the number of features . data with a small p is referred"
Page 4,National University of Singapore 4 Problems with High Dimensionality . Takes up storage space and is hard to visualise .
Page 5,"as dimensionality increases, the data becomes increasingly sparse in the space that it occupies . for clustering, the definitions of density and distance between points become less meaningful ."
Page 6,many data mining algorithms work better if the dimensionality - number of attributes in the data-is lower . this is partly because dimensionalite reduction can eliminate irrelevant features and reduce noise . a reduction of dimensional
Page 7,there are two different approaches to reducing data dimensionality . remove all features that do not bring much information or add no new information . Feature Extraction Methods: Keep all original features by extracting them .
Page 8,best set of features is one that does not contain redundant and irrelevant features (i.e. less noise) Feature selection in machine learning is a goal of the national university of Singapore .
Page 9,"Copyright National University of Singapore 9 Feature Selection Methods Tan, Steinbach & Kumar, Introduction to Data Mining (2nd Ed), Addison-Wesley ."
Page 10,filter methods select features from a dataset based on the intrinsic characteristics of the features . Statistical techniques are often used to evaluate the relationship between predictor and target variable .
Page 11,"if a feature is highly associated to the target variable, then we retain it . if the p-value (associated with the F-statistic) is 0.05, we reject the feature and select it"
Page 12,wrapper methods work by systematically evaluating a subset of features using a machine learning algorithm . this method employs a greedy search strategy aimed to find the best possible combination of features . it is computationally
Page 13,forward Feature Selection is a greedy search that iteratively finds the best new feature . backward Feature Elimination follows the same idea but works in the opposite direction . forward and backward selection do not yield equivalent
Page 14,"Machine learning to Identify Flexibility Signatures of Class A GPCR Inhibition Biomolecules 2020, 10, 454 ."
Page 15,recursive feature elimination (rFE) is a popular technique that iteratively selects the best number of features . the goal of RFE is to select features by considering smaller and smaller sets of features.
Page 16,machine learning differs from RFE and SelectFromModel in that it does not require the underlying model to expose a coef_ or feature_importances_ attribute.
Page 17,Embedded methods perform the feature selection within the machine learning algorithm itself . a machine learning model is trained to derive feature importance from the model . it also removes non-important features using the derived feature importance
Page 18,Embedded techniques are implemented using algorithms that have their own built-in feature selection methods . LASSO and RIDGE regression have inbuilt penalization functions to reduce overfitting .
Page 19,"some features contain the most informative subset used to predict a target class . if a feature is selected very often as the best split, it is very likely to be an informative feature . Important features are those with higher scores"
Page 20,feature selection occurs intrinsically as part of model building (ML) the algorithm decides which attributes to use and which to ignore in order to maximize its performance .
Page 21,Selection is done by observing each iteration of model training Computation Speed Fast Very High Computation time with many features Somewhere between Filter and Wrapper methods Overfitting problem Avoids overfitting but sometimes may fail to select
Page 22,Copyright National University of Singapore 22 PRINCIPAL COMPONENT ANALYSIS Feature Extraction Method .
Page 23,"PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem . it was later independently developed and named by Harold Hotelling in the 1930s . concepts can be challenging even"
Page 24,National University of Singapore 24 PCA Intuition PC1 = 0.314*Income + 0.237*Education + 0.484*Age +0.466*Residence +0.459*Em
Page 25,"Principal components are linear combinations of the xi . aij are the weights, or coefficients, of each component ."
Page 26,"Feature aij coefficients are linear combinations of the original features . 1st component captures the most, followed by 2nd, then 3rd, etc."
Page 27,Copyright National University of Singapore 27 Principal Component Analysis Method Feature 1 Feature 2 Feature 3 Feature 4 Feature 5 Feature 6 Feature 7 Feature 8 Feature 9 Feature 10 Feature 11 Feature 12
Page 28,PCA tries to find a new axis that captures the maximum variance within the data once it is projected onto the new axes . https://i.stack.imgur.com/lNHq
Page 29,Copyright National University of Singapore 29 Concept of eigenvectors and eigevalues . Copyright Universitiet de Singapore 29 .
Page 30,the new axes are called principal components (PCs) the 1st PC accounts for the maximum variance in the data . the 2nd PC accounts . for maximum variance that has not been accounted by p-1 variables
Page 31,"the Principal Components are considered as new variables, and they are all uncorrelated . if a substantial amount of the total variance in the data is accounted for by a few principal components then one can use these fewer"
Page 32,Copyright National University of Singapore 32 PCA EXAMPLE - SINGAPORE . Copyright NUS Singapur .
Page 33,Copyright National University of Singapore 33 PCA Example: Loan Application Income: Yearly Income Education: no. of years Age: years Employer: years at current coy Savings: current saving Debt: current debts Credit cards:
Page 34,National University of Singapore 34 Approach for this example 1. Basic Data understanding • Summary statistics • Correlation Matrix/Heatmap 2. Check for appropriateness of PCA Method • Bartlett’s Sphericity Test •
Page 35,Copyright National University of Singapore 35 1.Basic Data Understanding Summary statistics Frequency Distributions Correlation Matrix/Heatmap
Page 36,Copyright National University of Singapore 36 Sommaire Statistics - Singapur - 36 . Copyright - Singapore - 37 .
Page 37,Copyright National University of Singapore 37 Correlation Matrix . Copyright nuu.sg .
Page 38,Copyright National University of Singapore 38 Scatter Plots . Copyright Universitie Nationale de Singapour
Page 39,Copyright National University of Singapore 39 Correlation Heatmap - Singapur . Copyright Universitiet de Singapore .
Page 40,Copyright National University of Singapore 40 2.Check for appropriateness for PCA Kaiser-Meyer-Olkin Test Bartlett’s Sphericity Test .
Page 41,the Kaiser-Meyer-Olkin (KMO) test is a statistical measure that helps to determine how suitable your data is for PCA . it returns values between 0 and 1 . low KMO value closer
Page 42,"bartlett’s test of sphericity tests the hypothesis that your correlation matrix is an identity matrix . if true, it implies the existence of redundancies between the variables such that PCA can summarize this"
Page 43,National University of Singapore 43 Running the KMO and Bartlett’s Test These 2 tests provide a minimum standard which should be passed before a Principal Component analysis can be conducted successfully .
Page 44,Principal component analysis Principal Components Interpretation of PC (Loading matrix) . Copyright National University of Singapore 44 3.
Page 45,Copyright National University of Singapore 45 PCA Transformation Principal Components . Copyright Universitaet of Singapore .
Page 46,Copyright National University of Singapore 46 Principal Components extracted . Copyright national university of singapore .
Page 47,"copyright National University of Singapore 47 Principal Components can be interpreted to see how much each original variable contributes (i.e. correlates) to each PC . the result is the loading matrix, which tells you how"
Page 48,Component loadings are interpreted as correlation coefficients between variables and components . the squared loading is the percent of variance explained by the component .
Page 49,Loading Plots allow you to identify which variables have the largest effect on each component . loadings close to -1 or 1 indicate that the variable strongly influences the component; 0 indicates that it has a weak influence
Page 50,Copyright National University of Singapore 50 4 Eigenvalues (Kaiser Rule) Total variance explained Scree Plot Communalities Matrix.
Page 51,Eigenvalues are the sum of squared component loadings across all items . eigenvalues can be extracted using explained_variance_method .
Page 52,the Kaiser Rule is perhaps the most widely used criterion for selecting which components to keep . it is based on the idea that a component should be considered insignificant if it does worse than a single field .
Page 53,Eigenvalues can also be expressed in terms of a percentage of the total variance of the original fields . first 4 components explains 90% of the variability with 10% loss of information .
Page 54,"Scree Plot shows the eigenvalues vs the components in decreasing order . beyond the elbow point, the variances explained tapers off . if the scree is “not ideal” –"
Page 55,high communality values indicate that the original field is sufficiently explained . low communality implies an insignificant contribution to the formation of the PCA Solution .
Page 56,Copyright National University of Singapore 56 5Proceeding to the next step: Copyright . Copyright: National University Singapore 56 5.
Page 57,Principal components are the linear combinations of the original variables that account for the variance in the data . the eigenvectors comprised of coefficients corresponding to each variable are used to calculate the principal component scores .
Page 58,the scores are standardized values with mean=0 and SD=1 . they represent the number of standard deviations above or below the overall mean where each customer lies .
Page 59,"this score plot graphs the scores of the first and second PCs . if the first two components account for most of the variance in the data, you can use the score plot to assess the data structure . clusters:"
Page 60,PCA assumes a linear relationship between features . too many one-hot-encoding will result in extremely sparse and information-parched multidimensional space .
Page 61,"goal of PCA is to extract the smallest number of components which account for as much as possible of the information of the original features . to illustrate information lost, we will reconstruct the original data using only the saved principal components ."
Page 62,"Copyright National University of Singapore 62 Data loss in Image Compression 75%, 9 PC 85%, 18 PC 95%, 60 PC 100% ."
Page 63,the process of performing is as follows (like the example we did earlier): 1. Read in the dataset 2. Basic Data Understanding and Preparation 3. Check appropriateness of data for PCA 4. Call the PCA algorithm (from skle
Page 64,Copyright National University of Singapore 64 LINEAR DISCRIMINANT ANALYSIS (LDA)
Page 65,Copyright National University of Singapore 65 What if you had access to class labels? we not only reduce the number of features BUT at the same time create class separability.
Page 66,"there are lots of similarities between LDA and PCA . LDA takes the class label into consideration whereas PCA ignores them . since it uses the class information, LDA is a “supervised learning” method ."
Page 67,LDA finds a new dimension that yields • Maximum separation between the class means • Minimum variance within class means .
Page 68,the goal of LDA is to find a projection where the projected data from two classes do not overlap as far as possible . the projection in Figure 2 is preferred .
Page 69,Copyright National University of Singapore 69 LDA method for more than 2 variables d1 d2 d3 = Mean value of the projected data .
Page 70,copyright National University of Singapore 70 Example This data set has 178 examples from three classes. Each example consists of 13 real-valued features.
Page 71,Copyright National University of Singapore 71 Results of LDA vs PCA . Copyright n.u.s.
Page 72,PCA and LDA rank the components according to their importance . PCA ranks them according to the largest amount of variance explained in the data . LDA ranks them based on the largest variance explained between the classes .
Page 73,"LDA is not stable for problems with small sample size . LDA produces c-1 discriminant vectors, where c = number of classes . if the discriminatory information is not in the mean but in the variance of"
Page 74,"Copyright National University of Singapore 74 Demo, Exercises and Workshops . Copyright: National University Singapore ."
Page 75,Copyright National University of Singapore 75 Thank You! Copyright Universitiet Nationale de Singapur 75 Thank you!
Overall Summary,"the goal of LDA is to find the smallest number of components that account for as much as possible of the variance of the data . it also aims to maximize the separation (or discrimination) between the classes in the data, which can be useful for multi-variate classification-type problems ."
