Page,Summary
Page 1,Dr Zhu Fangming NUS-ISS National University of Singapore fangming@nus.edu.sg .
Page 2,DAY 2 AGENDA 2.1 Solving Pattern Recognition Problems Using Supervised Learning Techniques (II): Decision Trees Neural networks Support Vector Machines 2.2 Pattern Recognition Workshop 2
Page 3,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved .
Page 4,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 4 Supervised Learning Techniques (II)
Page 5,decision tree is a flow-chart-like tree structure . internal node performs a test on an attribute . leaf node represents a class label .
Page 6,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 6 Decision Tree .
Page 7,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved.
Page 8,Basic Algorithm: Quinlan’s ID3/C4.5/C5.0 - create a root node for the tree . if all examples from S belong to the same class Cj - then label the root
Page 9,"ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. At start, all training data are at the root."
Page 10,ATAS-PSUPRDay2.pptV4.0 . All Rights Reserved 10 Heuristic Search .
Page 11,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore .
Page 12,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 12
Page 13,heuristic: choose attribute that produces the “purest” nodes . popular impurity criterion: information gain increases with the average purity of the subsets .
Page 14,"All Rights Reserved 14 Entropy • S - training set, C1,...,CN - classes . entropy in binary classification problems E(S) = - p+ log2p+ -"
Page 15,"Gain(S,A) - expected reduction in entropy of S due to sorting on A . aims to minimize the number of tests needed to classify a new object ."
Page 16,"ATAS-PSUPRDay2.pptV4.0 . All Rights Reserved 16 Which Attribute to Select? 247.0)Outlook""gain("" = 029.0)e""Temper"
Page 17,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved.
Page 18,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 18 Continuing to Split Finally .
Page 19,ATAS-PSUPRDay2.pptV4.0 . All Rights Reserved . achieving maximum information gain for A is selected as the split-point for A .
Page 20,ATAS-PSUPRDay2.pptV4.0 . All Rights Reserved 20 Stopping Criteria .
Page 21,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved.
Page 22,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore . Information gain is maximal for ID code ID Outlook Temperature Humidity Windy Play?
Page 23,Gain ratio is a modification of the information gain that reduces its bias on highly -branching attributes . it takes number and size of branches into account when choosing an attribute .
Page 24,"gini(T) is defined as where pj is the relative frequency of class j in T . if a data set T contains examples from n classes, the class in T is skewed"
Page 25,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 25 Overfitting / Overtraining .
Page 26,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. all rights reserved.
Page 27,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 27 Overfitting and Tree Pruning.
Page 28,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved.
Page 29,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 29 Decision Tree Summary.
Page 30,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved.
Page 31,Neural Networks (NN) are biologically inspired and attempt to build computational models that operate like a human brain . these networks can “learn” from the data and recognize patterns .
Page 32,Neural Networks http://www.asimovinstitute.org/neural-network-zoo/ .
Page 33,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved.
Page 34,"ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 34 General Architecture of Neural Networks • Framework (in general, but not for all NNs"
Page 35,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 35 General Architecture of Neural Networks.
Page 36,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 36 Activation functions.
Page 37,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved.
Page 38,ATAS-PSUPRDay2.pptV4.0 . All Rights Reserved 38 Neural Network Learning /Training .
Page 39,Multilayer Perceptron (MLP) with Backpropagation Learning . Propagate signals forward and then errors backward . Weights in hidden layers adjusted to reduce aggregate errors .
Page 40,too many nodes or layers can be very hard to train (requiring many samples & long training time) LOW RISK 0 o o 1 o 2 o 3 0 1 0 2 0 3
Page 41,"backpropagation algorithm is a weight-adjustment algorithm . it is based on a training pattern pair (xp, tp) and a network output pattern (zp) backpropagate"
Page 42,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 42 Gradient Descent Learning ( )
Page 43,MLP Networks 0.32 0.36 0.77 0.35 0.54 0.80 X1 X2 Y1 0.28 0.30 0.63 0.21 0.65 0.91 ... Training Error Number of training cycles MSE
Page 44,generalization is the ability of a network to correctly classify a pattern it has not seen (not been trained on) NNs generalize when they recognize patterns not previously trained on or when they predict new outcomes from past behaviors
Page 45,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 45 Building NN & Pre-processing Data.
Page 46,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 46 Testing / Evaluation • Testing the Generalization ability of a trained NN .
Page 47,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved.
Page 48,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved.
Page 49,support vector machines (SVMs) can be used for pattern classification and non-linear regression – but uses statistical learning theory .
Page 50,training can be slow but accuracy is high owing to their ability to model complex nonlinear decision boundaries (margin maximization)
Page 51,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved.
Page 52,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 52 .
Page 53,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 53 Linear Separability.
Page 54,the goal of a support vector machine for linearly separable patterns is to find the particular hyper-plane for which the margin of separation is maximized . SVM searches for the optimal hyperplane .
Page 55,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore . All Rights Reserved 55 Learning SVM as Optimization .
Page 56,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 56 Learning SVM as Optimization .
Page 57,Learning SVM as Optimization is given by Inner product bybd ii l i i T +=+= = xxxx .
Page 58,"goal of a support vector machine for not linearly separable patterns is to find an optimal hyperplane that minimizes the misclassification error, averaged over the training set ."
Page 59,ll = 0 if there is no error for xi ii T i by + 1)( subject to xw)
Page 60,"there are optimization functions proposed for the case with soft margin, such as CC is a penalty parameter . hard margin minimizes 1 2 wwTTww+ CC ii ii subject to y"
Page 61,higher value of C implies you want lesser errors on the training data . https://blog.statsbot.co/support-vector-machines- tutorial-c1618e635e93 .
Page 62,"a subset of training samples x1, x2, ...xm1 will be used as support vectors . define the separating hyperplane as a linear function of vector drawn from the feature space"
Page 63,"Map the original 2-dimensional input space to a 3-dimensional feature space x = (x1, x2) (x1 x2, x1x2) The original non-linearly separable problem becomes linear"
Page 64,"the target of learning is to achieve a minimized error of classification with decision surface . a function that performs this direct computation of inner product is known as a kernel function, which is equivalent to the distance between x and"
Page 65,"Apply a kernel function K(Xi, Xj) to the original data . Typical Kernel Functions for Nonlinear Classification ."
Page 66,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 66 SVM Example .
Page 67,"a new point z is solved by solving f(2)=1 or f(5)=-1 or by f(6)=1, all three give b=9 . all rights reserved ."
Page 68,"all rights reserved 68 SVM in Practice . you can use the values suggested by the SVM software, or you can set apart a validation set ."
Page 69,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 69 Multi-class SVM Classifier .
Page 70,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved 70 Applications of SVM .
Page 71,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore . all rights reserved .
Page 72,SVM is an elegant and highly principled learning method . design hinges on the extraction of a subset of the training data that serves as support vectors . learning algorithm operates only in a batch mode .
Page 73,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore. All Rights Reserved.
Page 74,"you will build decision tree, neural network and SVM models in this workshop . save your notebook with the cell output and upload it to Canvas ."
Overall Summary,ATAS-PSUPRDay2.pptV4.0 2024 National University of Singapore . all Rights Reserved 61 SVM with non-linear kernels . 'simple' sVMs can be used to build a decision tree based on training data .
