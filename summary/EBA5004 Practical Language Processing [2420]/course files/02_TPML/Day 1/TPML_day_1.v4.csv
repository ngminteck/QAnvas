Page,Summary
Page 1, Day 1. Classic DNN for Text Processing. Day 1 . Day 2 . Day 3. All rights reserved for 2022 NUS .
Page 2, Day 1: Classic DNN for Text Processing. Day 2: Attention & Transformers. Day 3: Transfer Learning with Pre-trained Models. Day 4: Reinforcement Learning with Finetuning .
Page 3, Automatically put text into categories- Classification- Sentiment detection - Spam email detection - Emotion detection . Extract specific information from the text- Extraction - Named Entity extraction .
Page 4," Other Fancier Use Cases: Object Classification/Clustering & Recommendation, Search Engine, Voice Assistant, Machine Translation, Grammar Error Correction & Language Learning ."
Page 5, The moment when NLP became useful is the moment when it became useful . GPT-3 is the result of a successful study of NLP techniques . NUS will publish the findings in 2022 .
Page 6, The moment when NLP became useful is the moment when it became useful . MicroSoft/OpenAI/GitHub‚Äì Copilot 2021 .
Page 7," The moment when NLP became useful is the moment when it became useful, says Microsoft's Open-AI (Microsoft) 2022‚Äì DALLE-2 ."
Page 8," The moment when NLP became useful was the moment when it became useful, says NUS . 2021-2022 is the year of the year when the world's most powerful language is spoken ."
Page 9," What can NLP do now? Without Reasoning without Reasoning? NLP can do so much more with the help of NLP . NUS will publish a book in 2022: ""Without Reasoning"""
Page 10, The RNN and LSTM project is the work of 2022 NUS researchers . The project will focus on data Splits and Evaluation and Optimization .
Page 11," Data SplitsDNN are always Supervised, except for Reinforcement Learning . All data Splits are always supervised and supervised ."
Page 12, Train on this Evaluate ¬†Evaluate  during ¬†training¬†and¬†evaluate¬†evaluation¬†before and after training .
Page 13," K-Fold Cross Validation Evaluation: ""Train,"" ""Train"" and ""Train Valid,"" ""train Valid"" Train Valid Train valid valid Train valid Train validated Train TrainFold 0-fold 1-2 ."
Page 14, K-Fold Cross Validation Evaluation is part of the NUS initiative to improve the quality of cross-crosscross technology . The NUS is committed to developing the technology to improve cross cross technology in the future .
Page 15," 2022 NUS. All rights to be published in 2022 . NUS: ""Evaluation & Optimization"" and ""Optimization"" are open to the public ."
Page 16," Only out-of-date methods such as Na√ØveBayes and LogR are facing under-/overfitting problems . Latest Deep Learning methods have no such problems, thus dominating in world of Machine Learning ."
Page 17," 2022 NUS. All rights reserved. Page 17: ""Optimization and Generalization"" page 17: Optimization, Generalization and Optimization ."
Page 18," NUS: Train longer and increase the complexity of models . Improve the data quality by removing the noisy data from noisy data . Train longer, increase complexity and improve data quality ."
Page 19, Weights regularization (i.e. adding a cost associated with having  large weights)  put constraints on complexity of the model by forcing its weights to take small values . Dropping out randomly randomly setting activated outputs to zero is
Page 20, A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes . Encourage the network to keep the weights small .
Page 21," Cost function = Loss (say, binary cross entropy) + Regularization term. L1:. The weights may be reduced to zero. L2 forces the weights to decrease towards zero (but not exactly zero)."
Page 22," Weights regularization will be regularized when w=2, we get the minimum loss . Without any regularization, the loss of weight will be lessened ."
Page 23, The Delta rule may be reduced to zero given different ùùÄw weights . It is the result of an iterative approach to weight regularization . The rule is based on the fact that the weight is regularized .
Page 24," Weights regularization (26,2) regularization and vocabulary_size (26) normalized word level n=26 . Weighs regularization is a form of language that can be used to describe a word ."
Page 25, The Delta rule has no chance of being 0 has been applied in 2022 NUS . The rule is the result of an iterative approach to weights regularization . NUS.
Page 26, Weights regularization  forcing its weights to take small values . Dropping out (i.e. randomly setting activated outputs to zero) is effective in regularizing the model .
Page 27, The Dropout Layer (Dropout Layer) is the dropout layer of a garment that is designed to prevent overfitting . NUS is committed to helping people avoid overfitting in the future .
Page 28, P=3/6=0.5 -0.1/6 = 0.5/3 =0.2/6/7 = 0/6 . P= 3/6 is 3/4/6; P
Page 29, The RNN and LSTM project is the work of 2022 NUS researchers . The NUS team is working on the RNN code for text processing and data analysis . The project aims to use the CNN for Text Classification and L
Page 30, CNN for Text Classification provides a sliding window towards the (1-D) direction of Text . Weights to be learned: Not matrix multiplication (Dot Product) But Convolution .
Page 31, CNN for Text Classification: Weights to be learned and learned . Weights are shared and shared to be shared by users .
Page 32, CNN for Text Classification: 'Kernan (Ngram feature extractor)‚Äô ‚Äì Sliding window towards the (1-D) direction of Text. Weights to be learned and shared and shared . MaxPooling the
Page 33, CNN for Text Classification: Multi-Kernels (Ngram feature extractors) Weights to be learned and shared . MaxPooling the  results (re-shaping the rows)
Page 34, Multi-Kernels (Ngram feature extractors) are multi-kernels . Add Dropout layer (optional) and Add Dense layer (re-shape the matrix) ‚Äì Add Softmax to obtain Probs .
Page 35, The 2022 NUS workshop will take place in the UK and Australia . It will be the first of its kind to take part in the U.S. National History Month celebrations .
Page 36, The RNN and LSTM model is based on RNN text Encoder and SeqtoSeq model . The NUS project aims to improve the quality of text data . The project will be presented at the University of Cambridge
Page 37, Recurrent Neural Networks are the result of recurrent neural networks . The network is a form of a network of neurons and neurons that form a network . It is the result result of a neural network called Recurrent Neural Networks .
Page 38, RNN Encoder is a better ( than Ngram) way to represent word/sentence . Encode the sequence of word tokens (which CNN lacks of)
Page 39, RNN Encoder is a better ( than Ngram) way to represent word/sentence . Encode the sequence of word tokens (which CNN lacks of)
Page 40, NUS Encoder is a better ( than Ngram) way to represent word/sentence . Encode the sequence of word tokens (which CNN lacks of)
Page 41, Bi-RNN Encoder is a better ( than Ngram) way to represent word/sentence . Encode the sequence of word tokens (which CNN lacks of)
Page 42," The good the good movie is a good movie, says NUS. The good is the good is good, says the author of a new book ."
Page 43," W3_h: ""What‚Äôs inside A?"" ""A: The good"" ""A"" is ""the good movie"" ""The good"" is the ""good"""
Page 44," Recurrent Neural Networks with Dense and Softmax (Dense) and Hardmax (Hardmax) have been described as ""predicting networks"""
Page 45," Recurrent Neural Networks have been created by NUS researchers . They have been called SoftMAX, SoftMAX and SoftMAX . SoftMAX is SoftMAX. SoftMAX SoftMAX (SoftMAX) SoftMAX(1,Classes)("
Page 46," RNN: ""Bi-Directional RNN is never random"" NUS: ""Never random,"" ""Predict Neutral,"" ""Never Random"""
Page 47," RNN: ""Bi-Directional RNN is never random"" NUS: ""Never random,"" ""Predict,"" ""never random"""
Page 48, The RNN and LSTM text Encoder and CNN for Text Classification are discussed in the RNN-LSTM workshop . NUS is committed to the project in 2022 .
Page 49, RNN Vanishing Gradients: ‚Äúpoof‚Äù and ‚Äúpredict‚Äù is ‚ÄúPoof‚ÄôÔøΩÔøΩs‚Äô. ‚ÄúPredictLoss‚Äô‚Äù .
Page 50, RNN Vanishing Gradients: Long Short Short Term Memory . Historical words are less influential to represent the future words . Keeping a memory of past time steps: Add the memory to the current step. Gates control the strengths of the memory
Page 51, NUS is committed to developing a framework for learning and problem-oriented programming . The framework is based on the work of NUS professor Peter Schmidhuber and his colleagues .
Page 52," The shape of the Matrix remains in the shape of LSTM . It is similar to Conv without summing up and is different from Element Wise Product . It's not the same as Element Wise product, but it's different from dot"
Page 53, The Element Wise Product is based on LSTM and is being developed by NUS at the University of Cambridge . It is the first phase of a new system that has been designed for the NUS platform . NUS is developing a
Page 54," W3_hW3_i(1,hidden_dim) is W33_HW3-i (1, hidden_dim), W3-HW-H (1) is HW3W"
Page 55, LSTM uses an Empty/Random Matrix C to hold memory . Calculate the strengths/weights of the current memory of C . Gates control the strengths of the memory .
Page 56, A for LSTM? Introduce one Empty/Random Matrix C to hold memory . Calculate the strengths/weights of the current memory of C . Forget some useless memory before adding a movie .
Page 57, A for LSTM? Introduce one Empty/Random Matrix C to hold memory . Calculate the strengths/weights of the current memory of C . Forget some useless memory before adding a new movie .
Page 58," As long as there are many layers nested, the functions and gradients gets multiplied in a nested manner‚Äì It is a DNN problem ."
Page 59, LSTM/GRU doesn‚Äôt guarantee no ¬†guarantee no ipient gradient vanishing . It‚Äôs just better than the ¬†vanilla RNN RNN .
Page 60, Na√Øve Dropout RNN with Dropout P1=0.5.5; P2 = 0.5%; P1 =0.4; P1 P1 is 0.4 . P1: 0.3
Page 61, The RNN is based on the structure of the NUS' LSTM with Dropout . The NUS NUS LSTm was created by NUS in 2012 .
Page 62, LSTM with Dropout and Variational RNN with dropout can be used in the future . NUS is the world's largest university and research group of researchers in the U.S.
Overall Summary, Aobo Wang Aobo: The moment when NLP became useful is the moment when it became useful with Google/OpenAI/GitHub . He also discusses Text Processing Using Machine Learning with Machine Learning . The first day of the
