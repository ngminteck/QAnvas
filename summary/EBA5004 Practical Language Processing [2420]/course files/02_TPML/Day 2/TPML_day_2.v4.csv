Page,Summary
Page 1,TEXT PROCESSING WITH MACHINE LEARNING MODULE 2: Advanced DNN systems Dr Wang Aobo isswan@nus.edu.sg
Page 2,Seq2Seq models • Advanced DNN systems • Attention • Transformer • Workshop • Sentence/ Document representation
Page 3,Sequence to Sequence
Page 4,RNN starts with a random state (vector) hh00 . RNN ends with 'sequence to sequence' if it's something non-random .
Page 5,RNN starts with a random state (vector) hh00 . what if it is something non-random?
Page 6,• “Translate” (Ideally) any object A to any object B French English Image Text Audio Text Document Summary Question Answers
Page 7,training requires the paired condition and target generation . a large amount of data is needed for model to train well .
Page 8,"Language Modeling is capturing – long-term dependencies – hierarchical relations – sentiment . the service was poor, but the food was ______ ."
Page 9,ELMo (2017) LSTM for Language Modelling Starting from WordVec Given “Let’s stick to” Predict “improvisation”
Page 10,ELMoAlways Bi-LSTM vec(“stick”): “elmo” means “stick” in Spanish .
Page 11,LEGO Arts - Have Fun with it . Have fun with it.. Have FUN with it.... LEGO Arts: Have Fun....
Page 12,GloVe Language Modelling LSTMTransformer 2018 . Before and After the Moment Oct 2018 Jun 2014 .
Page 13,Workshop LSTM AND SEQ2SEQ . SEQ1SEQ2 is a two-day workshop aimed at improving the quality of air quality in the air.
Page 14,Turing Award to “NVIDIA” Oct 2018 LSTM Transformer 2018 lstm 1997 Attention 2012 .
Page 15,"the “ImageNet” Moment for NLP (*Image from Jay Alammar’s blog) 24 Transformer blocks 1024 hidden layers, and 340M parameters 16 TPU pods for training Apr 2017 Oct 2018 ??? 2018 12"
Page 16,"in psychology, attention is the cognitive process of selectively concentrating on one or a few things while ignoring others ."
Page 17,A neural network to mimic human brain actions in a simplified manner . Attention selectively concentrating on a few relevant things . more parameters and calculations to capture contextual information .
Page 18,the attention mechanism introduced by Bahdanau et al in 2015 prevents parallelization . ‘C” does not sufficiently capture the prior information .
Page 19,Laurent Lives In France And he Speaks Excellent ‘French’. Copyright National University of Singapore.
Page 20,"at every decoding step, the decoder allocates a set of attention weights – , aligned to the input words ."
Page 21,Sequence to Sequence • “Translate” (Ideally) any object A to any object B CC00CCRR .
Page 22,Attention Encoder CCii+11 hhii Attention CC11 CC22 CCtthh11hhtt11
Page 23,Attention based Sequence to Sequence hhii tanh WWRR . hheeeeaaaiiiaahaaaeett softmax AAttt
Page 24,• tanh Box Attention based Sequence to Sequence based on a 'sequence to sequence'
Page 25,the hidden vectors are multiplied to form the ‘scores’ with 3 alternatives . the encoder and decoder use top hidden layer states .
Page 26,"the Bahdanau use a global attention model, in which all the words in the sentence are weighted . a key issue is the length of the ‘sentence’ – or the attention span . the solution"
Page 27,BE TRANSFORMED Copyright National University of Singapore. All Rights Reserved 27 a.s.c.
Page 28,The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease . foundation of the BERT and OpenAI GPT-2/3/4 algorithms
Page 29,National University of Singapore. All Rights Reserved 29 • speech recognition • biological sequence analysis • machine translation • natural language generation.
Page 30,"the transformer is used for machine translation, translating from french to english . it is still a seq2seq model ."
Page 31,"Copyright National University of Singapore. All Rights Reserved 31st june, 2018. Transformer architecture National university of singapore."
Page 32,the Encoder and Decoder box will not be RNNs but Self-Attentioned layers . the box will be able to stack multiple layers of encoders and decoders .
Page 33,broken down the Transformer architecture . breaking down the architecture of the decoders . Breaking down the Encoder Architecture . National University of Singapore .
Page 34,looking into the Transformer architecture Encoder Copyright National University of Singapore . 34 Decoder One Single Encoding Box has 2 layers – multi-head attention layer feed forward layer . one single decoder box has 2
Page 35,Copyright National University of Singapore. All Rights Reserved 35 . Copyright nasu.singapore.
Page 36,Copyright National University of Singapore . All Rights Reserved 36 october 2014 . Copyright National University . of Singapore.
Page 37,"self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence ."
Page 38,copyright National University of Singapore. All Rights Reserved 38 . ‘kicked’ relates to the different words in the sentence .
Page 39,Training Self attention scores matrices to be learned Computation summary . I Kicked The ball . The ball was kicked . the ball was scored .
Page 40,Copyright National University of Singapore. All Rights Reserved 40 . Copyright: national university of singapore.
Page 41,"the original paper has 8 attention heads, with each set of encoder/decoder . each of the attention heads uses a slice of the original dataset for training . the use of this multi-heads is supposed to provide"
Page 42,'multi-headed attention' is computed multiple times independently and in parallel . these attentions are concatenated by a Wo weighting matrix .
Page 43,Memory Wall
Page 44,Speed Up Attention Flash Attention reduces the number of times data is read from DRAM memory . this improves the efficiency of Softmax's processing .
Page 45,Speed Up Attention MQA MHA GQA 30%-40% faster with performance drop Trade off Trade off : .
Page 46,"Lower the Memory usage & Speed up the inference . inference: Lower the memory usage, Speed up inference."
Page 47,"deepseek-v2: a strong, economical, and efficient mixture-of-experts language model . low-rank joint compression for key and value to reduce the KV cache ."
Page 48,Speed Up Attention LLAMA2 – Grouped Query – LLama2 - .
Page 49,the word will have a vector for positional encoding that sheds details on where the word lies . this results in a new vector – ‘embedding with time signal’ .
Page 50,Residuals summing There is also some residuals that directly ‘by-pass’ the attention layer .
Page 51,"each word goes through the decoder and decoded in the process . in this decoding stage, each word is only affected by the words before it . the future positions are ‘masked’ by giving them an –"
Page 52,"the linear layer is a FFN that projects the vector produced by the stack of decoders, into a much larger vector called a logits vector . the word with the highest probabilities is then chosen as the"
Page 53,Decoding Progress
Page 54,Decoding Progress
Page 55,"8Head Attention + Transformers 8 GPU + 3.5 day Google Research, 2017 . Google Research is a non-profit organisation based out of the National University of Singapore ."
Page 56,"57 Single Head Attention + LSTMs Single GPU + 1 day Stephen Merity, 2019 8Head Attention + Transformers 8 GPU + 3.5 day Google Research, 2017 ."
Page 57,a new flurry of progress may have thrown away the past era of models . instead we’re too committed to our existing stepping stones to backtrack .
Page 58,Continue with Transformers Transformer s
Page 59,Continue with Transformers
Page 60,BERT MASK LM 15% of the words in each sequence are replaced with a [MASK] token .
Page 61,BERT MASK LM & Next Sentence Prediction S.A.P. & S.S.
Page 62,GPT 2/3 Transformer Decoders
Page 63,no Silver Bullet (2020) https://arxiv.org/pdf/2009.07238v2.pdf
Page 64,All Rights Reserved 68 . Useful article explaining positional encoding using sinusoidal function . Youtube video on transformer https://www.youtube.com/watch?v=rBCqOTEfxvg
Page 65,Seq2Seq models • Advanced DNN systems • Attention • Transformer • Workshop • Sentence/ Document representation
Page 66,"Doc2Vec vectors represent the theme or overall meaning of a document . the documents here can refer to paragraphs, articles or whole documents ."
Page 67,document comparison can be done by a similarity measure . genism seems to have better accuracy as noted in industry .
Page 68,"Document Similarity Word Movers Distance allows to assess the ‘distance’ between two documents in a meaningful way, even when they have no words in common . uses Euclidean distance and a ‘transport matrix’ –"
Page 69,"two main training methods of these Paragraph Vectors (PV) are Self-supervised methods . from the original paragraphs/ documents, you try to create a representative paragraph/ document vector ."
Page 70,similar to CBOW Word2Vec except with a new paragraph vector that represents the documentconcept .
Page 71,faster but may not be as accurate as the PV- DM . similar to word2vec skipgram Copyright National University of Singapore .
Page 72,Sentence to Vectors - Copyright National University of Singapore. All Rights Reserved 76 . Skip thoughts to generate the previousand nextsentences.
Page 73,"Quick thoughts to predict/classify the nextsentences . Sentence to Vectors is a free, open-source, open source software application ."
Page 74,quick thoughts generally to create document vectors . copyright National University of Singapore. All Rights Reserved 78 .
Page 75,Sentence-Bert 2018 Sentance to Vectors . SentENCE-BERT 2018 .
Page 76,"Sentence to Vectors BertScore 2020: a resounding yes, a no, and a yes, no."
Page 77,https://www.baeldung.com/java-langchain-basics Framework - LangChain • QA with Knowledge Base
Page 78,Data Pipeline . Split Documents into Chunks . Calling OpenAI API . Embeddings and vector storage .
Page 79,"performance also a key consideration . word vectors can be generated from word2vec, GLOVE, BERT, ELMO etc."
Page 80,84 https://towardsdatascience.com/word-embeddings-and-document-vectors-part-2-order-reduction- 2d11c3b5139c . https://
Overall Summary,"'attention is all you need': a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease . in the original paper, the transformer is used for machine translation, translating from french to english . all rights reserved ."
