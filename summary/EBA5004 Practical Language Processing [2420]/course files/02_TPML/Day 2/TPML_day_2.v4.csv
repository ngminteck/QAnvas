Page,Summary
Page 1," Dr Wang Aobo Â isswan@nus.edu.ngu: ""Advanced DNN systems"" Dr Aobo: ""DNN is the world's most advanced technology"""
Page 2, National University of Singapore's Seq2Seq models and DNN systems are discussed in a workshop at the Singapore Institute of Science and Technology . The workshop will discuss how to use these models to represent sentences and sentences .
Page 3,Sequence to Sequence
Page 4, RNN starts with a random state (vector)  Â ğ’‰ğŸ Â - Â predictÂ PredictÂ R R R RÂ RÂ RoutineÂ is something non-random .
Page 5, RNN starts with a random state (vector) Â ğ’‰ğŸ  Â - Â vanishingÂ randomÂ stateÂ is something non-random . RNN begins with a state (veget)
Page 6," Sequence to Sequence: â€œTranslateâ€ (Ideally, any object A to any object B) French English: â€˜French English.â€™ â€˜â€˜ â€˜Translatingâ€™ is a form of"
Page 7, The Generalized Encoder-Decoder Framework is a generalized encoder-decoder framework . Training requires the paired condition and target generation to train well .
Page 8, NLP counterparts of â€œImageNetâ€™sâ€  counterparts are NLP-like datasets with labels . NLP is capturing long-term dependencies and hierarchical relations . Plain Text are everywhere: Plain Text is everywhere. Plain
Page 9, ELMo (2017) LSTM for Language Modelling: Starting from WordVec. Given â€œLetâ€™s stick toâ€™veâ€™ï¿½Predict â€˜improvisationâ€™ Given â€˜Let
Page 10, ELMoAlways Bi-LSTM  is always Bi-lSTM . ELMo always always has the word â€œstickâ€ - stick .
Page 11, LEGO Arts - Have Fun with itâ€¦. LEGO Arts . Have fun with LEGO Arts! LEGO Arts is a fun way to build your own LEGO creations .
Page 12, Before and After the Moment: Before and Before the Moment. The moment is the moment of the moment . The moment was the moment that changed the way people react to it .
Page 13," WorkshopLSTM and SEQ2SEQ Workshop. Workshop. workshop. Workshop: LSTM, SEQ 2SEQ . Workshop: Workshop."
Page 14, Turing Award to â€œNVIDIAâ€™sâ€ â€œLSTMâ€ is given the Turing Award for â€œtransformerâ€ technology . Turing Award is given to â€˜NVIDIAâ€
Page 15, The â€œImageNetâ€ Moment for NLP is Jay Alammarâ€™s blog post . The imageNet moment is a moment for the first time in the history of NLP .
Page 16," In psychology, attention is the cognitive process of selectively concentrating on one or a few things while ignoring others . Many of the new advances in NLP starts from attention -transformer, BERT ."
Page 17, A neural network to mimic human brain actions in a simplified manner . Attention selectively concentrating on a few relevant things . Useful in sequence modelling .
Page 18," The attention mechanism was introduced by Bahdanau et al in 2015 . Vanishing gradients prevent parallelization and prevents parallelization . â€˜Câ€™ does not sufficiently capture the prior information, in comes an attention mechanism ."
Page 19," Attention â€“ what does it do exactly? What does it mean for â€˜goodâ€™ attention? Suppose a statement: To predict the next word â€˜Frenchâ€™, which precedent word is most important with â€˜weightsâ€™"
Page 20," At every decoding step, the decoder allocates a set of attention weights aligned to the input words . Also known as additive attention as it  performs a linear combination of   Â encoder states and theÂ decoder"
Page 21," Sequence to Sequence: â€œTranslateâ€ (Ideally) any object A to any object B. â€˜Translate' is â€˜translateâ€™, â€˜Aâ€™ or â€˜Bâ€™ is"
Page 22," The Decoder is based on an Attention based Sequence to Sequence . The Encoder is called a Decoder with a Conductor . Decoder has been called a ""Decoder,"" or ""Encoder"""
Page 23, The Attention based Sequence to Sequence is an example of a sequence . The Attention Based Sequence is a Sequence of a Sequence . The Sequence is based on a Sequence that is followed by a Sequence Sequence .
Page 24," Attention based Sequence to Sequence . tanh tanh Box: ""Attention based sequence to Sequence"" tanh box: ""Tanh box"""
Page 25, Luong architecture model uses top hidden layer states in layers of encoder and decoder . Luong attention uses top hidden layer states to form the â€˜scoresâ€™ with 3 â€˜alternativesâ€™ The hidden 
Page 26," Bahdanau use a global attention model, in which all the words in the sentence are weighted . A key issue is the length of the â€˜sentenceâ€™ â€“ or the attention span . The solution is a local attention"
Page 27, BE TRANSFORMED: Be Transformed . Be transformed. Be transformed! Be transformed . Be transform. Be transform! Be transform .
Page 28, The Transformer in NLP aims to solve sequence-to-sequence tasks while handling long-range  dependencies with ease . Foundation of the BERT and OpenAI GPT-2/3/4 algorithms .
Page 29," National University of Singapore's Transformer Applications. 29 Â  Â  Â  Â  Â  Â  Â  Â Â speech recognition, biological sequence analysis, machine translation and machine translation. Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â transformer applications. Â  Â  Â  Â  Â  Â  Â  Â Â Â Â transformedÂ speechÂ recognitionÂ receiveÂ "
Page 30," In the original paper, the transformer is used for machine translation,   Â translating from French to English . It is still a seq2seq model ."
Page 31, National University of Singapore's Transformer architecture is a form of architecture . Transformer technology has been used to transform Singapore's urban landscape .
Page 32, Stackable Encoders and Decoders will not be RNNs but Self-Attentioned layers . Similar idea of stacking multiple layers of En/Decoders .
Page 33, Breaking down the Transformer  Â architecture . The Transformer is the brainchild of the National University of Singapore .
Page 34, The National University of Singapore's Encoder Box has two layers â€“ â€“ multi-head attention layer and feed forward layer . It has 2 attention layers +  Â feed forward layer â€“ the attention layer . The Encoder has 2 layers
Page 35, National University of Singapore's encoder encoder . Inside the encoder. The encoder was created by Singapore's National Institute of Science and Technology .
Page 36," Inside the decoder, the world's largest transformer has been created by Singapore's National University of Singapore . The device was designed by the university's National Institute of Technology in Singapore ."
Page 37, Transformers use self-attention instead of RNNs or CNNs instead of using CNNs . Self attention is an attention relating different positions of a single sequence in order to compute a representation of the sequence .
Page 38," For the words : â€œI kicked the ballâ€, we want to know how the word   â€˜kickedâ€™ relates to the different words in the sentence ."
Page 39," The National University of Singapore has published a series of articles on computer science . The findings were published at the National Institute of Singapore . The results were published on October 1, 2013 ."
Page 40, The National University of Singapore has published a number of articles on the topic of Attention . It is the first chapter of a new series of books by the National Institute of Singapore .
Page 41," The beast with many heads. The original paper has 8 attention heads, with each set of encoder/decoder . Each of the attention heads uses a slice of the original dataset (in this case 1/8th of the dataset"
Page 42, Multi-headed attention is computed multiple times   independently and in parallel â€“ â€˜multi-headed  Â attentionâ€™ These attentions are concatenated by a Wo  weighting matrix .
Page 43,Memory Wall
Page 44," Flash Attention reduces the number of times data is read from DRAM memory when computing Softmax, Â thus improving efficiency ."
Page 45, MHA GQA is 30-40% faster with performance drop than 30% faster . Performance drop is 40% faster than at least 40% slower than at first .
Page 46, Lower the Memory usage and Speed up the inference . Lower the memory usage and speed up the infrairmary . Speed up inference. Speed up an inference .
Page 47," DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model . Low-rank joint compression for key and value to reduce the KV cache . Lower the Memory usage"
Page 48,"LLAMA2 â€“ Grouped Query â€“ Grouping Query . Speed Up Attention: Grouped Grouped Question . Grouped Questions: Grouping Attention, Grouping Grouping Questions . Grouping Question: Group of Questions, Group of Qu"
Page 49, Each word will have a vector for positional encoding that sheds details on where the word lies in the sentence . This results in a new vector â€“ 'embedding with time signal'
Page 50, There are also some residuals that â€˜by-passâ€™ the attention layer . Residuals summing up are also â€˜directlyâ€™ by-passingâ€™ attention layers .
Page 51, The future  positions are â€˜maskedâ€™ by giving   Â  Â  Â  Â  Â  Â  Â  Â them an â€“inf weight . Each word is only affected by the words before it .
Page 52," Final Layer and Softmax Layer is a vector for each different word that is output from the decoder, which is turned into a word . The word with the highest  Â probabilities is then chosen as the  iop"
Page 53,Decoding Progress
Page 54,Decoding Progress
Page 55, Researchers at the National University of Singapore have created a new system for the first time . The new system uses a 3.5-day GPU and 3.8-second memory memory . The system was created by the National Institute of Technology
Page 56, Single Head Attention + LSTMs. single GPU + 1 day. single GPUs + 1.5 days. Single GPU + 3.5 day . Single GPUs + 2.6 day days. Fighting the direction of research.
Page 57, Perhaps we were too quick to throw away the  past era of models simply due to a new flurry of progress . Perhaps we're too committed to our existing models to backtrack .
Page 58,"Continue with Transformers
Transformer s"
Page 59,Continue with Transformers
Page 60, 15% of the words in each sequence are replaced with a  Â  Â  Â  Â  Â  Â  Â  Â [MASK] token .15% of words in the sequence are replaced with aÂ replacedÂ with aÂ progressiveÂ token .
Page 61, BERTMASK LM  & Next Sentence Prediction Prediction: Sentencing will take place in the U.S. BERTERTMASk LM  and Next Sentencing Prediction: Death .
Page 62," GPT 2/3Transformer Decoders: Transformer Decodeers . Transcoders: Decodes, Decodes and Decodes . GPT: Transcode: Decodees. GPT. Decodes. G"
Page 63, No Silver Bullet (2020) is No. Silver Bullet . The project was created in 2009 and will be published in 2020 . It is the first time the project has been presented in a public forum .
Page 64, East to read blogs on transformer/ attention:Â https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a.
Page 65," National University of Singapore's 6.9-seq models: Seq2Seq models, DNN systems, Attention, Transformer and Transformer systems. 6-9-9:9.9: 9-11:"
Page 66, Doc2vec Sentence Representation is an extension of Word2vec . Doc2Vec vectors represent the theme or overall meaning of a document .
Page 67, The gensim (also in Tensorflow) seems to have better accuracy than TF as noted in industry industry .
Page 68," Document Similarity Word Movers Distance is based on WordVectors such as GLOVE, W2V . Allows to assess the â€œdistanceâ€ between two documents in a â€˜meaningful way, even when they"
Page 69," Distributed Memory Model (PV-DM) and Distributed Bag Of Words (PVDBOW) Both are Self-supervised methods, in that from the original paragraphs/  documents, you try to create a"
Page 70, Paragraph Vectors - Distributed Memory Model (PV-DM) are obtained by training FFN   on the synthetic task of predicting the next word based an average of  Â both context word-vectors and
Page 71, Paragraph Vector Distributed Bag Of    Words (PVDBOW) PVs obtained by training a neural  network on  the synthetic task of predicting a target word .
Page 72," National University of Singapore's Vectors. 76: Skip thoughts to generate the previousand nextsentences . Sentence to Vector: ""Vectors"""
Page 73, Quick thoughts to predict/classify the next sentence . Sentence to Vectors . Quick thoughts of the nextsentences .
Page 74, Quick-thoughts generally to create document  vectors . Skip thoughts more for word/sentence  vectors. Skip thoughts for word and phrase phrases more for words/sentences .
Page 75," Sentence-Bert 2018: Sentence to Vectors . Sentence of Bert Bert 2018: ""Bertert 2018"" Sentence for Bert Bert's crimes: ""Vectors"""
Page 76," Sentence to VectorsBertScore 2020 is set to be served at a maximum of six years . Sentence is set for the prison term of 20 years . For more information, visit www.vectors.com/"
Page 77," LangChain: QA with Knowledge Base, Retrieval Augmented Generation and QA . Langchain:Â QA with knowledge base, QA, QC, QQA and QC-QA-QC"
Page 78," Data Pipeline: Split Documents into Chunks, Split Chunks and Calling OpenAI API . Embeddings and vector storage: Embedding and vector Storage: Calling pre-trained models and calling pre-training models . Data pipeline:"
Page 79," Averaging word vectors â€“ strong benchmark. The word vectors can be  generated from word2vec, GLOVE, BERT, ELMO etc. Performance also a key consideration ."
Page 80, Researchers from the National University of Singapore have created a framework for word embeddings and document-vectors . The framework was created by the National Institute of Singapore and the National Singapore Institute of Technology .
Overall Summary, The â€˜ImageNetâ€™ Moment for NLPâ€™s Moment for â€˜Attentionâ€™ is capturing long-term dependencies â€“ long term dependenciesâ€“ hierarchical relationsâ€“ sentimentâ€“ Plain Text are everywhere(Large) Language Model
