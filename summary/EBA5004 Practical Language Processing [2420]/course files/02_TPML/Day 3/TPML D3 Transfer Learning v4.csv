Page,Summary
Page 1,Page 1 Text Processing Using Machine Learning Transfer Learning & Pre-trained Models Dr. Fan Zhenzhen NUS-ISS National University of Singapore. All rights reserved.
Page 2,Page 2: Transfer Learning • The Evolution of Pre-trained Language Models • Representative LMs – Auto-encoding - BERT - Auto-regressive - GPT & GPT2
Page 3,Transfer learning is a transfer learning program at the National University of Singapore . page 3 Transfer learning focuses on the transfer of knowledge and skills from one university to the next .
Page 4,"Page 4 Transfer learning - a pre-trained model on a new problem . ImageNet Large Scale Visual Recognition Challenge, 2012 – Break-through for computer vision ."
Page 5,"Pre-trained models useful for a variety of NLP tasks like text classification, sequence labelling, coreference resolution, question answering, machine translation, natural language inference ."
Page 6,Language modeling is the task of assigning a probability to sentences in a language . the language models also assigns a likelihood for the likelihood of a given word to follow a sequence of words .
Page 7,Randomly mask some of the tokens from the input . to predict the original vocabulary id of the masked word based on its context .
Page 8,transfer learning with language modeling . transfer knowledge (learned weights) from a model trained on one task to improve generalization on another related task .
Page 9,"Create task-specific architecture . use the pre-trained model (e.g. ELMo, BERT, etc.) as a feature extractor ."
Page 10,"Transfer Learning Approaches: fine-tune a small portion of the model parameters or extra parameters, for larger models, e.g., Low-Rank Adaptation (LoRA)"
Page 11,Page 11 Recall the DL training routine? Page 11 - DL: 'i'm not sure i'll ever be able to do this again'
Page 12,"Transfer Learning Approaches (TLAs) address the challenge of domain shift . different statistical properties of language (vocabulary, expressions, style, etc.) from the target domain compared with the source domain (pre"
Page 13,Multi-Task learning helps the model generalize better by sharing knowledge across tasks . zero-shot and few-shot learning: a model (Large Language Model) trained on one set of classes/tasks is
Page 14,National University of Singapore. All rights reserved. Page 14 The Evolution of Pre- trained Language Models . 2019-2025 .
Page 15,2019-2025 National University of Singapore. All rights reserved . page 15 The earlier path... - a non-exhaustive list .
Page 16,"2019-2025 National University of Singapore. All rights reserved . list includes: T5 (oct 2019) – 60M to 11B, text-to-text transformer, by google . ELECTRA"
Page 17,"LLaMA (feb 2023) - open & efficient foundation LMs, by Meta -> Alpaca, Vicuna, Koala, etc., to simulate ChatGPT ."
Page 18,"2019-2025 National University of Singapore. All rights reserved . page 18 'blazing-hot' list includes: 'gemini 1.5 (feb 2024) - massive context window, multimodal capabilities'"
Page 19,https://www.researchgate.net/publication/3734 51304_Examining_User-Friendly_and_Open-Sourced_Large_GPT_Models_A_
Page 20,2019-2025 National University of Singapore. All rights reserved. Page 20 Getting bigger and larger and larger.
Page 21,2019-2025 National University of Singapore. All rights reserved . Page 21 And larger... ...
Page 22,the rise of generative ai- large-language-models-llms-like-chatgpt/
Page 23,2019-2025 National University of Singapore. All rights reserved . Assessing Pretrained Language Models .
Page 24,"NLU GLUE Benchmark: General Language Understanding Evaluation (GLUE): a collection of 9 task datasets for training, evaluating, and analyzing NLP models ."
Page 25,National University of Singapore. All rights reserved . other common NLU benchmarks include: 950K question answer pairs from Wikipedia and the web .
Page 26,general capabilities – MMLU: Massive Multitask Language Understanding . to evaluate general knowledge and reasoning abilities of large language models (LLMs) across multiple domains . 16k MCQs covering 57 diverse subjects
Page 27,"MBPP: Mostly Basic Python Programming, 1000 crowd-sourced Python programming problems . CodeXGLUE: 14 datasets covering code completion, code summarization, code translation, bug fixing ."
Page 28,page 28 Common Benchmark for LLMs . page 28 Reasoning . ARC Easy/Challenge: AI2 Reasoning Challenge .
Page 29,"National University of Singapore. All rights reserved . NIAH: Needle In A Haystack, to test in-context retrieval ability of long context LLMs ."
Page 30,Page 30 More recent Benchmark for LLMs . Retrieval - RQABench - STaRK - benchmarking LLM retrieval on texual and relational databases . BFCL
Page 31,"3000 questions across various subjects by 1000 subject experts . scale AI and the Center for AI Safety (CAIS) to ""test the limits of AI knowledge at the frontiers of human expertise"""
Page 32,2019-2025 National University of Singapore. All rights reserved. Page 32 Auto-encoding Model: BERT.
Page 33,the evolution of llms in practice: A survey on chatgpt and beyond . arXiv preprint:2304.13712 (2023).
Page 34,Page 34 Pre-trained Transformer Models . • Autoregressive models – The decoders – Pretrained by corrupting the input text .
Page 35,2019-2025 National University of Singapore. All rights reserved . Autoregressive model Autoencoding model Sequence-to-sequence model .
Page 36,"Google's mBERT encoders are available in two sizes, and cased/uncased versions . BERT Base: 12 encoder layers, 768 hidden units, 12 attention heads . Large: 24 layers"
Page 37,"BERT Jay Alammar, ELMo, and co. (How NLP Cracked Transfer Learning)"
Page 38,"page 38 BERT vs OpenAI GPT . ""deep bidirectional transformers for language understanding"" page 38 ."
Page 39,"Page 39 BERT Pre-training tasks . Useful for QA and NLI ""Out of [MASK], out of mind"" ""sight"""
Page 40,"Pre-training data -- 3.3 billion words in total . batch size: 256 sequences * 512 tokens = 128,000 tokens/batch . Took 4 days to train each model ."
Page 41,"Frequently used words should not be split into smaller subwords, but rare words should be decomposed . 30,000 most common words – Subwords occurring alone or at the beginning of words ."
Page 42,"Page 42 BERT Input Representation My dog is cute. He likes playing . Devlin, Jacob, et al."
Page 43,"Page 43 BERT for Feature Extraction Jay Alammar, The Illustrated BERT, ELMo and co."
Page 44,"Page 44 Fine-tuning BERT Devlin, Jacob . ""deep bidirectional transformers for language understanding."""
Page 45,"page 45 National University of Singapore. All rights reserved . ""deep bidirectional transformers for language understanding"" page 45 ."
Page 46,National University of Singapore. All rights reserved . Page 46 Fine-tuning BERT • Many experiments on various tasks .
Page 47,"BERT achieves new state-of-the-art results on eleven NLP tasks . pushing the GLUE score to 80.5% (7.7% point absolute improvement), – MultiNLI accuracy to 86.7% (4"
Page 48,"scale to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre- trained . 'how to use large scale models in production, under low latency constraints?'"
Page 49,2019-2025 National University of Singapore. All rights reserved. Page 49 Knowledge Distillation: a distilled spirit distilled in a distillery.
Page 50,Page 50 Reduce the size of a BERT base model by 40% - retaining 97% of its language understanding capabilities - being 60% faster . TinyBERT (Huawei) is 7.5x smaller than B
Page 51,"Other BERT-related Models – German, French, Italian, Spanish, Finnish, Portugese, Russian, Japanese, etc."
Page 52,"Rotary Positional Embeddings to support sequences of up to 8192 tokens . unpadding to ensure no compute is wasted on padding tokens, speeding up processing time ."
Page 53,Page 53 Auto-regressive Model: GPT & GPT-2 . All rights reserved. Page 57 Auto-adjusted Model: gpt & gt-2 & auto-re
Page 54,Autoregressive is a decoder-based language model with 117M parameters . task: to predict the next word given the previous words in context . 100 epochs – Batch size – 64
Page 55,"124M, 355M, 774M, 1.5B (released from Feb to Nov 2019) Trained on WebText, a dataset of 8 million web pages with diversified topics (40 GB)"
Page 56,2019-2025 National University of Singapore. All rights reserved . Page 57 Input: 2019-2025 .
Page 57,Page 57 Output – one token at a time decoding a string of characters . page 57 Decoding is done by encoding the string of digits .
Page 58,"Page 58 Different Decoding Methods • Strong influence on the generated output . Greedy search, beam search and top-p sampling are the most common decoding methods ."
Page 59,"at each step, select the token with the highest probability . Pros: Fast and efficient, deterministic (no randomness) Cons: – Can be repetitive and uncreative ."
Page 60,Page 60 Beam Search: keep multiple top-k sequences at each time step (beam size = k) Choose the full sequence that has the overall highest probability .
Page 61,"Randomly pick the next word based on its conditional probability . Pros: adds diversity to generated text, and prevents repetitive output . Cons: generated text could be incoherent ."
Page 62,Page 62 Top-K Sampling . probability mass is redistributed among only those K next words . used by GPT-2 .
Page 63,Page 63 Top-p Sampling is also known as nucleus sampling . a word's cumulative probability exceeds the probability p .
Page 64,"national university of Singapore. All rights reserved . high temperature can cause hallucinations, while low temperature leads to boring outputs ."
Page 65,GPT-2 https://transformer.huggingface.co/doc/gpt2-large Max time: the maximum amount of time for generation (second)
Page 66,"GPT’s Generative Pre-training + Discriminative Fine-tuning . ""Improving language understanding by generative pre-training."""
Page 67,Page 67 Performance of GPT Results on question answering and commonsense reasoning . page 67 Semantic similarity and classification results .
Page 68,GPT-2 displays broad set of zero-shot capabilities without supervised adaptation or modification . large improvements on small datasets and datasets measuring long-term dependencies .
Page 69,Page 69 Seq2Seq Model: T5: T3: T4: T6: T7: T8: T9: T10: T11: T12: T13: T14: T15
Page 70,"Page 70 Seq2Seq - T5 - Text-to-Text Transfer Transformer, from Google . masked language modeling: – Max sequence length – 512, batch size – 128"
Page 71,– Discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words . – Removed any line with the word Javascript (warnings stating that Javascript should be
Page 72,Page 72 T5 downstream tasks A unified encoder-decoder framework that converts various text-based language problems into a text-to-text format . fed some text for context/conditioning (with task-specific
Page 73,Training a smaller model on more data was often outperformed by training a larger model for fewer steps .
Page 74,"Page 74 Evaluation of Generated Text . Typically for evaluating machine translation or summarization results where human reference text is available . Precision and recall, usually based on ngram matching ."
Page 75,"scores do not correlate well with human judgement (faithfulness, coherence, relevance) - the reference text is not the only correct way to convey the same meaning ."
Page 76,"BertScore measures semantic equivalence, paraphrase detection . cosine similarity score for each token in candidate with each reference . BLEURT predicts ratings given reference and candidate ."
Page 77,"Page 77 Summary of 3 types of models Strengths Weaknesses Good for Encoders- Only (e.g., BERT) Great for language understanding with bidirectional attention ."
Page 78,Page 78 LARGE Language Models & In-Context Learning . National University of Singapore. All rights reserved.
Page 79,"the largest, most powerful language model ever, with 175 billion parameters . training data: mainly Common Crawl (45TB of compressed plaintext before filtering ."
Page 80,"GPT-3: Language Model Meta-Learning focuses on ""task-agnostic"" performance . the model develops a broad set of skills and pattern recognition abilities at training time . then uses those abilities at inference"
Page 81,2019-2025 National University of Singapore. All rights reserved . Page 81 Settings for In-Context Learning No fine-tuning!
Page 82,Page 82 Better In-Context Learning with Larger Model . National University of Singapore. All rights reserved.
Page 83,"page 83 GPT-3 Performances on Benchmark Tasks . Promising results in the zero-shot and one-shot settings for many tasks . – In the few-shot setting, sometimes competitive with or even occasionally"
Page 84,"Page 84 Diversified generative capability . Codes for latex, python, machine learning ."
Page 85,"LLM like GPT-3 impressed the world with its abilities in . Generating language to complete the given prompt, and then generate the solution for a new case."
Page 86,2019-2025 National University of Singapore. All rights reserved . Reject improper or out-of- scope questions .
Page 87,Page 87 RLHF: 2019-2025 National University of Singapore. All rights reserved. Page 88 SLHF: 2019/2025 .
Page 88,"LLM is stateless (no memory) – No access to external data, knowledge, etc. – Unable to perform actions – Complicated prompt design and engineering ."
Page 89,Answer questions related to the contents of your own documents . Often add memory to allow multi-turn conversation for a user session .
Page 90,"if the documents fit in a single context window, pass them in one go to the LLM to summarize . if not, take the map-reduce approach ."
Page 91,Page 91 chatbots powered by LLM provide information about the user . memory – remember past interactions (messages) with the user with a certain window size – Return the last K messages .
Page 92,Page 92 LLM + Tool = Agent combines the decision-making ability of an LLM with tools to create agents that can perform specific tasks . page 92 Agent + Tool: Agent is a tool that can be used to
Page 93,Page 93 LARGE Language Models: Risks and Concerns . 2019-2025 National University of Singapore. All rights reserved.
Page 94,https://www.businessinsider.com/chatgpt-essays-college-cheating-professors-caught-students-ai-plagiarism-2023-1
Page 95,"page 95 Various potential risks on language models, by Deepmind . 2019-2025 National University of Singapore. All rights reserved ."
Page 96,"page 96: ""Internet-trained models have internet-scale biases"" page 97: ""...when prompted for language generation with the input ""what is the gender of a doctor?"""
Page 97,training data extraction attack . large language models memorize and leak individual training examples . USENIX Security 21. 2021.
Page 98,three incidents using ChatGPT in Samsung . source code executing a semiconductor equipment measurement database . test sequences for identifying faults in chips .
Page 99,"bias inherit unchecked biases and associations from large, uncurated, Internet-based datasets . misinformation – sometimes generating false statements, a.k.a. ""hallucination"""
Page 100,2019-2025 National University of Singapore. All rights reserved . Page 100 From Generative Language Model to Geneative AI .
Page 101,"Transformer models are domain agnostic, applicable to 1-D sequences of any form . apply to: music generation with different instruments and styles, Jukebox (apr 2020, given lyrics, genre and artist as input"
Page 102,https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/
Page 103,page 103 The Gen AI Market Map V3 . https://www.sequoiacap.com/article/ generative-ai-act-two/
Page 104,2019-2025 National University of Singapore. All rights reserved. Page 104 For developers... To build Gen AI applications in production...
Page 105,page 105 Embracing Gen AI https://www.sequoiacap.com/article/ generative-ai-act-two/
Page 106,"Fei-Fei Li, Computer scientist and Co-director of the Stanford Institute for Human-Centered AI, 2021 . Li: ""together, we can achieve unimaginable heights of innovation and progress"""
Page 107,"LLMs have been revolutionizing NLP and rapidly progressing . fine-tuned models perform well on specific tasks . decoders-only models have been dominating the scene, paving the way to generative"
Page 108,"arXiv preprint:1810.04805 (2018). • Brown, Tom B., et al. ""Language models are few-shot learners."" (2018)."
Page 109,"How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources . ""Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond"""
Overall Summary,"2019-2025 National University of Singapore. All rights reserved . Page 102: 'the future of AI is not about man versus machine, but rather man with machine' page 108: ""How does GPT Obtain its ability?'"
