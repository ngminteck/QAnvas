Page,Summary
Page 1, Dr. Aobo Wang isswan@nus.edu.g.S.u.ngu: Texture is textured and computerized . Texture 4: Trending Topics on LLMs is next chapter of a
Page 2," Day 4 Trending Topics on LLMs include Model Tuning with Prompts/Instructions, Reasoning and Parameter-Efficient Fine-tuning ."
Page 3, Fine-tuning with Instructions . Fine-tuneing with instructions . Fine tuning with instructions. Fine-turbing with Instructions. Fine tuning instructions .
Page 4, Fine-tuning with Instructions . Fine-tuneing with instructions . Fine tuning with instructions. Fine-turbing with Instructions. All Rights Reserved .
Page 5, Model Supervised Fine-Tuning is a form of instruction for fine tuning . The company is developing a new program to improve its tuning capabilities . The program is based on the idea of fine tuning by hand .
Page 6,Model Supervised Fine-Tuning
Page 7," LLaMA’s family: Alpaca, alpaca and family members of the family . The family includes a llama, a llaca, a giraffe, a goat, a sheep and a goat alp"
Page 8,LLaMA’s family
Page 9,Prompt Tuning
Page 10," Prompt Tuning prefers  larger models and larger models . Prompt Engineering is not good enough . Prompt engineering is  not good enough. Prompt tuning prefers larger models. Prompt engineering prefers larger, better tuning. Prompt Engineering prefers  pre"
Page 11, Parameter-Efficient Fine-tuning is a key feature of the National University of Singapore’s new study . The study was published by the National Institute of Singapore .
Page 12, Parameter-Efficient Fine-tuning is a formality of fine tuning . Parameter Efficient Fine Tuning is an ideal formality for fine tuning in the world .
Page 13, Parameter-Efficient Fine-tuning is a formality of fine tuning . Parameter Efficient Fine Tuning is an ideal formality for fine-tunating algorithms . The University of Singapore has published a book on fine-
Page 14, Parameter-Efficient Fine-tuning is a key to fine tuning . Prefix-tuneing is an important part of fine tuning in fine-tunating .
Page 15, 16 progressivelyprompt vectors . Prefix Tuning (a) is a prefix to a phrase used in pre-fixing .
Page 16, Parameter-Efficient Fine-tuning (b) refers to Prompt Tuning . Usually it refers to  parameter-efficient fine tuning .
Page 17," 18-soft prompt with Embedding prompt . Prefix Tuning (b)    prefix tuning (b), prefixing (c) pre-fixing and pre-tuning (d) Prefix"
Page 18, Prefix Tuning (c)© Copyright National University of Singapore. All Rights Reserved . 19LSTM was created by the National Institute of Singapore .
Page 19, Visual Prompt Tuning (VPT) 2022 . Prefix Fine-tuning in CV prefix fine-tuned in CV Prefix .
Page 20, Context Optimization (CoOp) 2022 is a form of context-optimization . Prefix Fine-tuning in CV pre-programmed will be used to fine-tune in the future . Pre-programming will be
Page 21, Parameter-Efficient Fine-tuning (LORA) metrics are key to fine-tunating . Low-rank (LorA) metric metrics are important for fine tuning .
Page 22, Parameter-Efficient Fine-tuning (LORA) is a form of low-rank Adaptation . Parameter Efficient Fine Tuning is an ideal form of fine tuning .
Page 23, Low-rank Adaptation (LORA) is a form of low-rank adaptation . LORA is an acronym for low-ranking Adaptation and low-ranked Adaptation .
Page 24,LLaMA’s family
Page 25, There is still a large gap in performance between LLaMA 2 70B and GPT-4 and PaLM-2-L .
Page 26,LLaMA2
Page 27,LLaMA2
Page 28, MoE: Towards Ultimate Expert is a Mixture-of-Experts Language Model . MoE is a language model for the Mixture of Experts .
Page 29, Day 4 Trending Topics on LLMs: Model Tuning with Prompts/Instructions . LLaMA’s family . Parameter-Efficient Fine-tuning workshop . Extra topics to discuss: RL & Reasoning
Page 30, Scaling Law (2020) “linearly“increased exponentially” increase exponentially . Scaling law (2020) will be implemented in Singapore in the coming years .
Page 31, Emergent Phenomena: Knowledge driven logic driven by logic driven . Emergence Phenomenas: Knowledge and logic driven in the world of science .
Page 32, An ability is emergent if it is not present in smaller models but is present in larger models . Emergent Phenomena is an ability that can be emergent .
Page 33," FlOPs: floating point  metrics, floating point operations, a measure of a model's scale that considers not only parameters but also factors like data volume and training epochs . Emergent Phenomena: Emergentphenomena"
Page 34, Emergent Phenomena is the work of the National University of Singapore's National Institute of Science and Technology Institute of Singapore . It is the result of a collaboration between the two universities .
Page 35,Emergent Phenomena
Page 36," Emergent Phenomena13B is very basic, not likely to deal with logic heavy tasks accurately . 40B - 50B could be possible . Better to start from 70B ."
Page 37," Emergent Phenomena13B is very basic, not likely to deal with logic heavy tasks accurately . 40B - 50B could be possible . Better to start from 70B ."
Page 38,Emergent by Grokking ?
Page 39,"Impossible Triangle
Retention"
Page 40,"RetNet
41"
Page 41,RetNet
Page 42, RetNet explained the much-awaited transformers-killer is here . Training (QK^D)V Infrerence (Q(K)V) taught is a form of recurrence and parallelism .
Page 43," Is GPT-4 a Good Data Analyst? Use an incomplete and inaccurate evaluation method: Swap the order of the scenes . Utilize linguistic barriers: Incorporate a small amount of Chinese language data, programming languages,"
Page 44,Evaluation
Page 45,Evaluation
Page 46, Hallucination: 'Hallucination' 'Factualness: Factualness . 'Factuality' is 'factualness' and 'Faithfulness': 'Factoriality' ' Factuality' is a form of
Page 47, Intrinsic Hallucination: conflicting with the input content of input content . Extrinsical Hallucinations: creating unknown facts* IntrInsic Halluination: coincides with input content. Extr
Page 48, Hallucination is a formative form of memory loss . Training data quality is a factor of training data quality and memory quality .
Page 49," Deduplicating training data makes language models better . Hallucination: Causes (Data) and causes (Data):                 Training data quality:                . Hallucinations:                ,                ,  Halluc"
Page 50," Causes (Prompts) include: Hallucination, Role Play, Quiz/Instruct, Query/Instruct + Background Content . Prompt quality: ""Be more specific when ask question,"" ""Role Play,"" ""Query/Instruct"""
Page 51," Sampling algorithm (top-p) instead of BeamSeach instead of beamSeach (bottom-p), was used to sample samples . Sampling algorithms (top p) and top-p: Sampling algorithm "
Page 52, Hallucination: Causes (Sampling): Causes (sampling):  Causes (Samples):  Sampling algorithm (top-p) Algorithm (link) Sampling algorithms: Factual-nucleus sampling algorithm
Page 53," Causes (Exposure Bias): Training and the generation procedure mismatch . Minimum Risk Training (MRT), which avoids exposure bias, can mitigate this (link)"
Page 54," Hallucination: How to detect: By External Refence, by external refence or by external force . How to diagnose: BLEU/ROUGH score against the reference: Factualness/Factualness. How to"
Page 55," Hallucination: How to detect: 'By External Refence,' 'By external Refence' 'How to detect': 'How-to-detection: 'How To detect: ""By external refence,"" 'How How"
Page 56," How to detect: By External Refence or Reference Free method. Hallucination. How-to- detect: 'By external Refence. By External . Refence' How-To-Identify: ""By External. Ref"
Page 57," Hallucination: How to detect (Faithfulness) by Question generation (FEQA) How-to- detect (faithfulness) Refence-Free methods (FQA), How-To-Discover how-to detect"
Page 58," How to detect: (Faithfulness) (Refence-Free methods) (NLI) by Natural Language Inference . (Foghght) How-to-diagnosis: Faithfulness, Refence-free methods"
Page 59, How to detect: (Faithfulness) or (Refence-Free methods) by Natural Language Inference (NLI) Hallucination .
Page 60," Hallucination: How to detect: Faithfulness, Refence-Free methods and Factualness Classification Metric . Annotate/construct a set of data related to illusions/facts; Train a detection model; . Train a"
Page 61, Hallucination: Mixture with Multiple tasks; Sketch to content (Two-Stages Generation) Two-Stage Generation: Controllable grounded response generation (Inductive Attention) BETA: Reward Function by RL (BERT Ev
Page 62, Day 4 Trending Topics on LLMs: Model Tuning with Prompts/Instructions . LLaMA’s family . Parameter-Efficient Fine-tuning workshop . Extra topics to discuss: RL & Reasoning
Page 63,Reinforcement learning
Page 64, RL Framework  RL Framework Framework Reinforcement learning . RL Framework is based on the principles of reinforcement learning and reinforcement learning .
Page 65,RF with DL
Page 66," RF with DL Example: (Forcing +1 or -1 movement of the card) Reward: Score Function [seconds keeping up] State: {L,R,UP,DOWN}Agent: 2-layers NN agent"
Page 67, Reinforcement learning is a form of reinforcement learning that can be taught to learn with limited action spaces . Reinforcer learning is an important part of the learning curve .
Page 68," RF with NLP says: ""Transformers"" and ""transformers"" are Transformers, Transformers and Transformers . The word is selected by softmax to select a word from softmax . The sound of softmax is the result of soft"
Page 69,Reinforcement learning
Page 70, RL with Human Feedback with Human feedback . Policy: generate a sequence of words based on prompt . Action space: Select a word by softmax from V. Observation space: all possible sequences of words V**length .
Page 71, Reinforcement Learning from Human Feedback (Natural Language Processing at UT Austin) - YouTube. RL with Human Feedback .
Page 72," Instead of RF, use RM instead of LLM . Initial LLM, (GPT), 175B, 175B and 6B . Initial RM: 175B; RM: 6B; RL: RL with Human Feedback; RM"
Page 73, RL with Human Feedback: Policy generates a sequence of words based on prompt . Action space selects a word by softmax from softmax by selecting a word V**length . Reward function: RM + Policy shift (Tuned LM by RM
Page 74,Reasoning
Page 75, SimpleRL-reason is a replicate of DeepSeek-R1-Zero and DeepSeeks-R2-Zero . It is based on training on small models with limited data . RL with rule-based reward is RL with
Page 76,Reasoning
Page 77, The Power of Scale for Parameter-Efficient Prompt Tuning arXiv:2104.08691 . The power of scale is based on the factuality in Generation with Dependency-level Entailment (Goyal &
Overall Summary," Day 4 Trending Topics on LLMs: Fine-tuning with Prompts/Instructions and Reasoning Scaling Law (2020) Extra topics to discuss: LLaMA 2 70B, GPT-4 and PaLM"
