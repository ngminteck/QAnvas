Page Number,Summary
1,"The document discusses the concept of conversational user interfaces (CUIs) and how they involve the processing of spoken language. It mentions Dr. Gary Leung, who is an expert in this field and can be contacted via email. The document likely goes on to discuss the potential impact and development of CUIs in the future."
2,"The document outlines the agenda for Day 3 and 4 of the NUS-ISS-CUI-Speech workshop, which focuses on accelerating digital excellence. Day 3 will cover speech processing basics, speech recognition, and case studies on integrating speech recognition and natural language processing solutions. Day 4 will cover speech synthesis, voice conversion and generation, and spoken dialogue systems."
3,"The document discusses the basics of speech processing, which involves converting speech signals into digital form for analysis and manipulation. This process involves three main steps: speech signal acquisition, speech signal processing, and speech signal synthesis. Various techniques such as sampling, filtering, and feature extraction are used in speech signal processing. The goal of speech processing is to improve the accuracy and efficiency of speech recognition and synthesis systems."
4,"is an emerging leader in the field of speech signals, with a strong research and development infrastructure and a growing pool of talent. The country has also seen significant advancements in areas such as speech recognition, natural language processing, and speech synthesis.

Singapore is a leader in speech signals, with a strong research and development infrastructure and a growing talent pool. The country has made significant progress in speech recognition, natural language processing, and speech synthesis."
5,"The speech technology landscape is rapidly evolving, with the rise of voice assistants and smart speakers such as Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana. These technologies are being used in various settings, including customer service phone lines, voice command systems in vehicles, and voice-activated home automation. Additionally, there are also language learning apps and speech therapy tools that utilize speech technology. As this technology continues to advance, it will play a crucial role in shaping the future of human-computer interaction."
6,"The process of spoken dialogue involves several steps, including speech recognition, natural language understanding, dialogue management, natural language generation, and speech synthesis. Speech recognition converts spoken words into written text, while natural language understanding interprets the meaning behind the words. Dialogue management controls the flow of the conversation, and natural language generation produces responses in natural language. Finally, speech synthesis converts written text into spoken words. This process is essential for developing advanced technologies in spoken dialogue, and the National University of Singapore is committed to accelerating digital excellence in this area."
7,"The document discusses the process of capturing and recording human speech using a microphone and storing it in a computer as a sequence of numbers. This is an important aspect of accelerating digital excellence, which is a key focus of the National University of Singapore."
8,The analog speech signal captures pressure variations in air produced by the speaker and is converted into a sequence of discrete values through regular sampling. This process is known as the sampling rate or frequency. The National University of Singapore is committed to accelerating digital excellence in this area.
9,"The sampling rate of a speech signal is important for capturing all the necessary frequencies that the human ear can perceive. Ideally, a rate of 30kHz or higher is needed to accurately capture all speech details, but for practical reasons, lower rates such as 8kHz and 16kHz are often used in telephone, PC, and smartphone technology. CD recordings typically have a sampling rate of 44.1kHz."
10,"The document discusses speech coding and its key components. It mentions quantization, which is the process of converting a continuous signal into discrete values. Each sample is represented by a fixed-point number on a computer. The encoding format used is Linear coding, specifically Pulse-Code Modulation (PCM), which is the basic method for digital representation of audio signals. Non-linear coding schemes such as A-law and mu-law use logarithmic encoding techniques to improve the quality of quiet sounds by increasing the quantization resolution for low-amplitude signals."
11,"The document discusses popular audio formats such as WAV, MP3, FLAC, and OGG, highlighting their key features and uses. It notes that non-compressed PCM format is commonly used for speech input in speech processing, and that speech recognition models are often built for different sampling rates."
12,"The quality of recordings at NUS-ISS in Singapore depends on several factors, including microphone quality, environmental noise level, and proper recording level settings. If the recording level is too low, the resolution may be lost, while setting it too high can result in clipping, where the signal value exceeds the maximum. NUS is focused on accelerating digital excellence in this area."
13,"'s digital transformation strategy

In this section, the speaker discusses Singapore's digital transformation strategy for the next five years. The strategy is centered around three key pillars: building a digital economy, developing a digital government, and creating a digital society. The speaker highlights the importance of investing in digital infrastructure, promoting innovation and entrepreneurship, and upskilling the workforce to support the growth of a digital economy. The government's role in driving this transformation is also emphasized, with plans to digitize public services and enhance digital capabilities. Lastly, the speaker stresses the need for inclusivity and ensuring that all members of society can benefit from the digital transformation. 

Singapore's digital transformation strategy for the next five years focuses on three key pillars: building a digital economy,"
14,"The National University of Singapore (NUS) recognizes that speech is a one-dimensional signal and to better analyze it, it can be converted into a two-dimensional image known as a spectrogram. This image displays the strength of different frequencies of the signal at any given time. NUS is committed to accelerating digital excellence through this type of analysis."
15,"2025

The document discusses how different sounds have varying energy levels at different frequencies, leading to spectral differences in pronunciations. This is important to consider in the development of speech recognition technology, as it can impact the accuracy and effectiveness of the system. The National University of Singapore aims to accelerate digital excellence by 2025."
16,"The document discusses the concept of frames in speech signal processing. Frames are segments of speech that are typically 10, 20, or 25 milliseconds wide and can be overlapped with each other. The frame size refers to the length of the speech segment, while the frame shift is the number of samples shifted to the next frame. Understanding frames is important for effectively processing speech signals."
17,The speech signal in Singapore is analyzed frame by frame and each frame is converted into a vector. This allows for a speech signal to be represented as a sequence of feature vectors. These feature vectors are extracted from the speech waveform using signal processing techniques. This process is important for accelerating digital excellence in Singapore and is a focus of the National University of Singapore.
18,"The document discusses three common speech features used in Singapore, focusing on their applications and effectiveness. First, Mel-Frequency Cepstral Coefficients (MFCCs) are used to represent the short-term power spectrum of sound and capture the phonetic content of speech. They are widely used in speech and speaker recognition. Second, Linear Predictive Coding (LPC) analyzes the spectral envelope of speech and is commonly used in speech compression, synthesis, and recognition. These features are essential in accelerating digital excellence in Singapore."
19,"The document discusses the importance of providing environmental information in the context of human-computer interaction. This includes information about indoor and outdoor environments, as well as noise levels. The author emphasizes the need for this information to be accurate and up-to-date, as it can greatly impact the usability and effectiveness of computer systems. They also mention the potential for technology to assist in gathering and presenting this information to users. Overall, the document stresses the importance of considering environmental factors in designing and using computer systems."
20,"The document discusses the importance of content and timbre in speech technology. Content refers to the text transcription of a speech signal, and speech recognition is the process of extracting this content from the signal. On the other hand, speech synthesis involves using the text information to produce a speech signal. Timbre, on the other hand, refers to the unique speaker information conveyed through their voice. Different speakers have different timbres, making it an important factor in speech technology."
21,"Prosody is the way in which a text is read and is determined by elements such as intonation, rhythm, pause, emotion, speaking styles, and speech rate. These elements can be perceived at the acoustic level through fundamental frequency, duration, and energy. For example, the sentence ""I bought two books from the shop"" can be read in different ways depending on prosody. Understanding prosody is important for effective communication and can be enhanced through digital tools and technologies."
22,"The document outlines five examples of perceived prosody, including intonation, lexical tone, emphasis, emotion, and phrase break. It notes that statements typically have a falling intonation while questions may have a rising intonation. Mandarin has four lexical tones, and certain words can be emphasized in speech. Emotion can also affect prosody, with angry and happy speeches having different patterns. Additionally, phrase breaks within a sentence are considered part of prosody."
23,"The document discusses the fundamental frequency (pitch) of speech signals, which is produced when the vocal cords vibrate. Voiced signals are periodic and unvoiced signals are noise. All vowels are voiced, while sounds like /s/ and /f/ are unvoiced. Pitch is a characteristic of voice signals only and can be measured as the duration between two pitch marks. It is calculated as 1/period."
24,"The fundamental frequency, also known as pitch or FO, is a crucial aspect of speech perception and generation. It is represented by the number of vibrations per second and is measured in Hertz (Hz). The pitch of a speech can vary depending on the individual's voice and can be affected by factors such as age, gender, and language. Understanding and utilizing pitch is essential in achieving digital excellence in speech technology."
25,"The document discusses three common speech preprocessing techniques used in SINUS: voice activity detection (VAD), speech enhancement, and speech normalization. VAD is used to detect when speech is present and only starts processing when voice is detected. Speech enhancement is used to improve speech quality by reducing noise. Speech normalization is used to convert the speech signal to maintain consistency, through time and frequency domain normalization. These techniques are important for improving the accuracy and effectiveness of speech recognition systems."
26,"This section discusses the growing importance of audio software in Singapore and its potential impact on the local industry. It highlights the increasing demand for audio software due to the rise of digital media and the need for high-quality audio production in various industries such as film, gaming, and music. The government has also recognized the potential of this sector and is investing in initiatives to support its growth. However, there are challenges such as a shortage of skilled professionals and the need for continuous innovation to stay competitive. The section concludes by emphasizing the need for collaboration and partnerships to further develop the audio software industry in Singapore.

The third part of the document focuses on the significance of audio software in Singapore and its potential impact on the local industry. The demand for high-quality audio production"
27,"The National University of Singapore offers a professional audio editing software, Adobe Audition, which allows users to record, view, and play audio signals. It also has features such as cutting, copying, pasting, and mixing audio, as well as tools for creative editing, such as normalizing, filtering, and removing noise. Users can also view spectrograms and change sampling rate and file formats. This software is part of NUS's efforts to accelerate digital excellence."
28,"The National University of Singapore offers a free and open-source audio editor and recording software called Audacity. It is available for Windows, macOS, and Linux and allows users to edit, record, and analyze audio files. The software also has features such as monitoring, microphone and speaker options, and project settings. It has a project rate of 44100 Hz and a snap-to audio position function. The university is committed to accelerating digital excellence through this software and other tools."
29,"The document discusses the features of Audacity, a digital audio editing software. Users can record and save files, select sampling rate, file format, and resolution, and view and edit files by zooming in and out, cutting, copying, pasting, and generating silence and noises. The software also allows for mixing and applying effects, converting stereo to mono, and normalizing and reducing noise. Users can also change the pitch and speed of audio files. This software is part of the National University of Singapore's efforts to accelerate digital excellence."
30,"The National University of Singapore has developed an open-source cross-platform audio editing software called Sox (Sound of Exchange). It is a command line tool that can convert sampling rate, bits, and stereo/mono settings, as well as perform editing functions such as concatenation, trimming, and volume adjustment. Sox also offers various effects like chorus, flanger, and echo, and allows for adjustments of speed, pitch, and tempo. This software reflects the university's commitment to accelerating digital excellence."
31,"The document discusses the National University of Singapore's Institute of Systems Science's vision for 2025, which includes accelerating digital excellence. This involves leveraging emerging technologies such as artificial intelligence, data analytics, and Internet of Things to drive innovation and transformation in various industries. The document also mentions the importance of partnerships and collaborations with industry and government to achieve this vision. Additionally, the document highlights the use of tools such as SoX to manipulate audio files, providing examples of how it can be used to change sampling rates and convert files to mono."
32,"The document discusses the use of the SOx command in NUS-ISS-CUI-Speech-Part1-2025.pdf. SOx is a command line tool used for audio processing, with examples including converting audio files from raw format to wav format, concatenating files, and adjusting the speed of speech. The document emphasizes the importance of digital excellence and how SOx can contribute to this goal."
33,"The National University of Singapore's Institute of Systems Science has developed an open-source tool called WaveSurf for sound visualization and manipulation. It is available for use on Linux, macOS, and Windows platforms and can be accessed through the website https://sourceforge.net/project/wavesurder. This tool aims to accelerate digital excellence and is copyrighted by the university."
34,"'s National University of Singapore Institute of Systems Science (NUS-ISS) is committed to developing a strong and vibrant audio programming industry in Singapore by 2025. This includes providing education and training programs, conducting research and development, and collaborating with industry partners.

NUS-ISS is dedicated to building a thriving audio programming industry in Singapore by 2025 through various initiatives such as education and training, research and development, and partnerships with industry players."
35,"The document discusses the importance of ChatGPT and Gemini inColab code segments in enhancing the capabilities of conversational AI systems. These code segments allow for a more efficient and effective development process, as they provide pre-built components that can be easily integrated into the system. This not only saves time and resources, but also improves the overall performance and accuracy of the AI system. Additionally, ChatGPT and Gemini inColab code segments enable non-technical users to easily create and customize conversational AI solutions, making it more accessible and user-friendly. Overall, these code segments play a crucial role in advancing conversational AI technology and making it more accessible to a wider audience."
36,"The document discusses the use of Anaconda, a Python distribution for scientific computing that supports multiple operating systems. It includes basic packages and programming tools, as well as two IDE tools: Spyder and Jupyter Notebook. Anaconda is part of the National University of Singapore's efforts to accelerate digital excellence."
37,"The document discusses the use of the Python audio library SoundFile, which can be downloaded and installed through various methods. It supports various file formats such as WAV, FLAC, OGG, and MAT, and can be programmed in Python by importing the library. The functions of reading and writing audio files are demonstrated through code examples."
38,"The document discusses the use of PyAudio, a Python library for audio playing and recording, in programming. It provides instructions for installing PyAudio on Windows and Linux systems, and includes sample code for using PyAudio in a program. The library allows for customization of audio format, channels, and playback rate. This information is part of a larger discussion on accelerating digital excellence at the National University of Singapore."
39,"The LibROSA software is used for feature calculation and display in the NUS-ISS-CUI-Speech-Part1-2025.pdf document. It can be installed using the command ""pip install librosa"" and used for programming tasks such as converting amplitude to decibels and displaying a linear-frequency power spectrogram. LibROSA is part of the National University of Singapore's efforts to accelerate digital excellence."
40,"The advancement of speech recognition technology has greatly improved its accuracy and usability, leading to its integration into various devices and applications. However, there are still challenges to be addressed, such as dealing with different accents and languages, as well as the need for continuous learning and adaptation to new speech patterns. To overcome these challenges, there is a need for more data and better algorithms, as well as collaboration between researchers and industry experts. In the future, speech recognition will continue to play a crucial role in human-computer interaction, particularly with the rise of smart homes and virtual assistants. Additionally, the integration of speech recognition with other emerging technologies, such as artificial intelligence and natural language processing, will further enhance its capabilities and potential applications."
41,"The introduction section of the document outlines the purpose and objectives of the NUS-ISS Centre for Urban Infrastructure (CUI) for the year 2025. The CUI aims to be a leading research and education center in the field of urban infrastructure, with a focus on sustainable and resilient solutions. It also aims to collaborate with industry partners and government agencies to address real-world challenges in urban infrastructure development. The CUI envisions to be a hub for knowledge exchange, innovation, and thought leadership in the urban infrastructure space."
42,Automatic Speech Recognition (ASR) is a process that converts spoken words into written text. It is a natural way to input information into a computer and involves transforming a continuous sequence of values into discrete symbol representations. ASR is the first step in processing speech and does not interpret meaning at this stage.
43,"The document discusses the challenges of speech recognition technology in the future, specifically in the year 2025. It mentions the importance of understanding spontaneous speech and the differences between reading and spontaneous speech. The document also highlights the difficulties of dealing with reverberation and overlapping audio sources in speech recognition. It provides links to further readings and videos for more information on these topics."
44,"The history of speech recognition can be traced back to the 1950s when Bell Labs developed the ""Audrey"" system to recognize spoken digits. In the 1960s, statistical methods were used for pattern recognition and IBM's ""Shoebox"" machine could recognize 16 English words. The 1970s saw the use of template matching and DARPA funding leading to significant advancements. In the 1980s, Hidden Markov Models (HMMs) became the dominant approach for speech recognition and the first commercial systems were introduced."
45,"The history of speech recognition can be divided into four major eras. In the 1990s, systems were developed to recognize continuous speech and a large vocabulary. The rise of the internet in the 2000s enabled data-driven approaches, leading to the development of voice-controlled assistants like Siri and Google Voice Search. In the 2010s, the resurgence of neural networks, particularly deep learning, brought significant improvements to speech recognition. In the current era, the focus is on making models more efficient, understanding context, handling multiple languages and accents, and operating in noisy environments. Edge computing has also allowed for powerful on-device speech recognition without the need for a cloud connection. There is also a growing emphasis on user privacy and ethical considerations in"
46,"The National University of Singapore (NUS) has been focusing on accelerating digital excellence through the development of Automatic Speech Recognition (ASR) technology. This technology has seen significant advancements in recent years, as evidenced by the work of researchers D. Jurafsky and J.H. Martin in their book Speech and Language Processing. This highlights the university's commitment to staying at the forefront of digital innovation."
47,"The last 10 years have seen significant advancements in deep learning automatic speech recognition (ASR), with major tech companies such as IBM, Microsoft, Google, and Baidu making significant contributions. In 2012, NUS and ISEe developed a DNN-based ASR system. In 2017, Google achieved human parity on speech recognition using a DNN-based approach. In 2019, Google released an end-to-end offline mobile ASR system. Other notable advancements include the use of attention and waveLM in DNN-based ASR, as well as the use of RNN and LSTM in speech recognition. Multilingual ASR and speech recognition in videos have also been explored using deep learning techniques. These advancements have greatly accelerated"
48,"The document discusses the evolution of AI systems and how they have surpassed human performance in various tasks such as speech recognition, handwriting recognition, image recognition, reading comprehension, and language understanding. This has been demonstrated through the use of Dynabench, a new benchmarking system for evaluating AI performance. The document highlights the importance of continuously improving AI systems to maintain their superiority over human capabilities."
49,"The document discusses the evolution of automatic speech recognition (ASR) methods and how they have led to the acceleration of digital excellence. It highlights the importance of ASR in various industries, such as healthcare, education, and finance, and how it has improved efficiency and accuracy. The advancements in deep learning and natural language processing have greatly enhanced ASR capabilities, making it more accurate and adaptable to different languages and accents. The use of ASR has also led to the development of new technologies, such as chatbots and virtual assistants, further driving digital excellence."
50,The National University of Singapore (NUS) has developed a feature calculation system for speech analysis. The input consists of 400 sample points and the output includes 23 MFCC features and 80 filter bank outputs. This system is designed to accelerate digital excellence and improve speech analysis capabilities.
51,"in 2025

The document discusses the concept of phonemes and phones in English pronunciation. Phonemes are the smallest unit of pronunciation that represent meaning, while phones are the actual sounds of phonemes. English has about 44 phonemes, including monophthongs and diphthongs. Examples of phonemes include ""cat"" and ""good,"" while examples of phones include the ""K"" sound in ""cat"" and the ""G"" sound in ""good."" The document also mentions the four phonemes of Received Pronunciation, which is a standard form of British English. These concepts are important for understanding and improving English pronunciation."
52,"OF TEACHING


The traditional methods of teaching have been the predominant approach in education for centuries. These methods involve a teacher-centered approach, with the teacher as the main source of knowledge and students as passive recipients. This approach relies heavily on lectures, textbooks, and exams, and places a strong emphasis on memorization and regurgitation of information. However, with the rapid advancement of technology and changing needs of the workforce, there is a growing recognition that traditional methods may not be sufficient in preparing students for the future. As such, there is a need for a shift towards more innovative and interactive teaching methods that foster critical thinking, collaboration, and problem-solving skills."
53,"The speech recognition framework at NUS-ISS involves two main components: the acoustic model and the language model. The acoustic model evaluates speech features against pronunciations to determine how likely the speech signal sounds like the word sequence. The language model evaluates the word sequence to determine its reasonableness and likelihood of being a correct sentence. This framework is crucial in accurately recognizing and transcribing speech, and plays a key role in NUS-ISS's goal of accelerating digital excellence."
54,"The likelihood of a certain word sequence being correct can be evaluated by analyzing the speech signal and determining how likely it is to correspond to a sequence of sound units. This process involves using statistical models and algorithms to compare the speech signal to a database of known sound units and determine the most probable sequence. This method is useful for tasks such as speech recognition and language identification. However, it can be challenging due to variations in speech patterns and accents, and requires ongoing research and development to improve accuracy."
55,The most commonly used method for speech recognition is Hidden Markov Models (HMM). Each phone is represented by a HMM model with 3-5 states. Speech frames are mapped to state observations and the goal of speech recognition is to calculate the probability of a given sequence of observations (O) given a specific word (W). This method is important in accelerating digital excellence.
56,"CuI-Speech-Part1-2025.pdf


Page 56 of the document 'NUS-ISS-CUI-Speech-Part1-2025.pdf' discusses the importance of collaboration between NUS and industry partners in achieving the goal of becoming a leading global university by 2025. It highlights the need for joint research projects, industry-sponsored programs, and internships to bridge the gap between academia and industry. The document also emphasizes the importance of building a strong ecosystem for innovation and entrepreneurship to foster a culture of innovation within the university. Additionally, it mentions the role of technology in transforming education and the need for NUS to continuously adapt and innovate to stay ahead in the rapidly changing landscape of technology and education."
57,"The Language Model National University of Singapore is focused on finding the most likely word sequence from all possibilities. They use NN-Grams, which combines neural network and n-gram language models, to improve speech recognition. This approach was presented by authors Babak Damavandi, Shankar Kumar, and Antoine Bruguier at INTERSPEECH 2016. The university is committed to accelerating digital excellence."
58,"The National University of Singapore (NUS) uses N-gram models to improve language processing. These models calculate the likelihood of a word based on previous words, with the Unigram, Bigram, and Trigram models considering one, two, and three previous words, respectively. Language models are trained using text data, allowing for more accurate and efficient language processing. This is part of NUS's efforts to accelerate digital excellence."
59,"This section discusses the development and future of end-to-end speech recognition technology at the National University of Singapore's Institute of Systems Science. The technology aims to improve speech recognition accuracy and efficiency by using deep learning methods. The institute is also working on integrating this technology into various applications, such as virtual assistants and call centers. The ultimate goal is to create a seamless and natural interaction between humans and machines through speech recognition."
60,"The document discusses the difference between hybrid and end-to-end (E2E) modeling in speech recognition. In hybrid modeling, multiple models are trained separately and then integrated to map the speech waveform to the target word sequence. In E2E modeling, a single model is used to directly map the speech waveform to the target word sequence. The models used in both approaches include classical signal processing, Gaussian mixture models, pronunciation models, and language models. The goal is to accurately recognize and transcribe spoken words."
61,"The document discusses the advantages of end-to-end (E2E) models in speech recognition. These models use a single objective function that is consistent with automatic speech recognition (ASR) objectives and directly output characters or words, simplifying the ASR pipeline. They are also more compact than traditional hybrid models and can be deployed on devices with high accuracy and low latency. The effectiveness of E2E models has been demonstrated in various studies, such as those by Graves and Jaitly (2014) and Hannun et al. (2014)."
62,"The article discusses the current status of end-to-end (E2E) models in automatic speech recognition (ASR). These models have achieved state-of-the-art results in most benchmarks and have optimized practical challenges such as streaming, latency, and adaptation capability. E2E models are now the mainstream models in both academic and industry settings."
63,"The document discusses the use of end-to-end (E2E) models in accelerating digital excellence. These models, such as the Encoder Connectionist Temporal Classification (CTC), Attention-based encoder decoder (AED), and Prediction RNN-Transducer (RNN-T), aim to streamline and improve processes in various industries. They achieve this by combining different components, such as encoders and decoders, to make predictions based on input data. These models are being used to enhance efficiency and accuracy in areas such as speech recognition and natural language processing."
64,"The document discusses various methods for end-to-end speech recognition, including Connectionist Temporal Classification (CTC), Listen, Attend, and Spell (LAS), and RNN Transducer (RNN-T). CTC generates a letter sequence before converting it into text, while LAS uses an encoder/decoder with attention to directly generate a sequence. RNN-T integrates a language model into prediction and does not rely on the full sequence. These methods aim to accelerate digital excellence and improve speech recognition technology."
65,"The National University of Singapore has developed a new end-to-end Automatic Speech Recognition (ASR) system called CTC-based End-to-end ASR. This system combines multiple modules into one neural network for joint training and uses a Sequence to Sequence model to map acoustic features to text results. The Acoustic Model uses a Deep Neural Network (DNN) to approximate the distribution over characters, and a simple collapsing function is used to generate the final results. This system was developed to accelerate digital excellence and is copyrighted by the National University of Singapore."
66,"The document discusses the development of the first and simplest E2E ASR model by the Institute of Systems Science (ISS) at the National University of Singapore (NUS). The model addresses the challenge of target label length being smaller than speech input length by inserting blank and allowing label repetition. It also utilizes the Transformer encoder and self-supervised learning technologies to improve accuracy. This model was presented at the 2025 NUS-ISS CUI Speech Conference, which focuses on accelerating digital excellence."
67,"The National University of Singapore's Institute of Systems Science has developed an attention-based end-to-end automatic speech recognition (ASR) method known as the Listen, Attend and Spell (LAS) method. This method involves three key steps: listen, attend, and spell. The listen step uses an encoder to transform input speech into a higher-level representation. The attend step uses attention to identify relevant encoded frames for producing the current output. Finally, the spell step uses a decoder to predict each output token based on previous predictions. This approach aims to accelerate digital excellence and improve ASR accuracy."
68,"The National University of Singapore (NUS) has developed a LAS model for speech recognition called Performance Based Clean WER (PBCW). It combines CLDNN-HMM and LAS+LM Rescoring methods and has shown success when trained with a large dataset of 12500 hours. This model, along with a proposed model size of 0.4 GB, outperforms the conventional system (6.7/5.0) which requires 7.2 GB of data. This shows the potential for NUS to accelerate digital excellence in speech recognition."
69,The article discusses the importance of streaming in ASR systems and how it can be achieved through various methods such as applying attention on chunks of input speech. The use of RNN-T is highlighted as the most popular end-to-end model for streaming ASR. The article also mentions specific studies and conferences related to this topic.
70,"The RNN Transducer (RNN-T) is a sequence-to-sequence architecture used for speech recognition, jointly learning both acoustic and language model components. It combines acoustic and language information and is trained on text-only data, explicitly conditioning on the history of previous non-blank targets predicted by the model. It can use different units such as grapheme, word, or word-piece, and maps acoustic frames into a higher-level representation. The RNN-T is initialized from the CTC model and is considered a state-of-the-art end-to-end speech recognition system."
71,"The RNN Transducer (RNN-T) developed by the National University of Singapore has the ability to continuously process input samples and stream output symbols, making it ideal for speech dictation. It outputs characters one-by-one in real-time, with proper spacing, and uses a language model to predict the next symbols. This technology is part of the university's efforts to accelerate digital excellence."
72,"The paper discusses various end-to-end (E2E) models for automatic speech recognition (ASR) and their capabilities. The most popular E2E model in the industry is the RNN-T, which requires streaming ASR. This model has surpassed the quality and latency of conventional server-side models. However, further work is needed to improve its performance in natural and long-form scenarios. Two recent studies, Sainath et al. and Li et al., have shown promising results for RNN-T models in terms of customization and performance."
73,"The encoder is the most crucial component in Singapore's digital transformation. It is responsible for converting input data into meaningful representations for further processing. The three main types of encoders are Connectionist Temporal Classification (CTC), Attention-based encoder decoder (AED), and RNN-Transducer or Transformer Transducer. These encoders play a vital role in predicting and improving the accuracy of data processing. Accelerating digital excellence in Singapore relies heavily on the development and optimization of these encoders."
74,"The document discusses the development of a Gy Encoder for RNN-T, specifically at NUS-ISS. This encoder uses a Transformer model to predict the next output based on the previous input, improving the accuracy and speed of speech recognition. The goal is to accelerate digital excellence at NUS and contribute to advancements in the field of speech recognition technology."
75,"The Transformer model, originally developed for natural language processing, is being applied to speech recognition tasks. It uses self-attention to compute attention distributions over the input speech sequence, which are then used to combine value vectors and generate layer outputs. The multi-head self-attention approach applies multiple parallel self-attentions on the input sequence. The Transformer Transducer model, which combines Transformer encoders and RNN-T loss, is a streamable speech recognition model that has shown promising results. A good tutorial on the functionality of Transformers is available for those interested in learning more."
76,"The Conformer is a new model that combines the strengths of the Transformer and Convolutional Neural Network (CNN) for speech recognition. While the Transformer is good at capturing global context, it is less effective in extracting local patterns. On the other hand, the CNN excels at processing local information. By combining the two, the Conformer is able to achieve better results in speech recognition tasks. This model was developed by researchers at the National University of Singapore and was presented at the Interspeech conference in 2025."
77,"Part 4 of the document discusses the emerging trends and advancements in speech processing. It highlights the importance of speech processing in various industries such as healthcare, education, and customer service. The use of artificial intelligence and machine learning in speech processing is expected to increase, leading to more accurate and efficient speech recognition and synthesis. Other advancements include the integration of speech processing with other technologies such as natural language processing and emotion recognition, as well as the development of personalized and adaptive speech systems. The potential impact of these advancements on society and the need for responsible and ethical use of speech processing technology are also discussed."
78,The concept of accelerating digital excellence involves building a foundational layer that can support specialized applications. This requires extensive training data and the ability to adapt to a wide range of downstream tasks.
79,"The document discusses different types of learning at NUS-ISS, including supervised, unsupervised, semi-supervised, and self-supervised learning. Supervised learning requires human-labeled data, making it expensive and difficult to build databases. Unsupervised learning does not require human labels and is easier to build databases with, as it discovers patterns from unlabeled data. Semi-supervised learning uses a small amount of labeled data, while self-supervised learning uses information from input data as the label to learn representations. These different types of learning can help accelerate digital excellence at NUS-ISS."
80,"The Whisper Model is a digital model that has been trained on a large amount of multilingual and multitask supervised data. This model has three main tasks: speech recognition, speech translation, and language recognition. It has a significant number of parameters, ranging from 39 million to 1550 million. This model is developed by OpenAI and has the potential to accelerate digital excellence."
81,"The National University of Singapore is committed to accelerating digital excellence through its Institute of Systems Science. One of the key initiatives is the use of Wav2Vec2.0, a self-supervised learning model, to improve speech recognition technology. This model has shown promising results in accurately transcribing spoken words without the need for large amounts of labeled data. This will greatly benefit industries such as healthcare, finance, and customer service, where accurate speech recognition is crucial. The university aims to further develop and apply this technology in various fields to drive digital transformation and innovation."
82,"Wav2Vec 2.0 is a method for training speech representations through self-supervised learning. It uses quantization to create targets and the training process aims to maximize the similarity between the learned Transformer contextual representation and the quantized input features. This method was developed by A. Baevski, H. Zhou, A. Mohamed and M. Auli in 2020 and is described in their paper ""wav2vec 2.0: A CNN Encoder Framework for Self-Supervised Learning of Speech Representations."""
83,"The Wav2Vec 2.0 model has shown good performance on low resource languages, achieving 8.2% Word Error Rate (WER) with only 10 minutes of fine-tuning data. This level of performance is typically only achieved with thousands of hours of speech data. The model uses context representations from 960 hours of data to achieve a WER of 1.8/3.3%, and latent speech representations from 100 hours of data to achieve a WER of 2.0/4.0%. The results of fine-tuning the Wav2Vec 2.0 model show its potential for accelerating digital excellence."
84,"The document discusses the use of sample representations in various speech-related applications, such as speech recognition, speaker classification, and emotion classification. These representations are created through self-supervised learning and can be used in multiple downstream tasks. Examples of these representations include Masked DeCoAR, TERA, and wav2vec 2.0. They can also be customized for specific applications. These sample representations are developed by the National University of Singapore and can be found on their GitHub page."
85,"The document discusses various toolkits and libraries that are used for pre-training models in speech recognition and processing. These include FAIRSEQ, S3PREL, ESPNet, and SpeechBrain, which offer a range of pre-trained models such as wav2vec, vq-wav2vec, and HuBERT. These tools are essential for accelerating digital excellence and advancing speech technology."
86,"- ISS

Self-supervised learning is a low-cost solution that allows machines to learn from unlabelled data without human annotation. By using contextual information, the accuracy of self-learned representations can be improved. When combined with fine-tuning, self-supervised learning has been shown to greatly improve accuracy in low resource speech recognition. Additionally, it has shown good performance in other downstream tasks such as speaker recognition, emotion recognition, and speech separation."
87,"The National University of Singapore (NUS) is committed to accelerating digital excellence through its ChatGPT program, which combines language and speech capabilities. ChatGPT is a collaboration between NUS and the Institute of Systems Science (ISS), and aims to develop a large language model (LLM) that can understand and respond to human speech in a natural and intelligent manner. This technology has potential applications in various industries, such as customer service and virtual assistants. The program is part of NUS' larger efforts to drive innovation and advance Singapore's digital economy. 

NUS is working with ISS to develop ChatGPT, a large language model with speech capabilities, as part of their commitment to accelerate digital excellence. The technology has potential use in"
88,"Part 5 of the document discusses the challenges faced in training automatic speech recognition (ASR) systems in Singapore. The first challenge is the diverse linguistic landscape of the country, with four official languages and a mix of dialects. This makes it difficult to develop a single ASR system that can accurately recognize all the different languages and dialects. Another challenge is the lack of publicly available speech data for Singaporean languages, which hinders the training and development of ASR systems. The document suggests that collaboration between industry and academia, as well as government support, are needed to overcome these challenges and develop robust ASR systems for Singapore."
89,"The document discusses the use of external language models (LMs) in end-to-end (E2E) models for domain adaptation. It suggests combining the two using shallow fusion, which involves using the output probabilities from both the external LMs and the E2E models to generate a final prediction. This approach has shown promising results in improving the performance of E2E models in different domains. Additionally, the document highlights the importance of choosing the right external LM for a specific domain and the potential of using multiple LMs for even better results."
90,"The document discusses guidelines for acoustic modeling units in different languages. For very phonetic languages like Spanish and German, letter-based or BPE encoding is effective. For reasonably phonetic languages like English, BPE-based acoustic models are preferred, but letter-based models can also be used with large training datasets. For non-phonetic or least phonetic languages like Chinese, it is recommended to use a pronunciation dictionary to map words to modeling units."
91,". The document discusses the importance of accelerating digital excellence, specifically in the field of acoustic modeling. It provides resources for further reading on the topic, including articles and repositories on fine-tuning whisper for multilingual ASR, end-to-end speech processing, and HMM-DNN acoustic modeling. It also mentions the use of HMM-based monophone and context-dependent triphone in speech recognition model training."
92,"The document provides further reading resources on N-gram language models, which are statistical models used in natural language processing. These resources include an overview of N-gram language models, training techniques using SRILM, and methods for building large n-gram models. The document also mentions a morph n-gram model, which is used for analyzing word morphology in language models. These resources can help in accelerating digital excellence in natural language processing."
93,"The training process for NUS-ISS-CUI involves using a dataset of speech that requires thousands of hours to train the modelling system. The dataset is divided into training, development, and testing sets. The system is trained and then tested on the development set, followed by tuning and repeating the process. Finally, the system is tested on the testing set. This process is crucial for accelerating digital excellence at the National University of Singapore."
94,"The importance of data quality and quantity in speech recognition systems is highlighted. It is crucial to have diverse data that covers various accents, dialects, speaking rates, and noise levels in order to ensure accurate and reliable performance. Data augmentation techniques such as time-stretching, pitch-shifting, and adding background noise can be used to increase and diversify the dataset. Transcription accuracy is also emphasized as errors can negatively impact model performance. However, collecting and transcribing speech data can be expensive. Modern speech recognition systems are typically trained with thousands of hours of data, with some recent systems using even more than 60,000 hours."
95,"The document discusses the use of Character Error Rate (CER) or Syllable Error Rate (SER) for certain languages. These metrics are used to measure the performance of speech recognition systems. CER is used for languages with a phonetic writing system, while SER is used for languages with a syllabic writing system. These metrics are important for evaluating the accuracy of speech recognition systems and can help improve their performance."
96,"The document discusses how the performance of automatic speech recognition (ASR) systems is typically measured by Word Error Rate (WER), which assumes that all errors are equal. However, there is a mismatch between this optimization criterion and the actual error measurement. As a result, task-specific measures such as task completion and concept error rate are sometimes used to better evaluate ASR performance. This is important for achieving digital excellence, as accurate ASR systems are crucial for various applications."
97,"The document discusses speaker dependent and speaker independent systems in automatic speech recognition (ASR). While most ASR systems are developed for any user, speaker dependent systems are specifically built for a particular speaker, making it easier to achieve higher accuracy. On the other hand, speaker independent systems work for any new speaker but are more difficult to achieve high accuracy."
98,"The National University of Singapore's Institute of Systems Science discusses additional considerations for their ASR system, including adaptability and fine-tuning for specific domains, online learning for evolving applications, robustness to different noise levels and types, and the impact of different environments on acoustics. These factors should be taken into account during training to ensure the success of the ASR system."
99,"'s National University of Singapore (NUS) Institute of Systems Science (ISS) has been working on developing a Conversational User Interface (CUI) platform that aims to revolutionize the way humans interact with technology. The platform is designed to be user-friendly, intuitive, and able to understand natural language input. However, there are several deployment issues that need to be addressed, such as the need for continuous training and improvement of the platform's natural language processing capabilities, ensuring data privacy and security, and integrating the CUI platform with existing systems and devices. Additionally, there is a need for collaboration with industry partners to ensure the platform meets the needs and expectations of users and stays relevant in the rapidly evolving technology landscape.

The NUS-ISS CUI"
100,"The document discusses considerations in deploying automatic speech recognition (ASR) technology. It mentions that ASR can be used for free text or limited text, such as in dialog systems or dictation. It also mentions that ASR can be used for voice commands in embedded systems. The deployment can either be on the cloud or embedded, with the former having a larger recognition model and the latter having a smaller data size. The document emphasizes the importance of considering these factors when choosing an ASR deployment method."
101,"The deployment of automatic speech recognition (ASR) technology requires consideration of three key factors: the distance between the speaker and the microphone (near field vs. far field), the channel and sampling rate (digital system vs. telephony), and the digital system's sampling rate (16KHz for smartphones vs. 8KHz for traditional telephony systems). These factors must be carefully considered in order to ensure the successful implementation and effectiveness of ASR technology."
102,"The National University of Singapore (NUS) discusses considerations for deploying automatic speech recognition (ASR) technology, including multi-thread processing, real-time factor, memory usage, and response time. Multi-thread processing allows for efficient use of CPU resources, with a real-time factor of 0.1 meaning that one CPU can support 10 users. Memory usage is important, as system data can be shared among running instances, but additional memory is required for working data for each user. Response time is affected by data transfer and delay in the speech recognition engine. NUS emphasizes the importance of these factors in accelerating digital excellence."
103,"The document lists various examples of speech application at NUS-ISS in Singapore, including telephony systems, medical records, court hearing transcription, speech analytics for call centers, voice assistants and smart speakers, voice control devices, in-car applications, and air flight traffic control. These applications showcase the university's commitment to accelerating digital excellence."
104,"'s National University of Singapore Institute of System Science (NUS-ISS)

The NUS-ISS offers a wide range of resources and programming to support the development of professionals in the technology industry. This includes a comprehensive library with access to online databases and e-journals, as well as a dedicated learning management system for online courses. The institute also offers various executive education programs, including customized courses for organizations and industry-specific programs. Additionally, the NUS-ISS has partnerships with leading universities and institutions around the world, providing opportunities for international exposure and collaboration. The institute also hosts events and conferences to facilitate networking and knowledge sharing among industry professionals. Overall, the NUS-ISS is committed to providing a robust and dynamic learning environment for individuals and organizations"
105,"Kaldi is a speech recognition toolkit used by researchers and professionals. It supports advanced features and is open-source. HTK is a proprietary software toolkit for handling HMMs, while EPSnet is an end-to-end speech processing toolkit. Wenet is an open-source speech recognizer designed for deployment and Whisper is an open-source package with a pretrained model for speech recognition and translation. These tools are all aimed at accelerating digital excellence and are developed by various organizations."
106,"The document discusses different cloud services that can be used for speech recognition, including Google Speech Recognition, Google Cloud Speech API, Wit.ai, Microsoft Bing Voice Recognition, Houndify API, and IBM Speech to Text. These services are seen as important for accelerating digital excellence and can be utilized by the National University of Singapore."
107,"The National University of Singapore has developed an open-source Python library for speech recognition. It can be easily installed using pip and supports various engines such as CMU Sphinx, Google Speech Recognition, and IBM Speech to Text. The library also includes offline options like Snowboy Hotword Detection and OpenAI Whisper. This technology aims to accelerate digital excellence and improve the accuracy and efficiency of speech recognition."
108,"The National University of Singapore has developed an open-source Python speech recognition tool that can recognize speech from a microphone, transcribe audio files, and save audio data to a file. The tool also has the ability to calibrate the energy threshold for ambient noise levels and listen to a microphone in the background. The tool can be accessed through a website."
109,"The integration of speech recognition and natural language processing (NLP) solutions has been successfully implemented in various case studies. These include a virtual assistant for a banking company, a voice-activated chatbot for a healthcare organization, and a speech recognition system for a transportation company. These solutions have improved customer service, increased efficiency, and reduced costs. However, challenges such as data privacy and accuracy still need to be addressed. Overall, the success of these case studies shows the potential for further integration of speech recognition and NLP in various industries."
110,"The National University of Singapore is focused on accelerating digital excellence through the integration of speech recognition and natural language processing (NLP). This integration has many possible applications, including voice search, customer service analysis, speech translation, and meeting transcription and summary. Additionally, the university is working on improving punctuation and capitalization restoration and inverse text normalization. These efforts aim to enhance the capabilities of digital technologies and improve user experience."
111,The process of determining speaker turns in an audio recording involves segmenting and clustering the audio to group all segments from the same speaker together. This allows for easier identification and analysis of individual speakers in the recording. The ultimate goal is to accurately determine who spoke when in the recording.
112,"Importance and Applications of Speaker Diarization Speaker diarization is essential for multi-participant broadcasts, meetings, or interviews as it helps understand the conversation structure and each speaker's contribution. It also enhances transcription services by labeling speakers, making them more informative and readable, particularly in contexts where speaker identity is crucial. Additionally, diarization improves voice assistants' performance in environments with multiple speakers by allowing them to distinguish between the primary user's commands and background conversations."
113,"The document discusses key challenges in speech recognition faced by the National University of Singapore's Institute of Systems Science. One of the most difficult aspects is overlapping speech, where multiple speakers talk simultaneously. Traditional methods struggle in these scenarios, and advanced models are needed. Variability in speech, caused by emotions, health conditions, and environmental factors, also poses a challenge. Short speaker turns, with rapid back-and-forth exchanges, make it difficult to gather enough data for accurate speaker identification."
114,"(ADE)

The evaluation of diarization systems is measured by the Diarization Error Rate (DER), which is the sum of three errors: Missed Speech (MISS), False Alarm (FA), and Speaker Error (ERROR). MISS is the percentage of reference speaker speech that is not detected by the system, FA is the percentage of incorrect speech identified by the system, and ERROR is the percentage of reference speaker speech attributed to the wrong speaker. A lower DER indicates better performance."
115,"The document discusses the concept of voice search, which allows users to find information using spoken queries. However, the output from automatic speech recognition (ASR) often needs to be transformed or normalized to match the expected input format of existing text-based search systems. This includes optimizing for product terms such as brand or product names, such as changing ""Real me"" to ""Realme"" or ""Red me"" to ""Redmi"". This is important for ensuring accurate and efficient search results for users."
116,"The National University of Singapore is focused on accelerating digital excellence in customer service. They are implementing quality control measures such as analyzing customer and agent sentiment through text and speech, as well as analyzing voice characteristics like pitch and loudness. They are also monitoring non-talk time, talk speed, and interruptions, although this is still a challenging aspect."
117,"The National University of Singapore is focused on accelerating digital excellence through the use of machine translation and speech translation. These technologies have various applications, including cross-border communication, localization of websites and other digital content, language learning, and allowing users to watch foreign videos in their own language. These advancements are expected to greatly benefit society and facilitate global connectivity."
118,"The article discusses the evolution of machine translation methods from the Cold War era to the present day, with a focus on the advancements in deep learning. It highlights the challenges faced in machine translation, such as language complexity and cultural nuances, and how traditional methods were limited in their capabilities. The introduction of neural networks and deep learning has greatly improved machine translation accuracy and efficiency, with the use of large datasets and advanced algorithms. However, there are still challenges to be overcome, such as the need for more data and the potential for bias in the training process. The article concludes by emphasizing the importance of continued research and development in machine translation to achieve digital excellence."
119,"The document discusses the evolution of machine translation methods and the role of digital technology in accelerating this process. The first phase of machine translation involved rule-based methods, which relied on linguistic rules and dictionaries to translate text. This was followed by statistical methods, which used large amounts of bilingual data for translation. The current phase is focused on neural machine translation, which uses artificial neural networks to improve translation accuracy. The document emphasizes the importance of digital technology in driving this evolution and enabling faster and more accurate translation methods. It also highlights the potential for further advancements in machine translation in the future."
120,"The National University of Singapore is committed to accelerating digital excellence and has established evaluation metrics for measuring machine translation (MT) performance. One such metric is the Bilingual Evaluation Understudy (BLEU) Score, which compares n-grams of machine-translated sentences to those of human-translated sentences. Higher scores indicate better MT performance, but these scores may decrease as sentence lengths increase."
121,"The document discusses the challenges and potential errors that can occur in Machine Translation (MT). These include the generation of profanity, violent or inciting content, and mistranslation of proper names. While steps can be taken to eliminate offensive words, offensive language is not limited to specific words. It is important to address these issues in order to achieve digital excellence in MT."
122,"The article discusses the practical challenges in speech translation, such as misrecognized words, out-of-vocabulary words, and domain-specific terms leading to inaccurate or omitted translations. Low-resource languages also present a challenge as MT systems for these languages often have poor accuracy. In addition, hesitations, false starts, and filler words in ASR output can result in awkward or inaccurate MT output. Inaccurate segmentation can also affect translation quality. Finally, the need for low latency in live or near-live translation can conflict with the need for high accuracy."
123,"The document discusses the importance of digital excellence in today's world and the need to accelerate it. It specifically mentions the challenges of live or near-live translation, which includes real-time translation, readable subtitles, and syncing dubbing with the speaker's mouth movements. The speaker then introduces the topic of their speech, which is about energy and climate."
124,The document discusses the importance of accelerating digital excellence and mentions a study by R. Zhang et al on dynamic sentence boundary detection for simultaneous translation. This technology allows for real-time translation to start while a sentence is being spoken. This can greatly improve the efficiency and accuracy of translation processes.
125,"The National University of Singapore is aiming to accelerate digital excellence by implementing real-time translation technology. This technology will allow for translation to begin as soon as a sentence is spoken, rather than waiting for the speaker to finish. This approach, known as the Wait-info Policy, balances the source and target information for simultaneous machine translation. This technology is expected to be implemented by 2022."
126,"The article discusses popular methods for simultaneous speech translation, including re-translation, wait-K, and AED models. Re-translation involves translating partial ASR results from the beginning, but it is costly and unstable. Wait-K involves waiting for a certain number of words before translating, but it is not very flexible. AED models are used for end-to-end speech translation, but streaming AED is still a challenge."
127,"The article discusses popular methods for simultaneous speech translation, including re-translation, wait-K, and AED models. Re-translation involves re-translating partial ASR results from the beginning, but it is costly and unstable. Wait-K involves waiting for K words before translating ASR results, but it has limitations in flexibility and K is predetermined. AED models for end-to-end speech translation are being developed, but streaming AED remains a challenge."
128,"The Singapore 1 Streaming Multilingual Speech Model (SM2) is a small, device-friendly model that combines speech translation (ST) and automatic speech recognition (ASR) functions. It is trained using multilingual data without the use of any human-labeled parallel corpus, making it a weakly supervised model. This allows for truly zero-shot capability, meaning it can handle new languages without any prior training. The model was developed by Xue et al. at the National University of Singapore as part of their efforts to accelerate digital excellence."
129,"The document discusses the challenges of handling context across multiple sentences in translation. It highlights the difficulty in accurately translating pronouns, as they may refer to different antecedents in different sentences. This requires advanced technology and techniques to accurately interpret and translate the intended meaning. The National University of Singapore is committed to accelerating digital excellence in this area."
130,"The document discusses the National University of Singapore's (NUS) Institute of Systems Science (ISS) and their goal to accelerate digital excellence by 2025. They aim to do this by providing high-quality education and training in digital technologies, promoting research and innovation, and fostering collaborations with industry and other institutions. The ISS also plans to enhance their curriculum and programs to keep up with the rapidly evolving digital landscape and to equip their students with the necessary skills and knowledge. They also highlight the importance of partnerships and collaborations in achieving their goal of digital excellence."
131,", METEOR, and CIDEr

The document discusses the importance of digital excellence and how it can be accelerated through the use of evaluation metrics for summarization. These metrics, including BLEU, ROUGE, METEOR, and CIDEr, can measure the quality of a summary by comparing it to a reference summary. They focus on aspects such as word overlap, sentence structure, and semantic similarity. These metrics are crucial for evaluating and improving summarization algorithms, which are essential for efficiently processing and understanding large amounts of digital information. By utilizing these metrics, organizations can strive towards achieving digital excellence and staying competitive in the digital age."
132,"The document discusses practical challenges in meeting summarization, including misrecognized words, domain-specific terms, hesitations and filler words in ASR output, identifying and differentiating speakers, and proper co-reference resolution. These challenges can lead to inaccurate summaries and attributions in business, medical, or legal meetings. Additionally, different expectations of what is considered important in a meeting can also affect the accuracy of the summary."
133,"The LLM (Language Model) can be converted into an automatic speech recognition system by adding audial embeddings to the text token embeddings. This allows it to perform speech recognition tasks in the same way as its textual counterpart. Incorporating a conformer encoder into the open sourced LLaMA-7B model has shown to improve its performance by 18% compared to monolingual baselines, and it is also able to perform multilingual speech recognition despite being trained mostly on English text. This research was conducted by Yassir Fathullah and his team at the National University of Singapore."
134,"The figure shows the audio encoder architecture used in the Speech LLM BA NUS model. The initial conformer is trained on a CTC loss, and the outputs are stacked and projected to the dimension of the LLM. This allows for a stacking factor of 3, resulting in 240ms embeddings. The model, developed by Yassir Fathullah and his team at the National University of Singapore, aims to combine large language models with speech recognition abilities to accelerate digital excellence."
135,"The model architecture in Figure 2 shows the combination of an audio encoder and text embedding matrix, which is then fed into a decoder-only Large Language Model (LLM). The LLM is responsible for predicting the next token and can be frozen, adapted, or fully finetuned. The study will focus on exploring the use of LoRA and other parameter efficient approaches to adapt the LLM. The goal is to prompt large language models with speech recognition abilities, as part of the National University of Singapore's efforts to accelerate digital excellence."
136,"AudioPaLM is a large language model that combines text-based and speech-based models to create a unified multimodal architecture. It has the ability to process and generate both text and speech, making it useful for applications such as speech recognition and speech-to-speech translation. The model has shown significant improvements in performance compared to existing systems for speech translation tasks and can also perform zero-shot speech-to-text translation for many languages that were not included in its training. This technology is being developed at the National University of Singapore to accelerate digital excellence."
137,"The NUS of Singapore has developed a pre-trained language model called AudioPaLM, which is trained on text-only data. This model is capable of translating between French and English using audio and text inputs, and also has the ability to perform automatic speech recognition for Italian. It utilizes tokenizers and a text detokenizer to process the input data. This technology is expected to accelerate digital excellence."
138,"When considering infrastructure for production, it is important to find a solution that fits within your budget. Key performance metrics to consider include Token Per Second (TPS), Time To First Token (TTFT), and GPU usage. The recommended starting point for GPU is the T4 16GB, which works well for 3B or 8B models. It is also beneficial to consider quantization and distillation of models, which involves using smaller models for faster performance. Additionally, using small models with few-shot prompting can also improve efficiency."
139,"The National University of Singapore's Institute of Systems Science (NUS-ISS) is committed to accelerating digital excellence in Singapore. They have identified six key areas for this acceleration, including digital government, digital economy, digital society, digital workforce, digital innovation, and digital infrastructure. To achieve this, NUS-ISS is leveraging the Gartner Hype Cycle, a tool that helps organizations identify emerging technologies and their potential impact. They aim to use this tool to guide their research and development efforts and help organizations in Singapore stay ahead in the digital landscape."
140,"The NUS Institute of Systems Science (ISS) has an active presence on social media platforms such as Facebook, Instagram, LinkedIn, and YouTube. They provide updates and information about their programs and activities through these channels. Interested individuals can also visit their official website at www.iss.nus.edu.sg for more information."
