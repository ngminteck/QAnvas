Page,Summary
Page 1,2022 NUS. All rights reserved. Dr Wang Aobo aoba.wang@nus.edu.sg .
Page 2,Page 2 Agenda • Statistical Modelling vs. Deep Neural Nets • Workshop: Basic NN on Colab . Workshop: Word2Vec & DL Specific – CBOW and SkipGram
Page 3,the whole task here is... Lost glamor . ML + CPU Supervised Unsupervised Unsupervised ...
Page 4,"tfidf and PPMI vectors are – long (|V| > 100,000) – sparse (lots of zero) ... 4.23 0.00 8.90 sent1 0.01 0.00 0."
Page 5,"All models are wrong, but some are useful . SVM/KNN/Decision Tree/RandomForest/Nave Bayes/MaxEnt – for Sequence Labeling ."
Page 6,"the whole task here is... Lost glamor Rated 5 by hotogama on Feb 23, 2013 We had afternoon tea in the tiffin room at Raffles!"
Page 7,"the staff make this hotel a little bit of a history, great location with a bit of history . great service ... amazing 1 0 0 service 0 1 1 lost 0-1 0 glamour ..."
Page 8,"deep learning can create feature vectors that are – short (often fixed-sized 2000, decided empirically) – dense (most are non-zeros) and able to capture contextual dependency ."
Page 9,Page 9 Agenda • Workshop: Deep Learning from Scratch . Workshop: Word2Vec & DL Specific – CBOW and SkipGram .
Page 10,"Page 10 Deep Learning Basics . ""system that depends on probabilistic rather than deterministic principles . for its operation, gains reliability from the properties of statistical measurements ."
Page 11,"given a set of inputs x, perceptron – learns w vector to map the inputs to a real-value output between [0,1] – through the summation of the do"
Page 12,Hate x2 Good x3 Enjoy x4 Bad x5 Like 1 0 0 1 1 Bad 0 2 1 1 1 2 1 2 2 2 3 3 4 4 4 0 5 0 4
Page 13,Hate x2 Good x3 Enjoy x4 Bad x5 Like x1 0 0 1 0 2 0 Bad w1 w2 w3 w4 w5 w
Page 14,hate x2 Good x3 enjoy x4 bad x5 hate 0 1 0 0 good 0 2 0 bad 0 3 0 Good 0 5 0 Bad 0 6 6 6
Page 15,"w26 s f(s) 1|0 (1,26) (1,1) (1,2) (1,3) (1,4) (1,5) (1,6) (1,7) (1,8) (1,9) (26,"
Page 16,"wb26 wp26 s f(s) s (s) (1,26) (26,2) (wg1 wg2 1|0 (2,1) (1,1)"
Page 17,"wb26 wp26 s f(s) 1|0 (1,26) (26,2) weight wg1 ,2 ,3 . (1,1) weight . w w"
Page 18,Page 20 Deep leaning Training Routine Repeat the following until desired . Page 20 NUS. All rights reserved.
Page 19,Page 21 Deep leaning Training Routine “brute force” searching action with strategy until desired . Compute and keep the Cost/Loss – Define and compute the loss .
Page 20,"yy = wwww with n=5 training examples (guessing) y=3x y_true ypred – ytrue = 0 0 0, 1 3 2 1 2"
Page 21,2022 NUS. All rights reserved. Page 23 Apply Delta rule wwnnnww =ww2 4ww + 6 ddLL .
Page 22,"wb26 wp26 (1,26) (26,2) weight loss (ypred , Ycorrect) = x * bp * w* wg - 1 WWWWW"
Page 23,2022 NUS. All rights reserved. Page 27 Deep leaning Training Routine Repeat the following until desired .
Page 24,Workshop: NN Basics on Colab . Workshop: Word2Vec & DL Specifics – CBOW and SkipGram .
Page 25,2022 NUS. All rights reserved. Dr Wang Aobo aoba.wang@nus.edu.sg.
Page 26,Page 30 Features from WordToVec . Feature generation and selection could be tedious . how might we generate “universal” features automatically?
Page 27,Page 31 Features from Word Vectors – Count from Data – Word Co-occurrence + SVD . Learn from Data: GLOVE: Global Vectors for Word Representation .
Page 28,2022 NUS. All rights reserved. Page 32 32 Count from Data . Sent_2: I like deep learning Sent-3: I enjoy flying Window_size=1: .
Page 29,Page 33 Count From Data S1 S2 I 1.5 .1 like 3.14 .23 enjoy 2.7 -.98 Deep .55 .2 learning .8 2.5 NLP -2.5 3 flying 4.5
Page 30,Page 34 Count From Data S1 S2 I 1.5 .1 like 3.14 .23 enjoy 2.7 -.98 learning .8 2.5 NLP -2.5 3 flying 4.5 4.9 34 Sorted S
Page 31,Page 35 Features from WordToVec – Count from Data – Word Co-occurrence + SVD . NN Methods – Predictive Model .
Page 32,Page 36 36 GLOVE-Global Vectors for Word Representation . Counting context-words within a window_size = 1 (# of “like” as “I’s” context- words) =
Page 33,"GLOVE-Global Vectors for Word Representation . Count(# of “like” as “I’s” context-words) = CI,like / CI = 2/3 log (P ("
Page 34,2022 NUS. All rights reserved . GLOVE-Global Vectors for Word Representation .
Page 35,"vj = the vector representing “like” j refers to “like"" Thus we Define: Least Square Loss Function : L = ij [ log(Cij) - (vi"
Page 36,"weighted least square Loss Function : L = i,j [log(Cij) - (vi vj + vbias)]2 Weight_Func("
Page 37,weight_func(Cij) should have a upper bound as Cij can be a big number 41 GLOVE-Global Vectors for Word Representation .
Page 38,2022 NUS. All rights reserved . GLOVE-Global Vectors for Word Representation .
Page 39,NN Methods – Predictive Model – Count and Learn from Data . GLOVE: Global Vectors for Word Representation - Count + SGD .
Page 40,page 44 One-Hot Encoding (Sparse Representation) 44 give she at talk have ramen a drink have 0 0 1 0 2 0 drink 0 3 0 4 0
Page 41,Page 45 Lookup Function 45 0.10.2 -0.4 0.9 0.2 0.1 -0.3 0.9 0.2% 0.3 -2.0 0.5 -0.5 0.2 0.3% -3.0 0 0.5% -
Page 42,the bulk of linguistic questions concern the distinction between a and m . learn the matrix through “classification” task window_size = 2 .
Page 43,iterate through every word with a given window; learn W such the models can predict what’s the word given only the context words as inputs .
Page 44,"0 0. . . 0. 0.1 . 0,4 . 1,0 . 1 . 2*window_size,V) talk Re-shaping the values . (1,V) ."
Page 45,Page 49 Nave Sentence Embedding 49 v(‘give’) 0.5 -1.3 0.6 1.1 v (‘at’)
Page 46,"0 0. . . 0. 0.1 . 0,4 . 1,010 . 1 . 2*window_size,V) talk Re-shaping the values . (1,V) give"
Page 47,"page 51 Activation Function – Smoother decision function is expected – Activations bound in (0,1) – Support backpropagation – Need to be non-linear."
Page 48,Page 52 Activation Function – smooth output between 0 and 1 – interpreted as a probability of “Yes”
Page 49,Page 53 Activation Function - smooth output between -1 and 1 – 0 centroid . interpreted as a set of “weights”
Page 50,Page 54 Activation Function (Sigmoid) vanishing gradients . all rights reserved. 2022 NUS.
Page 51,"Page 55 Activation Function (ReLU) – Smoother decision function is expected . All you need is – Simple – Fast A(x) = max(0,x) vanishing gradient ."
Page 52,Page 56 Activation Function (ReLU): ReLU’s Family . ReLU's Family is a family-owned business .
Page 53,"give a at the 0.4 . 0.6 . 0.0.9 .0.2 (2*window_size,V) Softmax () talk ."
Page 54,Page 58 Last Layer Activation Function - SoftMax . All rights reserved . Page 57 Last layer . xi as Input Never be a single x .
Page 55,Sigmoid function – smooth output between 0 and 1 – interpreted as a probability of “Yes” (Image from Wikipedia)
Page 56,"u Embeddings AVG () + Relu () 0.5 . . (1,V) . 1 . 2 . 4 .2 0 1 0 0 0. ."
Page 57,u Embeddings AVG () + Relu () .1 . 2 .4 .2 0 1 0 0 Talk - 0*log(0.1) –
Page 58,u Embeddings AVG () + Relu () .1 . .2 .4 . 2 0 1 0 0 Softmax () talk loss=CE 0.17
Page 59,0 0. . . 1 - 0 - 1-1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 - 9 - 10 - 15 - 20
Page 60,uAVG () + Relu () = WW11(nnnww) = lloooooo ww22 .
Page 61,Page 65 Backpropagation SGD vs Adam Faster but sometimes not converging Well generalised but slower Used together with momentum .
Page 62,Page 66 Backpropagation • Learning Rate based on Experience • Vanilla model: MLP/RNN/CNN 10-2 10-3
Page 63,Fixed Learning Rate: lr == as iterations going up save and plot the loss as per iteration going up . Page 67 Backpropagation .
Page 64,Page 68 Backpropagation. All rights reserved. Dynamic Learning Rate – Estimate Initial Learning Rate.
Page 65,"0 W1 (V,N) 0.1 . 0.3 . 0.0.8 N AVG () + Relu () 0,3 . 0,43 ., 0.75 ."
Page 66,2022 NUS. All rights reserved. Page 70 CBOW Model Summary . Page 69 CBOOW model summary .
Page 67,iterate through each word with a given window; predict the context words within the window . page 71 Word2Vec (Skipgram)
Page 68,"language users never choose words randomly . 'language' is essentially non-random . ('words', 'choose' and 'and')"
Page 69,"language users never choose words randomly . 'never', 'is' is essentially non-random ."
Page 70,"language (1) users (2) never (3) choose (4) words (5) randomly (6),(7) and (8) language (1) is essentially (11) non-random (12)"
Page 71,2022 NUS. All rights reserved. Page 75 Skipgram Model: 'skipgram'
Page 72,different contexts lead to different embeddings . stackoverflow is a great website for programmers . large context window: more semantics related .
Page 73,Page 77 Properties of Word Embeddings Ingredients Corpus of text As large as possible Annotations 0 Initialize weights 1x per word Deep Learning Model 1x Cost Function Appropriately GPU Lotsa
Page 74,"Page 78 When to use pre-trained embeddings? Generally, when you don’t have much training/annotated data . Use as inputs to model for classification task ."
Page 75,Page 79 Embedding Bias: Page 78 . Page 81 . page 79: Page 80: Page 90: Page 95: Page 99: Page 100: Page 110: Page 120: Page 130:
Page 76,Neither GloVe nor Word2Vec has been shown to provide definitively better results rather they should both be evaluated for a given dataset .
Page 77,Page 81 Train Word2Vec from “scratch” CBOW AND SKIPGRAM . All rights reserved.
Page 78,"""deep contextualized word representations."" arXiv preprint:1802.05365 (2018). ""Attention is all you need."" Advances in neural information processing systems. 2017. 6. Howard, Jeremy, and Sebastian"
Overall Summary,neural networks are based on a set of vectors called 'global vectors' . the vectors can be used to predict a word's context . nnp is a
