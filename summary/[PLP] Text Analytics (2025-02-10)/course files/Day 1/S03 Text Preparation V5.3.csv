Page,Summary
Page 1," The contents contained in this document may not be reproduced in any form or by any . without the written permission of ISS, NUS, other than for the purpose for which it has been supplied . © 2015-2025 NUS."
Page 2, The term-Document Matrix was created by NUS in 2015 . It is based on a term-document matrix . The Matrix Matrix is designed to look at the complexity of the task .
Page 3," The Long Bar used to be on the ground floor and was patronised by many ex-pats drinking in the colonial atmosphere of the bar . Now it is on the first floor (so if you  didn’t know,"
Page 4, The 2015-2025 NUS document is published by NUS at the University of Cambridge . It is based on the work of NUS President Barack Obama and Barack Obama . The NUS has published a version of this article on the
Page 5, The TA tools provide functionality  of importing text from some  common formats . They can also be used to delete special characters and label different zones . The tools are available in the UK and Australia .
Page 6," Great location with a little bit of history,  the staff make this hotel though . Have a drink in the Long Bar, throw your nutshells on the floor, then go to the Tiffin Room for the best curry"
Page 7," With XML, we can insert tags onto a text to identify its parts . Eg. <DOC>. <SUBJECT>. <TOPIC> and <TEXT– Such tags are very useful as they allow selection/extraction of the"
Page 8, The Tiffin Room has the 'best curry in the world' at £40 a head for food . The staff make this hotel 'absolutely amazing' and 'amazing'
Page 9, JavaScript Object Notation (JSON) is a standard text-based format for  representing structured data based on JavaScript object syntax . Uses human-readable text to store and transmit data objects consisting of  attribute-value pairs and arrays
Page 10," The Tiffin Room has the best curry in the world, and the staff are 'amazingly welcoming' The hotel is a great location with a little bit of history ."
Page 11, Vectorization – Term Document Matrix – is a term for a Document Matrix . NUS has been awarded the title of 2015-2025 NUS for the first time .
Page 12," Term Document Matrix (TDM) uses terms (keywords) as features for  the vectors to represent documents . Common pre-processing steps (for English, may be different for other languages) include Tokenization, Case lowering, St"
Page 13," To break a stream of characters into smaller units called tokens, which  can be words, subwords, or characters, depending on the methods used . For TDM, word tokenization is used – unigram model – every token"
Page 14, The U.S.A. is the only country in the world to use the U.N. system of punctuation and punctuation punctuation .
Page 15," A large vocabulary size can capture more specific information, but increases memory usage and computational cost . Unknown words are also known as Out-of-Vocabulary words or OOV words . Modern models deal with unknown words using subword "
Page 16," Also known as case normalization,  to convert all tokens to lower case . May cause issue in some contexts . E.g., “Apple” -> “apple”, “US” to “"
Page 17, A word may come in varied forms and therefore need to be converted into a standard form . Stemming can reduce the number of distinct features in a text .
Page 18," Stemmers are some well-known stemming algorithms for English . They include single pass, longest-match and reforming the stem through recoding transformations ."
Page 19," An inflectional stemmer needs to be partly rule-based and partly dictionary-based . Derivational stemming is more aggressive and therefore can reduce the  number of features in a corpus drastically . However, meaning might be lost in"
Page 20," Some words are extremely common and carry little meaning . They are of limited  use in text analytics applications . Functional words (conjunctions, prepositions, determiners, or pronouns) like the, of, to"
Page 21," To support the stopword removal step in preprocessing, a stopword list is a list of very common words like preposition, conjunction, etc. A list of words that are unimportant for the mining task is also a filter dictionary"
Page 22, NUS has removed Punctuation removal and number removal . The site has been updated to reflect the changes made in this article . The article was published by NUS in 2015 .
Page 23," Using “bag-of-words” approach, create a vector representation of documents . Vector features: terms/keywords, usually only content words (adjectives, adverbs, nouns, and verbs)"
Page 24, Binary digits indicate whether a word has occurred in the document . Frequency-based terms are used to contrast with other documents . The term frequency is based on the frequency of words in a document .
Page 25," With frequency-based TDM, a list of words and their frequencies in the corpus can be generated . This list, sorted by frequency, can give us a rough idea of what the corpus is about ."
Page 26, Word Cloud: another example . Generated from http://worditout.com/word-cloud/make-a-new-one .
Page 27, NUS: “What do you like most…” and ‘What do we like least…’ The survey is based on a recent survey by NUS.
Page 28," To deal with varied document length, since a long document  has more occurrences of terms than a short  document . To modify the frequency of a word in a document by the perceived ipient importance of the word, the word"
Page 29," The number of documents in the corpus contains the document frequency of term t, i.e., the number of . documents that contain  the term . N: N : N: The total number of document documentaries in the"
Page 30," In this example, stopwords and very common words are not removed, and terms are not reduced to root terms ."
Page 31," The term document matrix is sparse, expected to have most of the values                 to be zero . It saves memory to store the matrix as a set of sparse vectors, where a row is represented by a list of pairs ."
Page 32, Vectorization – Embeddings – Embeddings . Vectorization is a form of programming that can be used to communicate with other users .
Page 33, A dense representation of words as vectors of real numbers in a continuous vector space with a much lower dimension . Learned by deep neural networks during a prediction task .
Page 34," Using pre-trained word vectors, e.g., GloVe (Global Vectors for Word                 Representation) by Stanford . Lookup each token (word) in the vectors ."
Page 35," Using a pre-trained (large) language model, e.g., BERT, GPT2, etc. Can handle different meanings of the same word with different context ."
Page 36," Given each word in the document is represented as an embedding vector, how to get the vector representation of the whole document? Common approach: summarizing all the word vectors by averaging (or  summiting) them . There are also"
Page 37," Parsing (syntactic and  semantic) is called ""Parsing"" and ""Part of Speech"" (POS) The system is based on the language language of speech ."
Page 38," The study was published by Pearson Education India,   (continuously updated) The NUS has published a number of resources to help students understand how to use the language of speech ."
Overall Summary," The contents contained in this document may not be reproduced in any form or by any means, without the written permission of ISS, NUS, other than for the purpose for which it has been supplied ."
