Page Number,Summary
1,"This document is the property of NUS and cannot be reproduced without written permission. It is part of the Text Analytics Module 3 and is focused on preparing text data for analysis. The author is Dr. Fan Zhenzhen from NUS-ISS, and the document can be contacted via [REDACTED_EMAIL]."
2,"Page 2 provides an overview of the task of text preparation, which involves converting raw text into a format that can be used for analysis. The two main methods of vectorization, or converting text into numerical data, are discussed: the Term-Document Matrix and Embeddings. The Term-Document Matrix represents the frequency of terms in a document, while Embeddings use algorithms to create numerical representations of words and phrases. Both methods are commonly used in natural language processing tasks."
3,"The document discusses the features and uses of a corpus, which is a collection of documents. It mentions that a corpus can be represented as numeric vectors, which can be used for tasks such as document classification and clustering. The document also includes reviews of a hotel in Singapore, with some praising the service and others criticizing the commercialism and tourist atmosphere. The reviews provide examples of how a corpus can be used for sentiment analysis and opinion mining."
4,"Page 4 of the document 'S03 Text Preparation V5.3.pdf' discusses the various sources of text data that can be used for analysis. These include existing business processes, such as customer call feedback, complaints, emails, and employee evaluations, as well as documents such as manuals, contracts, memos, and legal documents. Other sources include web pages, social media data, and various other sources. These sources provide a wealth of information that can be used for text analysis and can greatly benefit businesses."
5,"The document discusses file preprocessing for text data, including common formats such as HTML, Word, Excel, XML, PDF, and TXT. The key tasks involved in preprocessing include deleting formatting tags, removing special characters, detecting and labeling different zones, and determining sentence and paragraph boundaries. Many TA tools have the capability to import text from these formats, including JSON."
6,"The document explains that in a pure text file, the content is represented as plain text without any formatting or graphics. It also includes a review from a customer named travel-gini who gave the hotel a 5-star rating and praised its location, history, and staff. They recommend having a drink in the Long Bar and trying the curry at the Tiffin Room, which they claim is the best in the world. The food is priced at £40 per person, but the selection is excellent. The customer also mentions that the staff provided amazing service and even presented a cake for their wife's birthday. The stay occurred in February 2021."
7,"XML is a widely used standard exchange format in the industry and text-processing community. It allows for the insertion of tags onto a text to identify its parts, such as paragraphs, headings, and sentences. These tags are useful for selecting and extracting specific parts of the text for feature generation in data mining. Many word processors also have the capability to save documents in XML format."
8,"This page discusses the structure of an XML document and provides an example of a review for a hotel. The review highlights the great location and atmosphere of the hotel, as well as the excellent service provided by the staff. The Long Bar and Tiffin Room are also mentioned as must-visit spots, with a recommendation for the delicious curry. The review also mentions the cost of about £40 per person for food and the special birthday surprise for the reviewer's wife. The stay took place in February 2021."
9,JSON is a widely used data format that uses human-readable text to represent structured data in the form of attribute-value pairs and arrays. It is based on JavaScript object syntax and is commonly used in web applications and electronic data interchange.
10,"The example provided is in JSON format and includes a title, rating, date, reviewer, and review content. The review praises the hotel's location and staff, mentioning the unique experience of throwing nutshells on the floor and enjoying a curry at the Tiffin Room. The reviewer also mentions the price and variety of food options, and notes that the staff went above and beyond by presenting a cake for the reviewer's wife's birthday. The stay took place in February 2021."
11,"Page 11 of the document 'S03 Text Preparation V5.3.pdf' discusses vectorization and the creation of a term document matrix. Vectorization is the process of converting text data into a numerical format that can be used for analysis. This is done by creating a term document matrix, which is a table that contains the frequency of each term in a document or corpus. The document also mentions the importance of pre-processing the text data before vectorization, such as removing punctuation and stop words. Additionally, it explains the use of different weighting schemes, such as term frequency-inverse document frequency (TF-IDF), to adjust the importance of terms in the matrix. Overall, vectorization and the term document matrix are essential steps in preparing text"
12,"The Term Document Matrix (TDM) uses keywords to represent documents, and common pre-processing steps for English include tokenization, case lowering, stemming/lemmatization, stopword removal, punctuation removal, and numeric removal. These steps may vary for other languages."
13,"Tokenization is the process of breaking a string of characters into smaller units, such as words, subwords, or characters. In TDM, word tokenization is typically used, where each token is a single word. However, there are also other models, such as bigram and trigram, where tokens are composed of two or three words. An example of word tokenization is ""Great location with a little bit of history,"" which would be broken down into the tokens ""Great,"" ""location,"" ""with,"" ""a,"" ""little,"" ""bit,"" ""of,"" and ""history."""
14,"Tokenization is the process of identifying token delimiters, such as whitespace and punctuation characters, in a text. However, there are challenges in tokenization, as certain characters like periods and commas can have multiple meanings. For example, a period between numbers is part of the number, but it can also be part of an abbreviation or end of a sentence. Similarly, the apostrophe can indicate a possessive or be part of another token. These complexities make tokenization a more nuanced process than it may seem at first."
15,"The vocabulary of a model refers to all the unique tokens it can recognize, which are words that it has encountered in a training corpus. A larger vocabulary size can capture more specific information, but it also increases memory usage and computational cost. Unknown words, also known as Out-of-Vocabulary words or OOV, are words that are not present in the vocabulary but may be encountered by the model. These words are usually handled by replacing them with a special token. Modern models use subword decomposition to deal with unknown words."
16,"Case lowering, also known as case normalization, is the process of converting all tokens to lower case in order to remove variations in words due to case differences. While this can be useful in some contexts, it may cause issues in others, such as changing proper nouns like ""Apple"" and ""US"" to lowercase. For example, the phrase ""Great location with a little bit of history"" could be converted to ""great location with a little bit of history"" through case lowering."
17,"for more details

The process of stemming and lemmatization is used to convert words into a standard form in order to reduce the number of distinct features in a text corpus and increase the frequency of occurrence of individual features. Stemming can be either inflectional, where the part of speech remains the same, or derivational, where the part of speech may change. Lemmatization involves converting words to their base form, such as ""apples"" to ""apple"" and ""eating"" to ""eat"". Other forms of normalization, such as case normalization, may also be used. Refer to page 5 in Essential Linguistics for further information."
18,"and Martin

Stemmers are algorithms used in text preparation to reduce words to their root form, or ""stem"". Some popular English stemming algorithms include the single pass, longest-match method and the removal of the longest suffix while ensuring the stem is at least 3 characters long. The Lovins Stemmer, developed by Julie Beth Lovins in 1968, is widely used and has implementations in various languages. The Porter Stemmer, created by Martin Porter in 1980, is a framework for writing stemming algorithms and has a newer version called the Snowball Stemmer, developed by Porter and Martin."
19,"Stemming, the process of reducing words to their root form, can be done to varying degrees. An inflectional stemmer should use a combination of rules and a dictionary, while derivational stemming is more aggressive and can significantly decrease the number of features in a corpus. However, there is a risk of losing meaning and creating non-legitimate words if the stemming is too aggressive without the support of a dictionary."
20,"Stopwords are common words that appear in most documents and have little meaning in text analytics. These include functional words like conjunctions, prepositions, determiners, and pronouns. A stopword list can be created to remove these words from analysis. Depending on the domain, other words may also need to be added to the list."
21,"The stopword list, also called a filter or exclusion dictionary, is used in text preprocessing to remove common and unimportant words from a text. These words are typically functional words like prepositions and conjunctions, or words that are not relevant to the mining task. An example of a stopword list can be found on the website http://www.ranks.nl/resources/stopwords.html."
22,"The document discusses additional clean-up steps for text preparation, such as removing punctuation and numbers. These steps can help improve the quality of the data for analysis."
23,"The process of TDM indexing involves creating a vector representation of documents using a ""bag-of-words"" approach. This means that only content words, such as adjectives, adverbs, nouns, and verbs, are used as vector features. The weight of each term is also taken into account. This allows for efficient retrieval of relevant documents based on specified keywords."
24,"Term weighting is a technique used in text preparation to assign numerical values to words based on their frequency or occurrence in a document. There are two main methods of term weighting: binary, which assigns a value of 0 or 1 to indicate whether a word appears in a document, and frequency-based, which calculates the frequency of words in a document and provides more detailed information for comparison with other documents. This can be useful in analyzing text data and identifying important or commonly used words."
25,"Page 25 discusses the use of frequency-based term-document matrix (TDM) to generate a list of words and their frequencies in a corpus. This list, sorted by frequency, can provide insight into the main topics or themes of the corpus. The two types of frequency, global and document, are explained as well. A word cloud is mentioned as a visual representation of this information."
26,"The word cloud on page 26 of the document 'S03 Text Preparation V5.3.pdf' was generated using the website http://worditout.com/word-cloud/make-a-new-one. It is an example of how a word cloud can be created, and it is used to visually represent the most frequently used words in a given text. This particular word cloud was created using the phone number [REDACTED_PHONE] and all rights to it are reserved by NUS."
27,"Page 27 of the document 'S03 Text Preparation V5.3.pdf' discusses contrasting two groups by asking them what they like most and least. This technique can be used to gather opinions and preferences from different groups and can be helpful in understanding the differences between them. It is important to ask open-ended questions and avoid leading or biased questions in order to get genuine responses. Overall, this method can provide valuable insights into the contrasting views of different groups."
28,"Page 28 discusses other weighting methods for text preparation, including normalized frequency and tf-idf. Normalized frequency is used to account for differences in document length, as longer documents will naturally have more occurrences of terms. Tf-idf modifies the frequency of a word in a document based on its perceived importance, with words that appear in many documents considered less important and those that appear in few documents considered more important. This method is widely used in information retrieval. The formula for normalizing frequency is also provided."
29,"The tf-idf indexing method is a way to weight terms in a document based on their frequency and the overall frequency of the term in the corpus. This is calculated by multiplying the term frequency in a document by the inverted document frequency, which is the logarithm of the total number of documents divided by the document frequency of the term. This method helps to identify important terms in a document and is commonly used in information retrieval systems."
30,"The example of tf-idf indexing shows that stop words and common words are not removed, and terms are not reduced to root terms. The link provided gives more information on term vectors."
31,"The document matrix used in text preparation is expected to have a lot of zero values, making it sparse. To save memory, it is more efficient to represent the matrix as a set of sparse vectors, where each row is a list of pairs consisting of the column index and its corresponding value. This alternative representation is more memory-efficient and allows for easier manipulation of the matrix."
32,"Page 32 of the document discusses vectorization and embeddings. Vectorization is the process of converting text data into numerical vectors, which can then be used for machine learning tasks. Embeddings are a type of vectorization technique that maps words or phrases to high-dimensional vectors in a continuous space. This allows for the representation of semantic relationships between words. Embeddings can be trained on large amounts of text data and can be used for various natural language processing tasks such as sentiment analysis and text classification."
33,"task

Word embeddings are a type of vectorization method that represents words as vectors in a lower-dimensional continuous space. They are learned by deep neural networks during a prediction task and have been used in various models such as Word2Vec, GloVe, and FastText. The quote ""Thou shall not make a machine in the likeliness of human mind..."" suggests the potential dangers of trying to mimic human thought processes. To use word embeddings, one must look up the embeddings and calculate the prediction task."
34,"Pre-trained word vectors, such as GloVe, can be used to represent words in a text. This allows for capturing semantic and linguistic information of the words. However, a limitation is that each word is assigned only one vector, which may not account for words with multiple meanings."
35,"Contextualized word embeddings are generated by using a pre-trained language model such as BERT or GPT2. These embeddings are better than traditional lookup embeddings because they take into account the context of the word, allowing for different meanings of the same word to be distinguished. This is useful for tasks like information extraction that require word-level representation. An example of contextualized word embeddings in action is the input ""I like shopping"" and the output generated by a pretrained language model."
36,"The document discusses how to represent a document as a vector using embedding vectors for each word. The most common approach is to summarize all the word vectors by averaging or summing them. However, there are also other methods, such as using deep learning models like doc2vec, skip-thought, FastSent, and Sentence-BERT to learn how to represent documents as vectors."
37,"Page 37 discusses various techniques for text preparation, including Part of Speech (POS) tagging, shallow parsing or chunking, Named Entity Recognition, and parsing for both syntactic and semantic purposes. It also touches on file preprocessing and techniques like TDM embedding and vectorization for document-level tasks. These techniques are important for preparing text data for various natural language processing tasks."
38,"This page provides a list of resources for reference in text preparation. These include the continuously updated book ""Speech & language processing"" by Dan Jurafsky, the chapter ""From Textual Information to Numerical Vectors"" from the book ""Fundamentals of Predictive Text Mining"" by Weiss, Indurkhya, and Zhang, and a list of online word cloud generators. These resources can be useful for those working with text data and looking to convert it into numerical vectors for analysis."
