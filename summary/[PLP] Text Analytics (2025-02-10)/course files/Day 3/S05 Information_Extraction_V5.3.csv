Page Number,Summary
1,"The first page of this document introduces Module 5 of the Text Analytics course, which focuses on Information Extraction. It is taught by Dr. Fan Zhenzhen and Dr. Leong Mun Kew from NUS-ISS at the National University of Singapore. The course covers various techniques and tools for extracting information from text data."
2,"This page provides an outline for the Information Extraction (IE) module, including a definition of IE and an overview of how IE systems work. The main components of IE systems are listed, along with the two main methods for IE: rule-based and machine-learning based. The module will also cover practical applications of IE."
3,"Information extraction is the process of automatically extracting relevant information from unstructured or semi-structured data sources such as text documents, emails, and social media posts. It involves identifying and extracting specific pieces of information, such as names, dates, and locations, from these sources and organizing them in a structured format for further analysis. This process can be used in various applications, including data mining, natural language processing, and text analytics. Information extraction techniques include rule-based systems, statistical models, and machine learning algorithms. It is a crucial step in the data processing pipeline and is essential for tasks such as document classification, sentiment analysis, and knowledge discovery."
4,"This page discusses the process of extracting information from words and converting them into concepts. It uses the example of the election of Pope Francis in 2013 to illustrate this process. The key points are that words like ""Cardinal"", ""Jorge Mario Bergoglio"", ""Buenos Aires"", ""Pope"", ""Catholic Church"", ""13 May 2013"", and ""Pope Francis"" all represent different concepts, such as a prince of the Catholic Church, the Cardinal of Buenos Aires, a city in Argentina, the head of the Catholic Church, a religious organization, a date, and the current Pope."
5,"The document discusses the differences between named entities, concepts, and information in the context of information extraction (IE) systems. Named entities are specific nouns or proper nouns that refer to a unique thing. Concepts, on the other hand, are abstract ideas that group multiple instances together. Information refers to words, named entities, and concepts that fulfill a need, such as answering a question. Information is often regular and can be categorized into patterns, such as information about a person (name, age, sex, address, etc.) or a company (name, address, stock symbol, etc.)."
6,"NUS and HUJ are launching a joint PhD degree programme in biomedical science in August 2013. The agreement was signed by NUS Deputy President and Provost Professor Tan Eng Chye and HUJ President Professor Menahem Ben-Sasson, in the presence of Israel's Ambassador to Singapore and other guests. The exercise involves identifying and categorizing concepts mentioned in the text, such as the two universities, the joint degree program, the date of launch, the individuals involved, and the presence of the Israeli ambassador."
7,"Page 7 of the document discusses the difference between information extraction and information retrieval. While information retrieval involves searching for relevant documents that may contain the answer to a question, information extraction involves extracting the specific answer from a document. The example given is asking about the height of the Eiffel Tower, where a traditional search engine would return a list of documents that may contain the answer, but the user would still need to open and read the document to find the answer. Information extraction would directly extract the answer from the document."
8,"5.3

Page 8 of the document 'S05 Information_Extraction_V5.3.pdf' discusses information extraction version 5.3, which is a tool used for extracting relevant information from a large amount of data. This version includes improvements such as better performance, support for multiple languages, and the ability to handle unstructured data. It also mentions the copyright and ownership of the tool by NUS."
9,"5.3

Page 9 of the document 'S05 Information_Extraction_V5.3.pdf' discusses the Question Answering version 5.3, which is a system developed by NUS for extracting information from text. This system uses natural language processing and machine learning techniques to identify and extract relevant information from text documents. It has been trained on a large dataset and is able to accurately answer questions based on the information it has extracted. The system also has the ability to handle different types of questions and can provide explanations for its answers. It is constantly being updated and improved by NUS to enhance its performance."
10,"Information extraction is the process of automatically extracting information from natural language documents, such as facts about entities, events, and relations. This involves populating a structured information source, such as a table, with information from the documents. The documents can be semi-structured, unstructured, or free text."
11,"Page 11 discusses the different types of information that can be extracted from text, including entities, attributes, facts, and events. Entities refer to objects of interest, such as dates, people, or buildings. Attributes are properties of an entity, such as name or height. Facts can be predicates about an entity or relations between multiple entities. Events are abstractions of sequential relationships between entities, such as a terrorist attack or birthday party. These different types of information can be extracted using various techniques and tools in information extraction."
12,"Named entity recognition is the process of identifying and categorizing specific types of proper nouns, such as people, organizations, and locations, in text data. It can also recognize other types of phrases like money, dates, times, and percentages. This is a valuable tool in text analytics as it helps to organize and condense large amounts of text into a more structured format. The UIUC NER system is an example of a tool that can perform named entity recognition."
13,"temporal, spatial, causal, social, physical

Relation extraction is the process of identifying and extracting binary relations between entities, such as child-of, employee-of, part-of, is-a, etc. This is often done to populate a relational database, and is specific to a particular task or domain. Some common categories of relations include temporal, spatial, causal, social, and physical."
14,"The document discusses the concept of relations in information extraction. It explains that relations involve the names of the entities involved and can be expressed as an RDF triple, with a subject, predicate, and object. This allows for structured and organized representation of information."
15,"Event extraction is the process of identifying and extracting events in which specific entities are involved, such as fare increases by airlines or price drops of stocks. This is a domain-specific and task-specific task, and typically involves identifying verbs or noun phrases that represent the events. Event coreference, or identifying events that refer to the same real-world event, is also an important aspect of event extraction."
16,"Page 16 discusses the related tasks of information extraction, specifically extracting temporal expressions and creating event timelines. Temporal expressions refer to when an event occurred and can include days, dates, and times. These expressions are sometimes normalized for consistency. The task of creating an event timeline involves organizing events in a temporal order to show the sequence of events."
17,"Page 17 discusses the use of information extraction in natural language processing and its applications in various fields such as information retrieval, question answering, and text summarization. It explains the process of identifying relevant information from unstructured data and converting it into a structured format. The key techniques used in information extraction, such as rule-based, statistical, and machine learning methods, are also discussed. The document emphasizes the importance of understanding the domain and context in information extraction and the challenges faced in this field. It concludes by highlighting the potential for further advancements in information extraction."
18,"Page 18 discusses the challenges of information extraction, which involves identifying and extracting relevant information from unstructured data sources. The key challenges include dealing with noisy and incomplete data, handling ambiguity and variability in language, and integrating information from multiple sources. The author also emphasizes the importance of domain knowledge and the use of statistical and machine learning techniques in information extraction."
19,"The concept of automatic template filling is discussed in this document, specifically in relation to information extraction. This involves automatically filling in predefined templates with relevant information from a given text. This process can be achieved through various techniques such as rule-based systems, machine learning, and statistical methods. The goal of automatic template filling is to extract structured data from unstructured text, making it easier to analyze and use for various applications. The effectiveness of this approach depends on the accuracy of the extracted information and the quality of the templates used."
20,"The Boston Marathon bombings occurred on April 15, 2013 at 2:49 pm EDT, killing 3 people and injuring 264 others. The bombs, which were pressure cooker bombs, exploded about 13 seconds and 210 yards apart on Boylston Street near the finish line. The Federal Bureau of Investigation (FBI) took over the investigation and released photographs and surveillance video of two suspects on April 18. The suspects were later identified as Chechen brothers Dzhokhar and Tamerlan Tsarnaev. The perpetrators were caught and identified on the same day."
21,"An IE system works by using natural language processing techniques to identify and extract relevant information from unstructured data, such as text documents or web pages. This process involves several steps, including identifying the relevant data, segmenting it into smaller units, and tagging them with specific labels. The system then uses machine learning algorithms to analyze the data and identify patterns and relationships between the different units. This allows the system to extract meaningful information and present it in a structured format, making it easier for users to understand and use."
22,"This section discusses the use of greedy heuristics in information extraction. The first step is to identify nouns, names, and simple named entities using dictionaries or lists. Proper nouns, which start with capitals in the middle of a sentence, are also considered. If these nouns or names are contiguous, they are aggregated to form a single concept. For example, ""Jorge Mario Bergoglio"" and ""Pope Francis"" are aggregated to ""Jorge Mario Bergoglio"" and ""Catholic Church"" to ""Catholic Church"". Additionally, certain rules, such as identifying a concept if it follows a specific structure (e.g. X [title] + Y [name] + ""of"" + Z [place]), can aid in"
23,"The NUS Deputy President and Provost, Professor Tan Eng Chye, and the President of HUJ, Professor Menahem Ben-Sasson, signed a joint degree agreement at NUS in the presence of the Ambassador of Israel to Singapore, Her Excellency Amira Arnon, and approximately 30 guests. A lexical rule to recognize names of universities is provided, and a similar rule is requested to recognize the names of persons mentioned in the paragraph. It is assumed that dictionaries and appropriate parts of speech have been provided."
24,"Page 24 of the document discusses the main components of an IE (Information Extraction) system. These include tokenization, morphological and lexical analysis, syntactic analysis, domain analysis, sentence and word segmenters, POS taggers, shallow and deep parsing, co-reference resolution, and sense disambiguation. While these components are important for a comprehensive IE system, it is noted that many good systems can still work effectively without them."
25,"Tokenization is the process of breaking down a sentence into individual words or units, which is relatively easy in languages written in the Roman script but more challenging in character-based scripts like Chinese and Thai. Morphology, or the structure of words, is also more complex in languages like German and Hebrew compared to English. For example, German has multiple noun declensions, while Thai has a complex system of word composition. Chinese also has its own unique challenges in word segmentation."
26,"Page 26 discusses POS tagging, which is the process of determining the grammatical category of a term. This requires a dictionary that provides word-POS correspondence. POS disambiguation is also important, as 14-15% of words in the vocabulary are ambiguous, leading to 55-67% of word tokens in running text being ambiguous. The baseline approach is to choose the most frequent tag in the training corpus, but rule-based or stochastic methods can also be used, such as the UIUC POS Tagger."
27,"POS tagging is a task that involves assigning parts of speech to words in a sentence. There are two main types of POS taggers: rule-based and stochastic. Rule-based taggers, such as Brill's tagger, use a set of rules to assign tags based on dictionary and morphological rules. Stochastic taggers, such as CLAWS and Viterbi, use Hidden Markov Models and n-gram probabilities to assign tags. A manually tagged corpus is needed to estimate these probabilities. Many machine learning methods have also been used for POS tagging. Stanford's Statistical NLP website offers several free taggers."
28,".

HMM, or Hidden Markov Model, is a probabilistic sequence model that is trained using a tagged corpus. It uses transition probabilities to determine the likelihood of a tag occurring based on the previous tag, as well as observation likelihoods to determine the probability of a tag being associated with a specific word. This allows HMM to effectively label sequences of data."
29,"HMM decoding involves using a Hidden Markov Model (HMM) to compute the probability of different label sequences for a given sequence of words. This allows for the identification of the most likely sequence of labels (states) for the observed words. The HMM decoding process is important for tasks such as information extraction, where the correct labeling of words is crucial for accurate data extraction."
30,"Shallow parsing, also known as chunking, is a technique used to identify phrases in a text, such as noun phrases, verb phrases, and prepositional phrases. It is based on stochastic techniques that use probabilities from an annotated corpus to segment and label the phrases. This method is faster and more robust than full parsing, and is often used as a sequence labeling task. The example sentence ""The cat sat on the mat"" would be labeled as B-NP I-NP B-VP B-PP B-NP I-NP by the UIUC chunker."
31,"Parsing, also known as syntactic analysis, is a more advanced form of text processing that produces a tree-like structure of a sentence with syntactic functions for each word, such as subject or object. While it is a more costly process, it can provide information that shallow parsing cannot, as seen in the example of Johnson being replaced at XYZ Corp by Smith."
32,": (ROOT (S (NP (DT the) (JJ quick) (JJ brown) (NN fox)) (VP (VBZ jumps) (PP (IN over) (NP (DT the) (JJ lazy) (NN dog)))))


The document discusses trees and their representation in bracketed form. It mentions the use of the Stanford Parser and provides an example of a tree with a subject and verb phrase, as well as prepositional phrases. The example shows how the tree structure can be used to represent the relationship between words in a sentence."
33,Page 33 discusses the concept of dependencies in information extraction. These dependencies are typically shown in a visual format and are referred to as typed dependencies. The document mentions the Stanford Parser as a tool commonly used to visualize these dependencies.
34,The process of semantic analysis involves using parsing results to identify relevant entities for text mining. This is done using the UIUC Semantic Role Labeling system. This allows for a more accurate and efficient text mining process.
35,"The challenges in parsing include robustness, disambiguation, and efficiency. Robustness refers to the ability to handle unexpected or ill-formed input and still extract meaningful information. Disambiguation is needed to handle the potential for multiple interpretations of the input, which can result in a large number of possible parses. To address this, the n best analyses can be returned to the next level of processing. Efficiency is also a concern, as most formalisms have a polynomial time complexity."
36,"Word sense disambiguation is the process of determining the correct meaning of a word that has multiple possible meanings. This is important because words can be ambiguous, or have more than one meaning, which can lead to confusion in natural language processing tasks. However, disambiguation is a difficult problem to solve, in part because there is a lack of training data for machine learning algorithms to learn from. As a result, it is not commonly used in text analytics applications."
37,"WordNet is a comprehensive database of English language words, created and updated by Princeton University. It categorizes nouns, verbs, adjectives, and adverbs into groups of related words called synsets, which represent distinct concepts. The database is widely used in natural language processing and contains a large number of words. Statistics from the WordNet website can be found at http://wordnet.princeton.edu/wordnet/man/wnstats.7WN.html."
38,"WordNet is a lexical database that groups words into sets of synonyms, known as synsets, and links them through conceptual-semantic and lexical relations. These relations include synonymy, antonymy, morphological relations, hyponymy, and meronymy. Synonymy refers to words with similar meanings, while antonymy refers to words with opposite meanings. Morphological relations involve words with the same root or form. Hyponymy refers to the relationship between a general term and its specific instances, such as apple and fruit. Meronymy refers to the relationship between a whole and its parts, such as leg and chair. WordNet also includes other types of semantic relations."
39,"WordNet is a lexical database that organizes nouns into a hierarchical structure based on their semantic relationships. This allows for efficient extraction of information and knowledge from text, as it provides a comprehensive understanding of the meanings and relationships between words. The structure of WordNet is based on inheritance, where more general concepts are inherited by more specific concepts. This allows for the creation of a large network of interrelated nouns, providing a rich source of information for natural language processing tasks such as information extraction. Additionally, WordNet is constantly updated and maintained, ensuring its relevance and accuracy for various applications. 

WordNet is a lexical database that categorizes nouns into a hierarchical structure based on semantic relationships. This makes it a valuable tool for extracting information and understanding the"
40,"Page 40 discusses WordNet, a lexical database for the English language. It provides an example of how WordNet categorizes words, using the word ""happy"" as an example. The expanded view shows the different senses and synonyms of ""happy"" as well as its relationships with other words. This demonstrates how WordNet can be used for information extraction and analysis."
41,"WordNet is a free and open source resource that has been proven useful for various Natural Language Processing tasks, including word sense disambiguation, measuring word semantic distance, information retrieval, question-answering systems, machine translation, and document structuring and categorization."
42,"Co-reference resolution is the process of determining the relationship between related entities. This can include identifying entities with the same referent, such as pronouns, proper names, and definite descriptions. It can also involve determining whole-part relationships, such as the morning star and evening star being the same entity. Co-reference resolution is important for understanding and analyzing text."
43,"The article discusses the formation of La Jolla Genomatics by former UCSD Business School Dean Fletcher Maddox and his two sons. The company will release its product, Geninfo, in June 1999, which is a system to help biotechnology researchers keep up with the vast amount of literature in their field. Dr. Maddox will serve as the CEO, while his son Oliver, who holds patents for the algorithms used in Geninfo, will be the Chief Scientist. His other son, Ambrose, will be the CFO and the company will be headquartered in the Maddox family's hometown of La Jolla, CA."
44,"Page 44 of the document 'S05 Information_Extraction_V5.3.pdf' discusses examples of coreference, which is the relationship between different expressions that refer to the same entity. Anaphora, which refers to the use of pronouns to refer back to a previously mentioned entity, is illustrated through the examples of an elephant stepping on a rabbit and a landmine. Proper nouns, such as John Smith and Mary Brown, can also serve as coreferential expressions, as seen in the example of a wedding. Definite descriptions, like Usain Bolt, can also be used to refer to an entity, as shown in the example of him winning an Olympic gold medal. To accurately identify coreference, one needs to understand the context"
45,"Page 45 discusses different approaches to building Information Extraction (IE) systems. These include rule-based systems, statistical methods, and machine learning techniques. Rule-based systems use predefined rules to extract information from text, while statistical methods rely on probabilistic models and large amounts of data to identify patterns. Machine learning techniques involve training algorithms on data to automatically extract information. The choice of approach depends on the type of information to be extracted and the available resources. A combination of these approaches can also be used to improve the accuracy of IE systems."
46,"The main approaches for information extraction are rule-based systems, induced rules, and hybrid systems. Rule-based systems involve hand-coding rules by linguists with domain input, and the process is slow but yields good results. Induced rules use fully machine learning techniques and are derived from an annotated corpus. Hybrid systems combine machine learning and rule-based approaches to fine-tune the rules."
47,"The main approach for information extraction is through machine learning-based systems, which use a well-annotated corpus to derive a model that generates examples. This method has advantages over rule-based systems, such as being language independent and not requiring linguistic or domain knowledge in the team. However, it also has issues, such as a high complexity in the corpus and requiring a large number of training examples to attain good results."
48,"The document discusses rule-based information extraction (IE) methods, which involve manually creating rules to extract relevant information from text. These rules are based on patterns and regular expressions, and can be customized for specific domains or tasks. Rule-based IE methods have the advantage of being interpretable and easily modifiable, but they require a lot of effort and expertise to create and maintain. They also have limitations in handling complex or ambiguous data. However, they can be combined with other IE methods for better performance.

The document outlines the concept of rule-based information extraction (IE) methods, which involve creating rules based on patterns and regular expressions to extract relevant information from text. These rules can be customized for specific domains or tasks, but require a lot of effort and"
49,"Rule-based systems are effective in situations where the corpus is relatively stable and there is a limited domain. They can be faster than annotating training examples. A rule-based system consists of a set of rules and policies that determine when and how the rules are applied, such as the order and looping."
50,"This page discusses the structure of a rule used in information extraction. The rule consists of two parts: a pattern to match and an action to perform. The pattern is used to identify specific information in a text, while the action specifies what should be done with that information. This structure allows for the extraction of relevant information from a large amount of text."
51,"Page 51 discusses the types of recognition rules used in Information Extraction (IE). The first type is lexical pattern matching, which involves identifying whole entity patterns using a classic rule of left context, filler, and right context. This approach treats entities as independent and is commonly used in hand-coded rules."
52,"This page discusses an example rule from CoreNLP, a natural language processing tool. The rule uses TokensRegex to recognize company names, using patterns for the beginning and ending of a company name. The rule specifies that if a word with a capital letter followed by letters appears, and is tagged as a proper noun, and is followed by a word ending in ""Corp"" or ""Inc,"" it should be annotated as a company and the result should be labeled as ""COMPANY_RESULT."" This rule can be used to extract company names from text."
53,"Regular expressions, or regex, are useful for extracting specific concepts such as currency, dates, email addresses, and phone numbers. Operators can be used to define strings with certain characters or patterns. These patterns can be defined in a compact way, making it easier to extract the desired information. For example, the regular expression for email addresses is [a-zA-Z0-9._-]+@([a-zA-Z0-9.-]+\.)+[a-zA-Z]{2,4}, which can be used to extract email addresses from a text."
54,"The document discusses common operators used in information extraction, which are special characters that define character patterns. The period operator matches any single character, while the caret and dollar sign operators match the empty string at the beginning and end of a line or string, respectively. The backslash operators match specific types of characters, such as digits or alphanumeric characters. The backslash-b operator is frequently used to indicate a word boundary. The pipe operator separates two alternative values, and parentheses are used to group parts of a search expression together. These operators are useful for finding specific patterns in text."
55,"The document discusses common operators used in information extraction, including ? for matching a character 0 or 1 time, * for zero or more of the preceding character, + for matching 1 or more times, and [] for matching any character within the brackets. It also mentions the use of ^ to exclude characters, {n} to match a character n times, and {n,m} to match a character at least n times but not more than m times. Examples of how these operators can be used are provided."
56,"The document discusses regular expressions and provides an example of an email pattern regex. It explains that the regex includes one or more characters, followed by '@' and then one or more characters followed by a period. This pattern is repeated one or more times, with the final repetition being 2 to 4 characters."
57,"Page 57 discusses machine learning based information extraction methods. These methods use algorithms to automatically extract information from text data, such as identifying named entities and their relationships. They are more flexible than rule-based methods and can handle a variety of data sources. However, they require a large amount of training data and may not perform well on unseen data. Some common techniques used in machine learning based IE include supervised learning, semi-supervised learning, and deep learning. These methods have been applied successfully in various domains, such as biomedical and financial information extraction."
58,"ML based systems should be used when better performance is desired, as they are state-of-the-art when trained on large corpora. These systems require well-annotated corpora with multiple features, and various ML techniques can be used, from simple to advanced. They are especially useful when there are no domain or language resources available in a specific area. However, it should be noted that ML models do not transfer well across different domains."
59,": I-inside, O-outside


The concept of Named Entity Recognition (NER) can be approached as a sequence labeling task, where each token in a sequence is assigned a tag indicating if it is part of an entity and its type. This can be done using IOB tagging, where I represents inside, O represents outside, and B represents beginning, or IO tagging, where I represents inside and O represents outside."
60,"Page 60 of the document 'S05 Information_Extraction_V5.3.pdf' discusses the typical features used in information extraction. These features include the word and its neighboring words, their parts of speech, syntactic chunk label, presence in a gazetteer, certain prefixes/suffixes, word shape and case information, and presence of hyphens. These features are important in identifying and extracting information from text."
61,"The document provides an example of word features, which are characteristics of a word that can be used to identify and extract information. These features can include part-of-speech, word length, and capitalization. The example shows how these features can be used to extract information from a sentence, such as identifying proper nouns and numbers. Word features are important for information extraction as they help to narrow down the search for relevant information."
62,"Page 62 discusses feature-based sequence labelling, a technique used in natural language processing to identify and classify sequences of words. This approach involves extracting features from the input data, such as word and character-level information, and using them to train a model to recognize patterns and make predictions. Feature-based sequence labelling is commonly used in tasks such as named entity recognition and part-of-speech tagging. It allows for more flexibility and accuracy compared to rule-based approaches, but also requires careful feature selection and engineering to achieve optimal performance."
63,"Page 63 discusses popular models used in information extraction, including Hidden Markov Models (HMM), Conditional Random Fields (CRF), similarity algorithms, and Support Vector Machines (SVM). HMM uses joint probability while CRF considers features of current and preceding tokens. Similarity algorithms measure the distance of words to a dictionary list and work well for jargon and terminology. SVM is a training method for standard perceptron and optimizes points to determine a dividing hyperplane between positive and negative training samples."
64,"DNN-based IE involves using Deep Neural Network models to process sequences of tokens, such as RNNs, Bi-LSTMs, and Transformers. This approach is used for sequence labeling tasks, such as identifying tokens in a sequence with BIO tags. For NER, the last layer creates a probability distribution over all NER tags and then decodes the result. For Extractive QA, the goal is to identify the exact span of text from a given context passage to answer a question. This approach has shown good results with transfer learning and fine-tuning. For example, when asked ""When was the Eiffel Tower built?"", the extractive answer would be ""1889""."
65,"The document discusses large language models (LLMs) such as ChatGPT/GPT4, LLaMA, and Bard, which have the ability to follow instructions and generate responses without any prior training. This is achieved through instruction tuning and prompt engineering, which involves designing prompts that can elicit the desired response from the models. These LLMs have shown promising results in natural language processing tasks and have the potential to improve human-computer interactions."
66,"Page 66 discusses the use of Information Extraction (IE) with Labeled Lattice Models (LLM) and how it effectively handles named entities. IE is a natural language processing technique used to extract structured information from unstructured text. LLM is a probabilistic model that can handle complex structured data. Together, they can accurately identify and extract named entities such as people, organizations, and locations from text. This makes IE with LLM a valuable tool for tasks such as entity recognition and information retrieval."
67,"The IE with LLM method is a powerful tool that can be used to extract various types of information, even without prior knowledge or training. It is able to do so by using explicit prompts, making it a versatile and efficient option for information extraction tasks."
68,"The document provides several references and resources for information extraction. These include tutorials on the topic by Ron Feldman and Fabio Ciravegna, as well as books by Chris Manning and Hinrich Schutze and Dan Jurafsky. It also mentions the use of WordNet, a lexical database for English, and provides a link to NLP resources and a regular expression tutorial."
