Page,Summary
Page 1,TEXT ANALYTICS MODULE 6: CATEGORIZATION Dr. Wang Aobo Email: [REDACTED_EMAIL] 1 .
Page 2,"the end of the module, you can: Describe what is text categorization and how text categorisation systems work . understand what is topic modeling 2 and what is subject modeling 2 ."
Page 3,PLP Cert Deep learning libraries Transformers BERT / T5 / GPT LLMs pre-training & fine-tuning .
Page 4,"supervised text categorization works: Document data set, building a classifier . document clustering, topic modeling, topic modelling ."
Page 5,WHAT IS TEXT CATEGORIZATION? [REDACTED_PHONE] .
Page 6,"assigned by a cataloger • Slow, tedious • May be inconsistent 6 . may be inconsistent 5 ."
Page 7,MESH index of a single journal paper From: http://library.ulster.ac.uk/sci/mesh/7 .
Page 8,automatic text categorization (also known as “classification”) . hard classification The process of assigning text documents into two or more categories (a document cannot be in more than one category)
Page 9,email spam detection; medical diagnosis; identifying a language . identifying fraud (anomaly detection); negative reviews; monitoring news feed .
Page 10,HOW DOES AUTOMATIC TEXT CATEGORIZATION WORK? 10 of these are from a tv series titled 'automaTIC text CATegORIZ
Page 11,"Text Categorization Phases Two Phases for supervised method 1 . you need a set of documents, already categorized . use an assessor to assess the results ."
Page 12,DOCUMENT DATA SET 12
Page 13,movie reviews classified as “Human” and “Robot” generated From: http://karpathy.ca/mlsite/lecture2.php 13 .
Page 14,movie reviews classified as “Human” and “Robot” generated From: http://karpathy.ca/mlsite/lecture2.php 5000 reviews Training Set Test Set 70% 30% 14 .
Page 15,BUILDING A CLASSIFIER JUST SOME EXAMPLES (NOT EXHAUSTIVE) 15
Page 16,"F. Aiolli, Text Categorization, aiolli/corsi/SI-0607/Lez[REDACTED_PHONE]."
Page 17,"we are trying to find probability of event A, given the event B is true . event b is also termed observations ."
Page 18,"Nave Bayes Model 18 cccmccMMMMMM aaaeaaaccccaaccccCC PP ww1, w2 . P"
Page 19,"PP ww1 , and = count (neg docs having w) / count (total doc) () = count ."
Page 20,"PP ww1 p , P + DDDDDDww = ""II hccaaDDdd aahDDppdddppcccaaa"
Page 21,"list of girls names: Anna, Betty, Chelsea, Doris, Elizabeth, Fanny, Hortense . what is inside the leaf node?"
Page 22,example of a decision tree to decide if a name is male or female From: http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html 22
Page 23,a name is male or female 23 Lastletter =“vowel” Firstletter= “k” Lastletter=“t” Count(f) length Fanny [REDACTED_PHONE] .
Page 24,ATA/S-TA/Text Categorization/V[REDACTED_PHONE] National University of Singapore.
Page 25,the Rocchio Classifiers are classified by similarity to the profile vector . each category is represented by a prototypical document .
Page 26,support vector machines (SVMs) divide term space in hyperplanes . surface that provides the widest separation between support surfaces is selected 26 .
Page 27,EVALUATION 27
Page 28,ACCURACY EVALUATION 28 - 28 . a total of 58 % have been re-opened .
Page 29,automatic classification system . xOR XOR Category B 4 7 Category N 2 8 Category A 1 3 65 9 .
Page 30,"you measure an automatic categorization system by: how well it classifies a set of documents against a “reference” . 80% agree, good methodology . ""your boss tells you to do this, so"
Page 31,automatic classification system . classifies 7/9 of documents = 78% accurate 31/9 . xor xOR XOR Category B 4 7 Category N 2 8 Category A 1 3 65 9 .
Page 32,"weather prediction system predicts one week in advance . if you have a tolerance for error, will I get wet if I use the system to decide whether to carry an umbrella?"
Page 33,Confusion Matrix Predicted Categories Actual Categories A B C . N 33rd . .
Page 34,predicted Categories Actual Categories A B C . N [REDACTED_PHONE] = Tot(A) docs . automatic classification system 34 .
Page 35,N A 87% 2% 5% . 2% C 12% 2% 77% . the 1% B 6% 90% 0% .
Page 36,predicted Actual Yes No Yes 1350 90% 150 10% No 100 20% 400 80% False negative Desired positive prediction Desired negative prediction 36 .
Page 37,EVALUATING MULTIPLE CLASSIFIERS 37 - 37 . CLASSiFIERS ARE CLASSIFICED .
Page 38,you ask two people to predict whether it will rain or not in the coming week . questions: • Who is more “accurate”?
Page 39,Category B 4 7 Category N 2 8 Category A 1 3 65 9 System Human Assessor Actual Categories Same Set of Documents processed by classifier #1 Automatic Classification System .
Page 40,classifier #1 Classifier #2 Predicted Actual Y N Y 700 300 N 2 448 Seems quite good for both predictions Reduced the false positives but false negatives increased Same Set of Documents Which classifier is
Page 41,the courtroom Predicted Actual Guilty Innocent Guillty 900 100 Innocence 40 410 Classifier #1 Classifier #2 Predictted Actual guilti innocent . 700
Page 42,fraud investigation Predicted Actual Honest Fraud Honest 900 100 Fraud 40 410 Classifier #1 Classifier #2 Classifier 2 448 Which classifier is better?? Insurance Claim Statements The average fraud costs the company $2
Page 43,fraud investigation 43 • Consider Doing nothing (don’t act to identify fraud): predicted fraud = 0 cases @$500 per case costs $0k for investigation . andetected fraud is 450 cases @
Page 44,"classifier evaluation focuses on effectiveness, i.e., ability of classifier to make right classification decision . Precision is probability that if a random document di is categorized under category cj, that decision is correct"
Page 45,RUNNING THE CLASSIFIER ATA/S-TA/Text Categorization/V[REDACTED_PHONE] National University of Singapore.
Page 46,"False results eg: spam filtering Email data – non-spam Email data, spam Training Set Email non-spam Email spam Real email stream Falser negative Falsing positive 46."
Page 47,overfitting the Training Set Predicted Actual Y N Y 700 0 N 0 300Training Set . Which classifier is better?? 47 .
Page 48,overfitting the Training Set Predicted Actual Y N Y 700 0 N 0 300Training Set #1 predicted actual Y n Y [REDACTED_PHONE] N 50 250Real Data #1 Real Data
Page 49,Training Set Real Data False negative Falser negative #1 Truese positive #2 49 . the training set was overfitting the Training Set .
Page 50,"automated classifiers make “hard” binary decisions . in example to right, the document, E, is assigned to category C only . ranked categories according to their measure of appropriateness ."
Page 51,"y Y Y N 51 classifiers are classified by classifier, classifier and classifier ."
Page 52,running with more than one classifier Predicted Actual Y N Y 880 120 N [REDACTED_PHONE] . varying methods: Union • Intersection • Algorithmic • Voting • By confidence
Page 53,TEXT CATEGORIZATION APPLICATION EXAMPLES 53 - 53 .
Page 54,Boosting Identification of Fraudulent Claims From: http://www.youtube.com/watch?v=OlQpm8qTog4 54 .
Page 55,UNSUPERVISED TEXT CATEGORIZATION 55 - UNSUPRIVED 55 .
Page 56,DOCUMENT CLUSTERING 56
Page 57,text clustering is the task of grouping documents in such a way that the documents in each group are more “similar” to each other than to documents in other groups . clustering lets you explore your data .
Page 58,clustering Example 58 From: https://www.youtube.com/watch?v=CHlrx4gsoJI .
Page 59,patent clustering 59 from: https://www.youtube.com/watch?v=Z-4S7kIoHa8 .
Page 60,you can control the number of clusters (depending on the algorithm) you don’t need training phase to create clusters . clustering can be language independent (but monolingual)
Page 61,DIMENSIONAL REDUCTION 61 is a resurgence in the u.s. .
Page 62,Dimensional Reduction • Sparsity • High dimension • SVD • Low dimension • low dimension • lower dimension
Page 63,columns are orthogonal and unit vectors . Entries (singular values) are positive and sorted in decreasing order of importance .
Page 64,"top N=2 dimensions 3,11 11,N N,N 3,N T T Sorted Singular Values ."
Page 65,"Dimensions reduced from 11 to N=2 . new Matrix N,N 3,N Sorted Singular Values [REDACTED_PHONE]"
Page 66,Dimensions reduced from 11 to 2 Concept# /SVD# Cluster# [REDACTED_PHONE] Sorted Singular Values . KM Or other classifiers SVD2/Concept2
Page 67,Singular Value Decomposition
Page 68,Automatic Categorization of Documents From: http://www.youtube.com/watch?v=Q5K3gyQJkC0 68 .
Page 69,TOPIC MODELING 69
Page 70,Unsupervised categorization 70 observed Input Variables Latent Variable Observed Outputs Input variables may not be observable Causal .
Page 71,"the potential tags of ""Action"", ""War"" are latent variables Example 71 . the potential tag of ""action"" and ""war"" is latent variable ."
Page 72,$80 Ipoh 9 Apr 30 KL 14 Apr $70 KL 20 May $100 Johor 25 May $20 KL 31 May $3 Kiev 4 Jun $40 KL 23 Jun $50 KL 30 Jun $30 KL 16 Jul
Page 73,topics ATA/S-TA/Text Categorization/V[REDACTED_PHONE] National University of Singapore .
Page 74,Words which occur in similar documents are “topics” Number of Groups are pre-defined 74 From: http://topicmodels.west.uni-koblenz.de .
Page 75,LDA Topic Model Explanation 75 From: https://www.youtube.com/watch?v=3mHy4OSyRf0 .
Page 76,"analysis of text, e.g., • Diachronic analysis: • Speeches during election campaign Economy, abortion, build wall, reduce taxes,... • Different candidates positions and issues ."
Page 77,topic modeling 77 Input output • Predefine number of topics • TDM • Word-Topic distribution . Topics are represented by a list of (important) words .
Page 78,textCategorization: a tutorial on Automated Text Categorisation . F. Aiolli downloaded from http://www.math.unipd.it/aiolli/corsi
Overall Summary,TEXT ANALYTICS MODULE 6: CATEGORIZATION Dr. Wang Aobo Email: [REDACTED_EMAIL] 1 Objectives of this module . Describe what is text categorization and how text categories work . understand what is topic modeling 2 PLP Cert Deep learning libraries Transformers BERT / T5 / GPT LLMs pre-training & fine-tuning Outline .
