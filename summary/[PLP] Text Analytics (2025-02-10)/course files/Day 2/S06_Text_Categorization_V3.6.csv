Page,Summary
Page 1, Dr. Wang Aobo: Text Analysis of Text Analysis and Text Categorization . Dr. Aobo is a member of the Nusus University's Text Analysis Committee . He is the author of the Text Analysis Project
Page 2, Objectives of this module are: Describe what is text categorization and how it's categorization systems work. Understand how supervised and unsupervised texts categorize. Understand what is topic modeling.
Page 3," PLP Cert: Deep learning libraries, deep-learning libraries, pre-training and fine-tuning . Transformers: TransformersBERT / T5 / GPT: GPT, PLP, GPT and PLP"
Page 4, Outline for this module includes: What is text categorization? How does supervised text categorizing work? The module also discusses the use of a classifier and how it should be used to run the classifier . The module includes a
Page 5," NEW: The text of this article is published in this edition of this week's ""worldviewview"" Please submit a copy of the entire text to see how you feel about reading this article ."
Page 6," Contrasts with a library catalog . Subject: Statistics is assigned by a cataloger. Statistics is slow, tedious and may be inconsistent ."
Page 7, MESH index of a single journal paper is based on a paper published in a journal paper . The index is used to look at single journal papers published in the same journal .
Page 8, Automatic text categorization is also known as “classification’s” (or ‘soft classification’) It is the process of assigning text documents uniquely into two or more categories .
Page 9," Some Examples of Text: Email spam detection, medical diagnosis and medical diagnosis . Others: Sentiment analysis (e.g., positive/negative reviews) and monitoring a news feed ."
Page 10, How does  ipient text work? How does it work? We need to know how it works? How do you know how to use it? How are you using it?
Page 11," Text Categorization Phases: Training, Testing and Running . Assessing random sample of the documents against the predicted categories . Assess random samples against predicted categories against the results of the tests . Assessment of the results is performed by"
Page 12,"DOCUMENT DATA SET
12"
Page 13, Movie reviews classified as ‘“Human” and ‘Robot’ generated. ‘Human’ and “Robot.” generated.
Page 14, Movie reviews classified as  “Human” and “Robot” generated by movie reviews . Movie reviews are classified as “human” or “robotic”
Page 15, Just some EXAMPLES (not EXHAUSTIVE) are some of the best examples in the world of architecture .
Page 16, Hand-coded classifiers (the “good old days!”) If <conditions> then <category> then . else NOT<category> else NOT <category>.where conditions are normally in disjunctive normal form
Page 17," Generative Classifiers are trying to find probability of event A, given the  event B is true . Event B is also termed as observations ."
Page 18, The Bayes Model is based on a Bayes model of the Bayes Bayes Category Category . It is called a ‘girglycategory’ by 'girlyc’ and ‘glygly’
Page 19, The Bayes Model is an open-minded Bayes Bayes model . It is based on the likelihood of vocabulary words wrt. classes . The model is called a ‘girglyglyphobia’
Page 20," The Bayes Model is based on the likelihood of vocabulary words wrt. It is a free-form Bayes model . It is written in the form of ""girglyglyphilegistics"""
Page 21," Discriminative Classifiers are called Decision Tree Classifiers . List of girls names: ""Phenomenomeno"" and ""Grispepear"""
Page 22, Example of a decision tree to decide if a name is male or female . Example of decision tree used to decide whether a child should be called a baby .
Page 23," The decision tree is based on a decision tree to decide if a name is male or female . The tree includes Fanny, Kate, Fanny and Howard and Fanny . The name tree is the result of the decision tree ."
Page 24," Singapore's National University of Singapore has published a new book on the topic of Singapore’s new book, ""S-TA"" The book is published by the National Institute of Singapore ."
Page 25," Each category is represented by a prototypical document,  i.e., profile vector . Documents are classified by similarity to the profile vector from the Rocchio Classifier ."
Page 26, The surface that provides the widest separation between  the support surfaces is the support surface that provides the widest separation between the two support surfaces . Support Vector Machines (SVMs) divide the term glygly
Page 27,"EVALUATION
27"
Page 28, The author of this article has published a number of peer-reviewed articles published in the U.S. national Geographic Geographic Center . The author's findings have been published on Amazon.com and Google.com .
Page 29, The HumanAssessor (expert) has defined the criteria for accuracy in his work . The criteria is based on the fact that accuracy is not always accurate .
Page 30," You measure an automatic categorization system by how well it classifies a set of documents against a “reference” This ‘reference’ is normally a human expert, typically 80% agreement, good methodology ."
Page 31," Category N.2 is 86% accurate, Category A is 3% accurate . Category A.1 is 3%, Category B is 4% accurate and Category N N.1% accurate. Category A was 3% more accurate than Category"
Page 32, Discussion: Weather prediction (system predicts one week in advance) Questions: Questions: How accurate is the weather prediction system? How about outcomes – will I get wet if I use the system to decide  whether or not to carry an umbrella
Page 33, Confusion Matrix Predicted Categories: N.A. B C . N. A. C . A. B. C. N. C C N. B B C C . Confusion matrix: Confirmation Matrix . Confirmation
Page 34," A B C and N are examples of categories that can be used to categorise . The categories are based on predicted categories . Categories include A, B C, C, N and N ."
Page 35," The Categories of Categories are based on a number of categories . The Categories are predicted by the number of predicted categories . Categories include: A B C, A C, C, N, A, C and A ."
Page 36," Consider the simple 2x2 matrix: ""Predicted"" and ""Actual"" ""Predictable"" ""Prediction"" is based on the fact that certain documents were classified ."
Page 37,373737.3737 . 37.37. 37.38.37 . 38.37: 38.38: 37: 38: 39: 38E: 28: 38A: 38C: 39E: 39.
Page 38, You ask two people to predict whether it  will rain or not in the coming week: ZZ is right 5/7 times. MK is right 0/7 . You ask ZZ to be more “accurate�
Page 39, Automatic Classification System. What happens with 2 classifiers? Automatic classification System.ypes.                .                 Category A.1 3 65 9.718 Category N.2 86Category A. 1 Category A Category B Category N Category
Page 40, Comparisons are based on actual numbers of documents in order to get a better classifier . The classifier is based on the number of documents produced by the model .
Page 41, The   definition of the  classifier is better?? Adding the semantics – the  phraseology –  includes the  phraseology of  that  guilty innocent
Page 42, The average fraud costs the company $2000; it costs $500 to investigate each suspected fraud . The company spends $500 per suspected fraud; the average fraud cost $2000 .
Page 43, The average fraud costs the company $2000 to investigate each suspected fraud . Undetected fraud is 450 cases @$2k/fraud loses  $900k .
Page 44," Classifier evaluation focuses on effectiveness, i.e.,  the ability of the classifier to make the right classification decision . Precision is the probability that if a random document di is  categorized under category c"
Page 45," The National University of Singapore has published a series of articles under the name of the National Institute of Singapore’s ""ClassificationistATA/S-TA/Text Categorization/V3.0 © 2015 ."
Page 46, Email data – non-spam – Spam – Training Set . Spam is an email filter that filters emails for spam . Email data is filtered out of spam and out of spam .
Page 47, Overfitting the Training Set is a way of overfitting training sets . The Training Set has been predicted to be better than the training set .
Page 48, The Training Set is overfitting the Training Set . The training set was overfitting . The Data Set is based on the data that was predicted by the training set .
Page 49, Overfitting the Training Set – what  happened? The Training Set Real Data is based on data from the training set . The training set was overfitting .
Page 50, Automated (interactive) classifiers instead create ‘hard’ binary decisions by allowing ‘soft’ real-value decisions . Categories rank according to their measure of appropriateness for the document .
Page 51, Aggregations of multiple classifiers have been combined into multiple classifications . Aggregating multiple classifier is a formality for a number of classifications. Aggregative classifiers can be combined into a form of classification. Agg
Page 52," Running with more than one classifier, the results of the classifier was combined with a confidence level of confidence . The results of this analysis were compiled using various methods: Union, Intersection, Algorithmic, Voting and Algorith"
Page 53, The author of this article uses this article as a tool to help students understand the complexities of the world's most complex systems . The author also provides a comprehensive overview of the study of the subject subject subject subjects .
Page 54, Boosting Identification of Fraudulent Claims of Fraudulent Claims . Boosting identification of fraudulent claims to be investigated by the public .
Page 55, UNSUPERVISED CATEGORIZATION.55.55 . UNSUVAGE: The book is published in the U.S. and Canada .
Page 56, Documentary 4.5656.56 . Documentary 5.56 is published in the U.S. National Geographic Society of Columbia .
Page 57, Clustering is the task of grouping a set of documents in such a way that the documents in each group are more “similar” to each other than to documents in other groups .
Page 58, Clustering Example 58:58.58. 58: 58.58: 58: 56: 58 : 58: 57: 58; 58: 59: 58 .
Page 59, Patent Clustering Clustersering: Patent Clustersing.595959.59From: https://www.youtube.com/watch?v=Z-4S7kIoHa8 .
Page 60, Documents tend to fall into natural classes (clusters) Clusters should lead to fulfilling business outcomes . Clustering can be language independent (but monolingual)
Page 61," DIMENSIONAL REDUCTION: $61.61 . $61,000 less. $61 less . $59 less ."
Page 62," Dimensional Reduction: Sparsity, Sparsity or SVD: Dural Reduction: SVD, SVD and SVD . SVD is a low-dimensional reduction of dimensionality . Dimensional Reduction: High dimension, Sp"
Page 63," Columns are orthogonal and unit vectors, and column columns are Orthogonal . Entries (singular values) are positive and sorted in order of order of importance ."
Page 64," Singular Value Decomposition: N=2 dimensions. N =2 dimensions, N = 2 dimensions. The N=3 dimensions are N/2 dimensions and N/3 dimensions . The N-2 dimensions include: N/"
Page 65, The Singular Value Decomposition was reduced from 11 to N=2 . Dimensions reduced from 11 to 2 . The New Matrix was reduced to N-N with a New Matrix .
Page 66, The number of SVDs/Concepts has been reduced from 11 to 2 . The classifier is KM or KM or other classifiers .
Page 67,Singular Value Decomposition
Page 68, Automatic Categorization of Documents: Documentary Documents . Documentary Categorized for the first time . Documentaries are automatically categorised automatically .
Page 69,"TOPIC MODELING
69"
Page 70, Supervised vs Unsupervised: Supervised categorization is defined as unsupervised or supervised . The model is essentially similar to that of other similar models . Supervised versus UnSupervised: Inputs causally effects the
Page 71," The potential tags of “Action”, “War” are latent variables . Actors and their movies are observed inputs . The potential tag of ‘Action’, ‘War’ is latent variables "
Page 72, The cost of Penang is $40 . Anomaly detection is $80 . An anomaly detection is an anomaly . The cost is $100 .
Page 73," What is topic modeling? ""Topic” modeling,"" ""Topic"" modeling."" Can we figure out what discourses (latent variables) would generate the collection of documents? ""These discourses are just bunches of words"
Page 74, Animation of topic modeling: Columns = documents. Rows = words. Squares = frequency. Darker = higher frequency. Groups are pre-defined .
Page 75, LDA Topic Model Explanation: 75% of the LDA topic model is based on the topic of a topic . The Topic Model is a model of a Topic Model that explains the subject matter .
Page 76," More examples of applications include analysis of text, e.g., dachronic analysis, contrast analysis and contrast analysis . Analysis of text: Analysis of speech during election campaign, speeches after taking office. Analysis of different candidates positions and issues"
Page 77, Topics are indexed with numbers without tags without tags . Topics are represented by a list of (important) words . The number of topics is determined by a number of words per topic . The TDM • Word-Topic distribution .
Page 78," F. Aiolli, F. Sebastiani, Fabrizio Sebastiani and John Elder, Gary Miner, Bob Nisbet. Practical Text Mining and Statistical . Analysis for non-Structured Text Data Applications ."
Overall Summary, Aims to understand how text categorization systems work . Describe how supervised and unsupervised texts categorization works . Understand how topic modeling works and what is topic modeling .
