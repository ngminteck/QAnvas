Page Number,Summary
1,"The document discusses the use of deep learning in video analytics, specifically focusing on Convolutional Neural Network (CNN) and Transformer-based models. The author, Dr. Tian Jing, is an expert in this field and can be contacted at a redacted email address. The document is copyrighted by the National University of Singapore and all rights are reserved."
2,"The document provides an overview of video analytics and its applications, specifically focusing on video classification using deep learning. It discusses two methods for video classification - CNN-based and Transformer-based - and their respective advantages and limitations. The document is copyrighted by the National University of Singapore."
3,"The document discusses the benefits of video analytics, which can answer the four ""W"" questions (who, what, where, when) through people detection and identification, object and action analysis, and spatial and temporal analysis. This technology can be used in real-time for situation awareness and alerting, in forensic analysis for archive management and investigation, and in predictive analysis for event and anomaly detection. The NIST report on video analytics in public safety is referenced as a source."
4,"The document discusses the applications of video analytics, including intelligent assisted living, crowd analysis in surveillance, and social activity recognition. Examples of these applications include using deep learning to recognize actions and behaviors of senior citizens in assisted living facilities, analyzing crowd movement in surveillance footage, and recognizing social activities in public spaces. These applications have the potential to improve safety and security, as well as provide valuable insights for various industries."
5,"The document discusses the use of deep learning for action video recognition, which involves identifying and labeling different types of actions in a video. This includes recognizing atomic motion patterns, gestures, and single clear-cut trajectories, as well as series or compositions of actions and combinations of activities or actions. The goal is to accurately label the entire video clip with the appropriate action, and examples of this application include sports games and traffic accidents."
6,"The document discusses the use of deep learning for video analytics, specifically in the context of action recognition. It references a paper on video action understanding and highlights the challenges and potential applications of this technology. The goal is to accurately identify and classify actions in videos, which can have various real-world applications such as surveillance and sports analysis. The document also mentions the use of convolutional neural networks (CNNs) for feature extraction and the importance of large datasets for training deep learning models."
7,"The document discusses the use of deep learning in video analytics, specifically in action video recognition. It provides an example of UCF11, a dataset commonly used for action recognition research. The dataset contains 11 action categories and is used to train and evaluate deep learning models for action recognition tasks. The document also mentions the use of pre-trained models and transfer learning in action recognition."
8,"The document discusses the challenges of action video recognition, including intra- and inter-class variations, cluttered backgrounds and camera motion, insufficient annotated datasets, uneven contributions from frames, and long-tailed distributions. It also mentions the importance of deep learning in addressing these challenges and provides a link to a figure showing the major image and video datasets used for intelligent video analysis."
9,"The document discusses the Action video recognition dataset, which contains examples of human and non-human actions, annotated with action classes, temporal markers, and spatiotemporal bounding boxes/masks. This dataset is useful for studying and developing video action understanding techniques. The reference for this dataset is provided as ""Video Action Understanding"" on IEEE Xplore."
10,"The use of deep learning in video analytics has led to significant advancements in understanding and analyzing actions in videos. One key aspect of this is data augmentation, which involves adding variations to the training data to improve the model's performance. Geometric, photometric, and chronometric augmentations are commonly used techniques to create variations in the data. Geometric augmentation involves manipulating the spatial positions of objects in the video, while photometric augmentation alters the color and lighting conditions. Chronometric augmentation changes the temporal aspects of the video, such as playback speed. These techniques have been shown to improve the accuracy of action recognition models and can be applied to various types of action videos."
11,"The document discusses the use of augmentation techniques in action videos for deep learning models. Specifically, it mentions the use of RandAugment-T, which randomly selects a certain number of augmentations from a list of operations and applies them to a set of frames with specified magnitudes. Examples of augmentations include rotation, posterization, equalization, and color adjustments. The document also provides a reference for further information on this technique."
12,"The document discusses the use of augmentation techniques in deep learning for action video analysis. Temporally varying geometric augmentations, such as vertical-down translation and clockwise rotation, can help improve the robustness of models to changes in camera angles and viewpoints. Similarly, temporally varying photometric augmentations, such as increasing brightness and decreasing contrast, can help models handle changes in lighting conditions. These techniques can enhance the performance of deep learning models for action video analysis."
13,"of a running horse by Eadweard

The Horse in Motion, a series of cabinet cards by Eadweard Muybridge in 1878, is considered a significant study in action video recognition. It consists of sequential photos capturing the movement of a horse, which were later used to create an animation. This study is referenced in the field of video analytics using deep learning."
14,"This section discusses a study from 1992 that focused on recognizing human actions in videos using a hidden Markov model. The study used time-sequential images to identify the state of action, with a symbol representing the number of the state. This approach can be useful in analyzing action videos in their original background and foreground."
15,"The document discusses the idea of using a single frame to recognize actions in videos. This approach involves building an image classification model based on action categories and applying it to each frame independently, followed by a major voting from multiple frames in the input sequence. The MPII human pose dataset, which contains 410 actions and 40000 instances, is used for this method."
16,The document discusses a method for reusing single image classification techniques for video analysis. The idea is to convert a video sequence into a single frame by extracting and combining silhouettes from the frames. This approach is based on the concept of background subtraction and involves centering and normalizing the silhouette images to maintain their aspect ratio. The technique is inspired by a research paper on recognizing human activities from silhouettes.
17,"The motion energy image is a binary image that represents the cumulative motion in a video sequence, with brighter regions indicating movement. It is calculated based on the past N frames and can be used to detect and track human movement. This concept is based on the research paper ""The recognition of human movement using temporal templates."""
18,"The document discusses different methods for feature extraction in motion energy images, which are used in video analytics using deep learning. These methods include hand-crafted shape representation, such as chain code, histogram, and geometrical features, as well as region-based methods such as counting non-zero pixels or calculating gradient histograms. These methods can be used to generate labels and extract features from motion energy maps, which can then be used for tasks such as character recognition."
19,"The document provides an overview of video analytics and its use in deep learning, specifically through CNN-based and Transformer-based methods for video classification. It explains the benefits and challenges of using these techniques and discusses their potential applications in various industries. The document also highlights the importance of data labeling and model training for accurate results."
20,"The document discusses a tutorial on video modelling presented at CVPR 2020 and 2021. The tutorial covers various topics related to action video recognition, including deep learning techniques and applications. It also provides resources for further learning, such as links to related papers and code repositories. The tutorial aims to provide a comprehensive understanding of video modelling and its applications in computer vision."
21,"The process of action video recognition involves treating a video sequence as a fixed-size bag of clips, also known as ""trimmed video classification."" This can be done through sliding window or automatic segmentation into shorter sequences, known as action localization. The key questions in this process include determining which features, such as semantic or motion, need to be extracted and where they should be extracted from, such as every pixel or key points, and which frames should be used."
22,"The document discusses various milestones in action video recognition using deep learning, including non-deeply learned features such as sparse and dense features, and deeply learned features such as single-stream and two-stream CNNs, C3D, and self-attention models. It also addresses the specific challenges of action video recognition, such as long video sequences, and presents solutions such as LRCN, TSN, T-C3D, and SlowFast networks. The document also mentions the importance of model efficiency and introduces P3D and S3D models as examples."
23,"The HoF Bag-of-words representation is a method for analyzing videos using deep learning. It involves detecting local structures in space-time and extracting space-time patches in their neighborhood. These patches are then divided into a grid and histograms of oriented gradient (HoG) and histograms of optic flow (HoF) are computed for each cuboid. This approach was proposed by Laptev et al. in 2008 and has been used for learning human actions from movies. An additional bin is added for the histogram of optical flow to account for static pixels with no motion. A classifier, such as SVM, is used to classify the extracted features."
24,"The document discusses a method for densely sampling feature points on a grid and computing descriptors within spatial-temporal cuboids. The method involves quantizing orientations into 8 bins, with an additional zero bin for lower magnitudes in HOF. The final descriptor sizes are 96 for HOG and 108 for HOF."
25,"The MBH descriptor is a method for computing optical flow and spatial derivatives between two consecutive frames in a video. This allows for the quantization of orientation information into histograms, resulting in an 8-bin histogram for each component. The MBH descriptor is useful for action recognition as it encodes the relative motion between pixels, excluding the camera motion. It was introduced in a 2011 paper by H. Wang, et al. titled ""Action recognition by dense trajectories."""
26,The Single-stream CNN (early fusion) approach uses a convolutional neural network (CNN) and other layers such as Conv and Pooling to classify videos into categories. It combines information from the entire sequence of frames and uses a multi-layer perceptron (MLP) classifier to make predictions. This approach was proposed in a 2014 paper by A. Karpathy et al. and has been used in large-scale video classification tasks. The input for this approach is multiple frames from a video.
27,The single-stream CNN (late fusion) approach uses a multi-layer perceptron classifier to classify videos into different categories. It involves using multiple frames from the input video and merging them with a shared parameter network. This method is inspired by the work of A. Karpathy and his team on large-scale video classification using convolutional neural networks.
28,"The article discusses two-stream video analytics, which combines appearance and motion information to improve action recognition in videos. The spatial stream operates on individual frames, while the temporal stream uses optical flow displacement fields between consecutive frames. This approach has been shown to be effective in action recognition, as demonstrated by the research of K. Simonyan et al. in their 2014 paper ""Two-stream convolutional networks for action recognition in videos."""
29,"The goal of two-stream fusion is to combine two networks at a specific convolutional layer in order to align channel responses at the same pixel position. This technique is used for video action recognition and can be implemented at different layers, such as after the fourth conv-layer or after conv5 and fc8. In the latter case, one network tower is used as a hybrid spatiotemporal net while the other is used as a purely spatial network. The reference for this technique is the paper ""Convolutional two-stream network fusion for video action recognition"" by C. Feichtenhofer et al. at CVPR 2016."
30,"The document discusses different methods of fusing two networks in video analytics using deep learning. These methods include sum fusion, max fusion, concatenation fusion, and bilinear fusion. Each method has its own way of combining two feature maps at the same spatial locations and feature channels. The output of each fusion method varies in dimensions, with bilinear fusion producing a matrix of outer products and summations. The reference for these methods is provided as a study by C. Feichtenhofer et al. in 2016."
31,"The C3D (Convolutional 3D) method involves convolving not only on the spatial dimensions but also on the temporal dimensions, allowing for the joint modeling of spatiotemporal information. This technique is used in deep learning for video analytics and was introduced in 2015 by D. Tran et al. in their paper ""Learning spatiotemporal features with 3D convolutional networks""."
32,"The C3D method uses a combination of early and late fusion using 2D and 3D convolutional neural networks (CNNs) to process video data. The model architecture example shows the size of the input and output feature maps for each layer. Early fusion combines the inputs from different frames using a 2D CNN, while late fusion uses a 2D CNN to process each frame separately and then combines the outputs. The 3D CNN is used in the C3D method to process the video data. Paddings are used to maintain the feature map resolution in the model."
33,"This section discusses other potential model architectures for video analytics using deep learning, specifically referencing a paper by J. Carreira et al. that introduces a new model and dataset for action recognition. The referenced paper can be found on arXiv."
34,"The concept of self-attention, as introduced in the paper ""Non-local neural networks"" by X. Wang et al., involves incorporating spatial-temporal self-attention blocks into existing 3D CNN architectures. This allows for improved performance in video analytics using deep learning techniques."
35,"The challenge of processing long video sequences is addressed by using LRCN, which combines a CNN for visual input with a stack of recurrent sequence models (LSTMs) to produce variable-length predictions. Both the CNN and LSTM weights are shared across time, allowing for scalability to arbitrarily long sequences. This approach is based on the research of J. Donahue et al. and was presented in CVPR 2015."
36,"The challenge of processing long video sequences is addressed by TSN models, which use a segment-based sampling and aggregation scheme. This involves dividing the video into segments and creating a video-level prediction by averaging the outputs of the segments. Each segment is further divided into short snippets, which are represented by RGB frames, optical flow, and RGB difference. The class scores of snippets from the same modality are combined using a segmental aggregation function, and the ConvNets used for processing these snippets share the same parameters. This approach has been shown to be effective in deep action recognition tasks."
37,The challenge of analyzing long video sequences is addressed by using the T-C3D (temporal convolutional 3D network) approach. This involves dividing the input video into smaller parts and selecting frames from each part to create clips. These clips are then fed into shared 3D-CNNs (convolutional neural networks) and the resulting feature maps or class scores are combined using an aggregation function to make predictions at the video level. This method has been shown to be effective for real-time action recognition.
38,"The challenge of analyzing long video sequences is addressed in the paper ""SlowFast Networks for Video Recognition"" by C. Feichtenhofer et al. The proposed method involves using two streams with different frame rates - a slow pathway with a low frame rate to capture semantic information and a fast pathway with a high frame rate to capture motion information. The parameters 𝛼𝛼 and 𝛽𝛽 can be adjusted to control the frame rate and convolutional channels, respectively, for computational complexity considerations."
39,"The document discusses model efficiency performance metrics for deep learning models. These metrics include the number of parameters, total or peak activations, model size, and MACs (Multiply-Accumulate Operations). The common units for model size are MB, KB, and bits. MACs can be calculated for different types of operations, such as Matrix-Vector Multiplication and General Matrix-Matrix Multiplication. FLOP (Floating Point Operations) is also a metric used to measure efficiency. The reference provided explains these metrics in more detail."
40,": P3D-63, P3D-131, P3D-199

The challenge addressed in this section is the computational complexity of Conv3D. The solution proposed is the Pseudo-3D Residual Net (P3D ResNet), which converts 3 × 3 × 3 convolutions to 1 × 3 × 3 convolutional filters on the spatial domain, equivalent to 2D CNN, and adds 3 × 1 × 1 convolutions to create temporal connections on adjacent feature maps in time. Three variants of P3D ResNet are proposed: P3D-63, P3D-131, and P3D-199. This approach is based on a paper"
41,"The challenge of model efficiency in video analytics using deep learning is addressed, specifically regarding the use of 3D convolution. The question of whether 3D convolution is necessary and which layers should be 3D is raised, as well as the importance of jointly convolving over time and space. Two different design approaches, bottom-heavy and top-heavy, are mentioned as potential solutions. A reference to a study on speed-accuracy trade-offs in video classification is also provided."
42,"The document discusses the use of deep learning for video analytics, specifically focusing on two methods: CNN-based and Transformer-based video classification. The CNN-based method involves using convolutional neural networks to extract features from video frames and classify them, while the Transformer-based method utilizes a self-attention mechanism to capture temporal relationships between frames. Both methods have shown promising results in accurately classifying videos."
43,"The document discusses different approaches for video recognition using deep learning. It mentions ViViT, a transformer-based method for tokenization of videos, TimeSformer which uses an attention mechanism for video understanding, MViT which allows for multiple scaling representations, and VideoSwin which extends image transformers to video. These methods have been presented in various conferences and publications, such as ICCV 2021 and CVPR 2022."
44,"The document discusses two methods for video tokenization and embedding: frame-based and tubelet-based. In the frame-based method, 2D frames are independently embedded into non-overlapping patches, while in the tubelet-based method, spatio-temporal ""tubes"" are extracted and projected into a linear space. The objective is to embed the input video clip into a sequence of tokens and add positional embedding before reshaping. The paper ""ViViT: A Video Vision Transformer"" is referenced for this approach."
45,"The attention mechanism for video involves breaking down each frame of a video clip into non-overlapping patches and flattening them into vectors. These vectors are then linearly embedded into an embedding vector, and a softmax function is applied to calculate the attention weights for each patch in three different ways: joint, space, and time. This approach was proposed by G. Bertasius et al. and was presented at ICML 2021. The embedding dimension for each head is [REDACTED_PHONE]."
46,"The document discusses the use of attention mechanisms for video analytics using deep learning. These mechanisms are applied to tensors with specific shapes, including time, height, width, and embedding dimension. The table from GPT-4 456 is used as a summary of these mechanisms."
47,"The MViT (Multiple head pooling attention and Multiple scale transformer block) is a self attention operator that allows for flexible resolution modeling in a transformer block. It operates at changing spatiotemporal resolution and progressively grows channel resolution while reducing sequence length. This allows for fine spacetime resolution in early layers and coarse spacetime resolution in late layers. The reference for this technique is a paper titled ""Multiscale Vision Transformer"" from ICCV 2021."
48,"The VideoSwin idea proposes to extend the concept of Swin Transformer to video by incorporating 3D shifted window multiple-head self-attention. This approach was introduced in a research paper by Z. Liu et al. titled ""Video Swin Transformer"" and presented at the CVPR 2022 conference. The goal is to improve the performance of video analytics using deep learning techniques."
49,"The workshop focuses on using the UCF11 Dataset, which contains 11 action categories such as basketball shooting, biking/cycling, and diving. The dataset can be accessed through the website of the Center for Research in Computer Vision at the University of Central Florida. The dataset covers a range of activities such as sports, leisure, and everyday tasks."
50,"The document discusses the future of video analytics using deep learning, referencing a survey on foundation models for video understanding. It suggests that the field will continue to evolve and advance, with potential developments including improved accuracy, faster processing, and the incorporation of more complex data sources. The document also highlights the importance of ongoing research and collaboration in furthering the capabilities of video analytics using deep learning."
51,"The document discusses a case study that showcases the paradigm shift in video analytics using deep learning. The traditional approach of using hand-crafted features for video analysis is being replaced by deep learning methods. This allows for more accurate and efficient detection and classification of objects and events in videos. The case study focuses on the use of deep learning for pedestrian detection and tracking, and shows significant improvements in accuracy and speed compared to traditional methods. It also highlights the potential for further advancements in video analytics using deep learning."
52,"The AI foundation models (FM) are the basic building blocks of artificial intelligence, including linear regression (LR), decision tree (DT), support vector machine (SVM), clustering (CS), large language model (LLM), vision (V), science (S), and audio (A). These models are used to train and develop more complex deep learning algorithms. IBM explains the differences between machine learning, deep learning, and foundation models in a YouTube video."
53,"This section discusses the concept of foundation models in the context of computer vision and deep learning. These models aim to unify the generative and discriminative approaches to visual understanding, and a survey of existing research in this area is provided. The paper referenced in this section explores the potential benefits of this unification and discusses various techniques and architectures used in foundation models. The goal is to create a more comprehensive and efficient model for visual understanding."
54,"The document discusses the use of deep learning in video analytics, specifically focusing on foundation models that combine language and vision. These models have shown promising results in tasks such as image captioning and video question-answering. They are also being used in various applications such as video surveillance and video summarization. The document references a survey on multimodal large language models, which provides further insights into the potential of these models in the future."
55,"The document discusses the evolution of computer vision models from hand-crafted features to machine learning and deep learning algorithms. It introduces the concept of foundation models, such as CLIP and Flamingo, which combine different techniques to achieve better performance. The emergence of these foundation models has led to a homogenization of learning algorithms and architectures, allowing for more efficient and effective development of machine learning systems across various applications."
56,"Page 56 of the document 'RTAVS 2 Video analytics using deep learning v3.3.pdf' contains contact information for Dr. TIAN Jing, an expert in video analytics using deep learning. This information is provided for those who have questions or need further assistance with the topic. The document is copyrighted by the National University of Singapore and all rights are reserved."
