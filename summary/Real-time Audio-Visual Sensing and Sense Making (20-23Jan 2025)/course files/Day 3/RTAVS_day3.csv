Page Number,Summary
1,"The document 'RTAVS_day3.pdf' discusses audio and speech analytics, specifically focusing on the work of Dr. Gary Leung from the National University of Singapore. It mentions that the rights to the document are reserved by the university until 2025."
2,"Page 2 of the document 'RTAVS_day3.pdf' discusses the importance of incorporating technology in the teaching and learning process. It highlights the benefits of technology, such as providing access to a wide range of resources and enhancing student engagement and collaboration. The document also emphasizes the need for teachers to be trained in using technology effectively and incorporating it in their lesson plans. It also mentions the role of technology in preparing students for the future workforce and the need for schools to have a well-equipped IT infrastructure. The document concludes by stating that technology should be viewed as a tool to enhance teaching and learning, rather than a replacement for traditional teaching methods."
3,"The module on audio and speech analytics aims to provide students with a strong understanding of the basics of audio signal processing, feature extraction, and representation for audio and speech analytics. Students will develop skills in designing, building, and implementing intelligent audio and speech analytics systems. This module is offered by the National University of Singapore."
4,"The document 'RTAVS_day3.pdf' outlines the topics covered on the third day of the RTAVS course, which focuses on audio signal processing and its various applications. The day begins with an introduction to the course, followed by a discussion on the basics of audio signal processing. Sound event detection and localization, audio fingerprinting, voice activity detection, and wake-word and keyword spotting are also covered. The second day of the course covers topics such as speech recognition, speaker recognition, speaker diarization, spoken language recognition, paralinguistic speech processing, security and privacy, and audio-visual applications."
5,"The document 'RTAVS_day3.pdf' mentions several resources for computational analysis of sound scenes and events. These include a book on the topic, a Python library for audio feature extraction and classification, and various GitHub repositories for deep learning and audio processing for machine learning. Additionally, the document recommends an online resource for learning about speech processing."
6,"The document discusses the applications of audio analysis, including speech recognition, virtual assistants, music search, surveillance, environmental monitoring, and recommendation systems. The objective is to extract meaningful information from raw audio signals through either signal analysis or machine learning techniques. This can be achieved through feature extraction or pattern discovery."
7,"The document discusses three potential applications for emotion-aware dialogs, urban sound monitoring, and using a phone to pick a good watermelon. The references provided include videos demonstrating these applications."
8,"The audio signal is a travelling vibration or wave that transfers energy through a medium, such as air, until it is perceived by our ears. The amplitude of the signal determines its loudness, while the frequency refers to the number of vibrations per second."
9,"The sampling rate is a key concept in converting a continuous audio signal into a discrete sequence of numbers. It is determined by the sampling period, which is the interval between two successive samples, and is described by the sampling frequency. Common examples of sampling frequencies include 44.1 kHz for CD recordings, 8 kHz for telephone audio, and 16 kHz for PC and smartphone audio."
10,"Sampling resolution, also known as quantization, is the process of representing real numbers in a sequence of samples with finite discrete values. This is measured in bits per sample and is an important property in the quantization procedure. For example, (0ùëâ, 6ùëâ) can be represented using 2 bits or 3 bits, with the latter providing a more accurate representation."
11,"The document discusses the use of analog input signals in audio signals. Analog signals are continuous and can be represented by a waveform. They are converted into digital signals for processing and transmission. The process of converting analog signals into digital signals is known as analog-to-digital conversion. This conversion involves sampling and quantization. Sampling is the process of measuring the amplitude of the analog signal at regular intervals, while quantization involves assigning a numerical value to each sample. The resulting digital signal can then be processed and transmitted through digital systems."
12,"The document discusses filtering in the context of analog input signals. It explains that filtering is the process of removing unwanted components from a signal, and is typically used to improve the quality of audio signals. The diagram shows how a filter can be used to remove noise from an audio signal over time. The document also mentions that the National University of Singapore holds the rights to the information presented."
13,"The document discusses the use of filtering in audio signals, specifically in the context of an analog input signal and a clock. Filtering is a process that removes unwanted noise and interference from a signal, resulting in a cleaner and more accurate output. In this case, the analog input signal and clock are used to represent the original audio signal and the timing of the signal, respectively. By filtering these signals, the output can be improved and provide a better listening experience. This is an important concept in audio processing and is used in various applications, such as music production and sound engineering."
14,"The content on page 14 discusses filtering and sampling techniques in audio signals. Filtering is used to remove unwanted noise and frequencies from the signal, while sampling involves taking discrete measurements of the signal at regular intervals. The analog input signal is converted into a digital signal through this process. A clock is used to synchronize the sampling process and ensure accurate measurements. Overall, filtering and sampling are important components in digital signal processing for audio signals."
15,"Page 15 of the document 'RTAVS_day3.pdf' discusses the process of filtering, sampling, and quantizing an analog input signal in order to convert it into a digital signal. The analog input signal is first filtered to remove any unwanted noise or interference. Then, it is sampled at regular intervals by a clock signal, which determines the frequency at which the signal is measured. The sampled values are then quantized into discrete levels, with the number of levels depending on the bit depth of the quantizer. This results in a digital signal that can be easily processed and transmitted."
16,"Page 16 discusses the process of converting an analog input signal into a digital signal through filtering, sampling, and quantization. The analog signal is first filtered to remove any unwanted noise, then sampled at regular intervals to create discrete data points. These data points are then quantized into binary code using an encoder. The resulting digital signal is represented by a series of bits, with the first bit indicating the sign and the remaining bits representing the amplitude magnitude. This process is essential in digital signal processing and is commonly used in audio signals."
17,"This page discusses the process of converting an analog input signal into a digital signal using filtering, sampling, and quantization. The analog signal is first filtered to remove unwanted noise, then sampled at regular intervals to create discrete data points. These data points are then quantized, or assigned a numerical value, based on their amplitude. The resulting digital signal is then encoded and can be decoded back into an analog signal when needed. The process is represented by a diagram and a phone number is redacted for privacy."
18,"Page 18 discusses the process of converting an analog signal to a digital signal. This involves using a filter to remove any unwanted frequencies, sampling the analog signal at regular intervals, and quantizing the samples to represent them digitally. The digital signal is then converted back to analog using a digital-to-analog converter. This process is important for preserving the quality of the original analog signal and ensuring accurate representation in the digital domain. The diagram on the page illustrates the steps involved in this process."
19,"Speech coding involves converting continuous signals into discrete values through quantization. Each sample is represented by a fixed-point number in a computer, typically 16 bits. The most commonly used method is Pulse-Code Modulation (PCM), which is a linear coding method. However, non-linear coding methods such as A-law and ¬µ-law, which use only 8-bit encodings, are also used in Europe and the US/Japan respectively. These methods use logarithmic encoding to improve the quality of quiet sounds by increasing the quantization resolution for low-amplitude signals."
20,"The document discusses popular audio file formats, including WAV, MP3, FLAC, and OGG. WAV is a Microsoft and IBM-developed format that uses uncompressed lossless PCM. MP3 is a lossy data compression format, while FLAC is a free lossless audio codec that is widely supported. OGG is a free multimedia container format that supports many codecs. In speech processing, non-compressed PCM format is typically used for input, and different systems may have different sampling rates."
21,"The quality of a recording is affected by several factors, including the microphone quality, ambient noise level, and recording level. It is important to properly set the recording level to avoid losing resolution or clipping, which occurs when the signal value exceeds the maximum. An example of clipping can be heard in the provided YouTube link."
22,"The document discusses the concept of sense-making from audio, which involves classifying audio recordings into predefined acoustic scene classes and identifying specific sound events within the recording. This is achieved through sound event localization and detection methods, which output the event labels, onset-offset times, and spatial locations. The use of spatiotemporal metadata can aid in predicting urban sound tags from recordings in an urban acoustic sensor network. Additionally, the document mentions the generation of textual descriptions and the identification of anomalous sounds emitted from a target machine."
23,"The document discusses various techniques for making sense of speech, including Voice Activity Detection, Wake-word and Keyword Spotting, Speech Recognition, Speaker Recognition, Speaker Diarization, Speech Language Recognition, and Speech Emotion Recognition. These techniques are important for understanding and processing speech in various applications. Voice Activity Detection is used to detect when someone is speaking, while Wake-word and Keyword Spotting are used to trigger a device or application. Speech Recognition is the process of converting speech into text, while Speaker Recognition and Diarization are used to identify and differentiate between speakers. Speech Language Recognition is used to identify the language being spoken, and Speech Emotion Recognition is used to detect the emotional state of the speaker. These techniques are essential for improving the accuracy and"
24,"The document discusses benchmark datasets used in computational analysis of sound scenes and events. These datasets are important for evaluating and comparing different algorithms and techniques. The reference for these datasets is a paper by Virtanen, Plumbley, and Ellis, and they can be accessed through the website https://cassebook.github.io/. The datasets can be either recorded, collected from available repositories, or produced synthetically."
25,"Page 25 of the document 'RTAVS_day3.pdf' discusses a benchmark dataset called SINGA:PURA, which contains strongly-labelled polyphonic urban sounds with spatiotemporal context. This dataset was created by the National University of Singapore in 2025 and is freely available for research purposes. SINGA:PURA is designed to improve the performance of sound recognition algorithms and contains various urban sounds such as traffic, construction, and human activity. The dataset also includes spatiotemporal information, such as location and time, to provide a more realistic and challenging environment for sound recognition tasks."
26,"Audio features can be extracted from an audio sequence by breaking it into smaller segments called frames. Two commonly used features are energy and zero crossing rate. Energy measures the intensity of a signal, while zero crossing rate measures how often the signal changes sign. This can be used to determine the smoothness of a signal, with a voice signal having fewer zero crossings compared to an unvoiced fricative. The number of zero crossings can be calculated within a segment of a signal to measure its smoothness."
27,"The document discusses audio features, specifically time-domain features, and how to extract them using the pyAudioAnalysis library. The function feature_extraction is used to implement the short-term windowing process, which extracts a set of features for each short-term window and stores them in a matrix. The arguments for this function include the input signal, sampling rate, window size, and window step. One specific feature mentioned is EnergyZCS, which is extracted using a sliding window approach."
28,"Fourier analysis involves breaking down a signal into its constituent sinusoidal functions, known as the Fourier coefficients. The coefficients are calculated by multiplying the signal values by a complex exponential function and summing them. The Fourier transform is a mathematical tool that converts a signal from the time domain to the frequency domain, and vice versa. This is useful for analyzing and manipulating signals in various applications, such as image and audio processing. The forward and inverse Fourier transforms are used to convert signals between the time and frequency domains."
29,The content on page 29 of the document 'RTAVS_day3.pdf' discusses the frequency representation of sound through spectrograms. It explains that spectrograms are 2D representations of sound in the time-frequency domain. The process of creating a spectrogram involves breaking the signal into short-term windows and using Fast Fourier Transform (FFT) to analyze the frequency components of each window. The document also provides a link to an online demo and a YouTube video for identifying sounds in spectrograms.
30,"A recent study found that wearing face masks may decrease speech intelligibility, with smaller differences at lower frequencies and significant decreases at higher frequencies. This is shown in a spectrogram where red indicates a large response, yellow indicates a small response, and blue indicates zero response. When a talker wears a mask, the spectrogram will likely show a decrease in response across all frequencies."
31,"The document discusses speech feature frames, which are used in speech recognition systems to represent speech signals. These frames typically contain 10-30 milliseconds of speech data and are extracted from the speech signal at regular intervals. The frames are then analyzed for their acoustic features, such as pitch and energy, which are used to classify the speech into different phonemes. This process is repeated for each frame in the speech signal, allowing for accurate transcription of the speech. The document also mentions the use of machine learning algorithms to improve the accuracy of speech recognition systems."
32,"The document discusses audio features, specifically frequency-domain features. One of these features is the spectral centroid, which is the center of gravity of the spectrum and is calculated as the weighted mean of frequencies present in the signal. This is demonstrated through an example of comparing male and female audio sequences."
33,"The audio features used in this document include Mel-Frequency Cepstral Coefficients (MFCC), which are calculated by taking the log of the filter bank energies after passing the power spectrum through a Mel-scale filter bank. The first 13 MFCCs are typically selected as they are considered to carry sufficient discriminative information for audio analytics. This process involves breaking the audio signal into frames, applying fast Fourier transform (FFT), calculating the log of energies, and then applying discrete cosine transform (DCT) to obtain compact coefficients."
34,The Mel scale is a way of representing frequency that takes into account human perception. It relates the perceived pitch of a pure tone to its actual measured frequency. Humans are more sensitive to changes in pitch at low frequencies compared to high frequencies. This scale is commonly used in speech processing to better match what humans hear.
35,"Hz

The Mel scale is a way of representing the perceived frequency or pitch of a sound in relation to its actual measured frequency. Humans are more sensitive to changes in lower frequencies and less sensitive to changes in higher frequencies. This scale is important in speech processing for machine learning as it allows features to better match what humans hear. The document provides a reference for further reading and includes a code for generating and differentiating sounds. The document also includes a table showing different thresholds for changes in frequency and corresponding frequencies that can be differentiated by humans."
36,"The cepstral domain is a type of audio feature that includes delta and delta-delta features, which approximate the first and second derivatives of a feature at a specific time. These features are often added to other features like MFCC, resulting in a dimension of 39."
37,"The use case for speech versus music involves analyzing the energy distribution and spectral changes in audio signals. A high energy entropy mean indicates a more spread out energy distribution, while a low energy entropy mean suggests a more concentrated and predictable distribution. Similarly, a high mfcc_3_std value indicates rapid and irregular spectral changes, while a low mfcc_3_std value suggests smoother and more regular changes. These metrics can help distinguish between speech and music in audio data."
38,"The use case for audio events involves using audio data to identify and classify different types of sound events. This can be useful in various applications such as surveillance, smart homes, and industrial monitoring. The process involves collecting audio data, preprocessing it, extracting features, and using machine learning algorithms to classify the events. This use case has the potential to improve efficiency and accuracy in various industries and can be further enhanced with the use of advanced technologies such as deep learning and sensor fusion."
39,The audio analytics system is a computational tool used to analyze sound scenes and events. It involves extracting segments or frames from the input audio and applying time-domain and frequency-domain features through transformations such as the Fourier transform. These features are then further stacked into a spectrogram and cepstral features. The system also utilizes filtering and transformation techniques to enhance the accuracy of the analysis.
40,"The use case for audio events involves obtaining a sequence of feature values for each input audio sequence, using a sliding window. This allows for the analysis of audio events and can be used for tasks such as speech recognition and music genre classification. Each dot represents a feature value calculated from the sliding window."
41,"The document discusses what has been learned about audio signal representation and various methods for extracting audio features. These methods include time-domain features, frequency-domain features, and cepstral features. These techniques are important for analyzing and processing audio signals in various applications."
42,"The objective of sound event detection is to recognize and identify different events or activities happening in an audio signal and determine when they occur. This is an important task in various applications such as surveillance, audio monitoring, and environmental monitoring. To achieve this, advanced algorithms and techniques are used to analyze the audio signal and extract relevant information. The goal is to improve the accuracy and efficiency of sound event detection for practical use."
43,"of RTAVS_day3.pdf discusses the evolution of sound event detection techniques, from early approaches using Gaussian mixture models and hidden Markov models, to the current dominant method of using deep neural networks (DNNs). DNNs have the advantage of being able to perform multi-label classification, indicating the presence of multiple sound classes at the same time. This allows for more accurate and efficient sound event detection."
44,"The document discusses sound event detection, which is the task of identifying and localizing specific sounds in an audio recording. This is a challenging task due to the variability and complexity of real-world sounds. The authors propose a deep learning-based approach that combines a convolutional neural network (CNN) and a recurrent neural network (RNN) to accurately detect and localize sound events. They also introduce a new dataset, called AudioSet, which contains a large variety of sound events and can be used to train and evaluate sound event detection models. The proposed approach outperforms previous methods on this dataset, demonstrating its effectiveness for sound event detection tasks."
45,"Sound event localization is the process of identifying the location of a sound source in relation to the microphone recording the sound scene. This involves determining the horizontal azimuth angle, vertical elevation angle, and distance from the microphone. This information is important for applications such as speech recognition and source separation."
46,"The document discusses sound event localization, which is the process of determining the location of a sound source in a given environment. This is an important task in computational audio scene analysis, as it can provide valuable information for applications such as automatic surveillance and acoustic monitoring. The localization process involves using multiple microphones to capture the sound and then using signal processing techniques to estimate the direction and distance of the source. This can be achieved through methods such as time difference of arrival and intensity-based techniques. The document also mentions the challenges in sound event localization, such as dealing with background noise and reflections, and the potential for future research in this field."
47,"The document discusses sound event localization, which is the process of determining the location of a sound source in an environment. This is an important task in the field of computational audio scene analysis, as it allows for better understanding and analysis of sound events. The key techniques for sound event localization include time delay estimation, beamforming, and machine learning algorithms. These techniques can be applied to various scenarios, such as single or multiple sound sources, and can also be combined for improved accuracy. The document also highlights the challenges and future directions for research in this area."
48,"Audio fingerprinting is a technique that uses unique characteristics to identify and match audio signals. It creates a compact digital representation that can be compared quickly to a database of known audio fingerprints. This technique has various applications, including music recognition services like Shazam and copyright protection by detecting unauthorized use of copyrighted music or audio."
49,"Audio fingerprinting is a method of identifying audio recordings using algorithms that detect spectral peaks and convert them into unique numerical identifiers. The lowest and highest frequencies are often used to focus on relevant ranges and reduce noise. These identifiers are then stored efficiently using data structures like hash tables or inverted indexes. When a new audio sample is input, its fingerprint is computed and compared to stored fingerprints to find a match. This process is explained in more detail in a reference provided."
50,"The concept of audio fingerprinting involves using a locality sensitive hashing function to group similar fingerprint values into the same buckets. This allows for efficient searching and retrieval of audio data based on their fingerprints. A popular implementation of this technique is the FAISS library, which uses locality sensitive hashing to speed up the process of finding similar audio fingerprints. This method has been successfully applied in various industries, including music streaming and audio identification."
51,"Voice Activity Detection (VAD) is a signal processing algorithm that determines if a sound signal contains speech or not. It is an important step in speech processing as it serves as a pre-processing tool for other methods such as speech coding and speech recognition. VAD helps to identify and isolate speech signals from background noise, making it easier for other algorithms to accurately process and analyze the speech."
52,"to detect voice activity in low-noise environments, as there is a clear distinction between speech and non-speech signals. This method involves setting a threshold for the energy level of the signal, and any signal with an energy level above the threshold is classified as speech. This approach works well in low-noise environments but may not be as effective in noisy environments where the threshold needs to be adjusted to account for background noise. Additionally, this method may not be suitable for detecting short bursts of speech or non-speech signals. Overall, while low-noise VAD using energy thresholding is a simple and effective method, it may not be suitable for all environments and may require adjustments for optimal performance.

In low-noise environments, voice activity detection (V"
53,"The document discusses the challenges of using voice activity detection (VAD) for noisy speech. It notes that it is difficult to set energy thresholds for varying levels of background noise, making it challenging to accurately detect speech in noisy environments. This can lead to errors in speech recognition and other applications that rely on VAD."
54,Rule-based voice activity detection (VAD) is a method that uses a series of yes/no decisions to determine whether a signal is speech or non-speech. This approach is useful for devices with limited resources.
55,"The performance evaluation for voice activity detection (VAD) involves two main factors: false positives, which occur when non-speech is detected as speech, and false negatives, which occur when actual speech is missed. The objective of implementing VAD is dependent on the specific application it is being used for."
56,"The article discusses the development of a neural network-based voice activity detection (VAD) system. This system uses deep learning techniques to accurately detect and classify speech signals in noisy environments. The neural network is trained on a large dataset of speech and noise samples to improve its performance. The results show that this VAD system outperforms traditional methods and is robust to various noise levels and types. This technology has potential applications in speech recognition, speaker identification, and noise suppression in communication systems."
57,"-2020

Wake-word and keyword spotting is a technology that activates more complex tasks, such as speech recognition and natural language processing, in voice-activated devices like Alexa. It is designed to run on devices with limited resources and requires tens of thousands of utterances of keywords for training. Google's Speech Commands Dataset is an example of a dataset used for training wake-word and keyword spotting models. This technology was recently researched at the Interspeech 2020 conference by Amazon's Alexa team."
58,"The document discusses a workshop for RTAVS Day 3, which can be accessed through the Assignments section on Canvas. The workshop is organized by the National University of Singapore and is open to all participants. It is important to submit the workshop materials through Canvas to receive credit for participation. The document also reminds participants to complete the pre-workshop survey and to bring their own laptops for the workshop. The workshop will cover topics such as data cleaning, data analysis, and data visualization using various software programs."
59,"The document 'RTAVS_day3.pdf' discusses the training program for the Real-Time Audio and Video Systems (RTAVS) course at the National University of Singapore. On page 59, the author thanks Dr. Gary Leung for his contribution to the course and provides his email for any further inquiries. The course is expected to be completed by 2025 and all rights are reserved by the university."
